{
    "docs": [
        {
            "location": "/", 
            "text": "Welcome\n\n\nSome welcome text should appear here.\n\n\nPerhaps the logos should be added here as well?", 
            "title": "Home"
        }, 
        {
            "location": "/#welcome", 
            "text": "Some welcome text should appear here.  Perhaps the logos should be added here as well?", 
            "title": "Welcome"
        }, 
        {
            "location": "/timetables/timetable_cancer/", 
            "text": "Cancer Genomics Course\n\n\n\n\nMelbourne - 6\nth\n to 8\nth\n February 2017\n\n\n\n\nInstructors\n\n\n\n\n(K)atherine Champ - Bioplatforms Australia, Sydney\n\n\n(A)nn-Marie Patch - QIMR Berghofer, Brisbane\n\n\n(G)ayle Philip - Melbourne Bionformatics, Melbourne\n\n\n(E)rdahl Teber - CMRI, Sydney\n\n\n(S)onika Tyagi - AGRF, Melbourne\n\n\n\n\nTimetable\n\n\nDay 1\n\n\n6\nth\n February\n - \nLab 14 Seminar Room, 700 Swanston St, Carlton, Victoria\n\n\n\n\n\n\n\n\nTime\n\n\nTopic\n\n\nLinks\n\n\nInstructor\n\n\n\n\n\n\n\n\n\n\n9:00\n\n\nWelcome\n\n\n\n\nK\n\n\n\n\n\n\n9:15\n\n\nIntroduction to cancer genomics and NGS techniques \u2013 focus on DNA\n\n\n\n\nG\n\n\n\n\n\n\n10:15\n\n\nBreak\n\n\n\n\n\n\n\n\n\n\n10:30\n\n\nExperimental design (interactive/ice breaker/group activity) and caveats; considerations on processing capacities; 10 ways to ruin your experiment\n\n\n\n\n\n\n\n\n\n\n11:30\n\n\nCommand line intro (Unix, R) \u2013 L(15\n) / P(30\n)\n\n\n\n\n\n\n\n\n\n\n12:30\n\n\nLunch\n\n\n\n\n\n\n\n\n\n\n13:30\n\n\nRaw data - FASTQ format and QC\n\n\n\n\n\n\n\n\n\n\n14:00\n\n\nAlignment (L)\n\n\n\n\n\n\n\n\n\n\n14:30\n\n\nManipulation of BAM files and QC (P)\n\n\n\n\n\n\n\n\n\n\n15:00\n\n\nCoffee break\n\n\n\n\n\n\n\n\n\n\n15:15\n\n\nManipulation of BAM files and QC (P) (cont.)\n\n\n\n\n\n\n\n\n\n\n17:00\n\n\nQ\nA\n\n\n\n\nAll\n\n\n\n\n\n\n\n\nDay 2\n\n\n7\nth\n February\n - \nLab 14 Seminar Room, 700 Swanston St, Carlton, Victoria\n\n\n\n\n\n\n\n\nTime\n\n\nTopic\n\n\nLinks\n\n\nInstructor\n\n\n\n\n\n\n\n\n\n\n9:00\n\n\nSNV detection (review, germline vs somatic, tools, pitfalls, data visualization) \u2013 L/P. Indels \u2013 cover in SNV lecture + bonus exercises (P), specific challenges of indels analysis, tools\n\n\n\n\n\n\n\n\n\n\n9:45\n\n\nSNV detection (P)\n\n\n\n\n\n\n\n\n\n\n10:30\n\n\nCoffee break\n\n\n\n\n\n\n\n\n\n\n10:45\n\n\nVariants annotation and filtration (L) \u2013 tools landscape\n\n\n\n\n\n\n\n\n\n\n11:00\n\n\nVariants visualization (IGV), annotation and filtration (P)\n\n\n\n\n\n\n\n\n\n\n12:30\n\n\nLunch + coffee\n\n\n\n\n\n\n\n\n\n\n13:30\n\n\nCNV analysis using NGS data (L)\n\n\n\n\n\n\n\n\n\n\n14:15\n\n\nCNV analysis (P) \u2013 deletion/amplification, calling CNVs, visualization, interpretation\n\n\n\n\n\n\n\n\n\n\n15:00\n\n\nBreak\n\n\n\n\n\n\n\n\n\n\n15:15\n\n\nCNV analysis (P) \u2013 deletion/amplification, calling CNVs, visualization, interpretation (cont.)\n\n\n\n\n\n\n\n\n\n\n17:00\n\n\nQ\nA\n\n\n\n\nAll\n\n\n\n\n\n\n\n\nDay 3\n\n\n8\nth\n February\n - \nLab 14 Seminar Room, 700 Swanston St, Carlton, Victoria\n\n\n\n\n\n\n\n\nTime\n\n\nTopic\n\n\nLinks\n\n\nInstructor\n\n\n\n\n\n\n\n\n\n\n9:00\n\n\nSV analysis \u2013 breakpoints/fusion (L), tools\n\n\n\n\n\n\n\n\n\n\n9:45\n\n\nSV analysis \u2013 breakpoints/fusion (P)\n\n\n\n\n\n\n\n\n\n\n10:30\n\n\nCoffee break\n\n\n\n\n\n\n\n\n\n\n10:45\n\n\nSV analysis \u2013 breakpoints/fusion (P) (cont.)\n\n\n\n\n\n\n\n\n\n\n12:30\n\n\nLunch\n\n\n\n\n\n\n\n\n\n\n13:30\n\n\nDownstream analysis and interpretation (L) \u2013 Exploration of resources that can be used for this. E.g. databases (COSMIC, TCGA, etc.), integration with clinical information\n\n\n\n\n\n\n\n\n\n\n14:15\n\n\nDownstream analysis and interpretation (P)\n\n\n\n\n\n\n\n\n\n\n15:00\n\n\nCoffee break\n\n\n\n\n\n\n\n\n\n\n15:15\n\n\nDownstream analysis and interpretation (P) (cont.)\n\n\n\n\n\n\n\n\n\n\n16:00\n\n\nHow does it all link together? Integration of different data types (L)\n\n\n\n\nGuest\n\n\n\n\n\n\n16:45\n\n\nQ\nA, wrap up (how to access course\u2019s VM) \n survey\n\n\n\n\nAll", 
            "title": "Timetable"
        }, 
        {
            "location": "/timetables/timetable_cancer/#cancer-genomics-course", 
            "text": "Melbourne - 6 th  to 8 th  February 2017", 
            "title": "Cancer Genomics Course"
        }, 
        {
            "location": "/timetables/timetable_cancer/#instructors", 
            "text": "(K)atherine Champ - Bioplatforms Australia, Sydney  (A)nn-Marie Patch - QIMR Berghofer, Brisbane  (G)ayle Philip - Melbourne Bionformatics, Melbourne  (E)rdahl Teber - CMRI, Sydney  (S)onika Tyagi - AGRF, Melbourne", 
            "title": "Instructors"
        }, 
        {
            "location": "/timetables/timetable_cancer/#timetable", 
            "text": "", 
            "title": "Timetable"
        }, 
        {
            "location": "/timetables/timetable_cancer/#day-1", 
            "text": "6 th  February  -  Lab 14 Seminar Room, 700 Swanston St, Carlton, Victoria     Time  Topic  Links  Instructor      9:00  Welcome   K    9:15  Introduction to cancer genomics and NGS techniques \u2013 focus on DNA   G    10:15  Break      10:30  Experimental design (interactive/ice breaker/group activity) and caveats; considerations on processing capacities; 10 ways to ruin your experiment      11:30  Command line intro (Unix, R) \u2013 L(15 ) / P(30 )      12:30  Lunch      13:30  Raw data - FASTQ format and QC      14:00  Alignment (L)      14:30  Manipulation of BAM files and QC (P)      15:00  Coffee break      15:15  Manipulation of BAM files and QC (P) (cont.)      17:00  Q A   All", 
            "title": "Day 1"
        }, 
        {
            "location": "/timetables/timetable_cancer/#day-2", 
            "text": "7 th  February  -  Lab 14 Seminar Room, 700 Swanston St, Carlton, Victoria     Time  Topic  Links  Instructor      9:00  SNV detection (review, germline vs somatic, tools, pitfalls, data visualization) \u2013 L/P. Indels \u2013 cover in SNV lecture + bonus exercises (P), specific challenges of indels analysis, tools      9:45  SNV detection (P)      10:30  Coffee break      10:45  Variants annotation and filtration (L) \u2013 tools landscape      11:00  Variants visualization (IGV), annotation and filtration (P)      12:30  Lunch + coffee      13:30  CNV analysis using NGS data (L)      14:15  CNV analysis (P) \u2013 deletion/amplification, calling CNVs, visualization, interpretation      15:00  Break      15:15  CNV analysis (P) \u2013 deletion/amplification, calling CNVs, visualization, interpretation (cont.)      17:00  Q A   All", 
            "title": "Day 2"
        }, 
        {
            "location": "/timetables/timetable_cancer/#day-3", 
            "text": "8 th  February  -  Lab 14 Seminar Room, 700 Swanston St, Carlton, Victoria     Time  Topic  Links  Instructor      9:00  SV analysis \u2013 breakpoints/fusion (L), tools      9:45  SV analysis \u2013 breakpoints/fusion (P)      10:30  Coffee break      10:45  SV analysis \u2013 breakpoints/fusion (P) (cont.)      12:30  Lunch      13:30  Downstream analysis and interpretation (L) \u2013 Exploration of resources that can be used for this. E.g. databases (COSMIC, TCGA, etc.), integration with clinical information      14:15  Downstream analysis and interpretation (P)      15:00  Coffee break      15:15  Downstream analysis and interpretation (P) (cont.)      16:00  How does it all link together? Integration of different data types (L)   Guest    16:45  Q A, wrap up (how to access course\u2019s VM)   survey   All", 
            "title": "Day 3"
        }, 
        {
            "location": "/preamble/", 
            "text": "Providing Feedback\n\n\nWhile we endeavour to deliver a workshop with quality content and\ndocumentation in a venue conducive to an exciting, well run hands-on\nworkshop with a bunch of knowledgeable and likable trainers, we know\nthere are things we could do better.\n\n\nWhilst we want to know what didn\u2019t quite hit the mark for you, what\nwould be most helpful and least depressing, would be for you to provide\nways to improve the workshop. i.e. constructive feedback. After all, if\nwe knew something wasn\u2019t going to work, we wouldn\u2019t have done it or put\nit into the workshop in the first place! Remember, we\u2019re experts in the\nfield of bioinformatics not experts in the field of biology!\n\n\nClearly, we also want to know what we did well! This gives us that \u201cfeel\ngood\u201d factor which will see us through those long days and nights in the\nlead up to such hands-on workshops!\n\n\nWith that in mind, we\u2019ll provide three really high tech mechanism\nthrough which you can provide anonymous feedback during the workshop:\n\n\n\n\n\n\nA sheet of paper, from a flip-chart, sporting a \u201chappy\u201d face and a\n\u201cnot so happy\u201d face. Armed with a stack of colourful post-it notes, your\nmission is to see how many comments you can stick on the \u201chappy\u201d side!\n\n\n\n\n\n\nSome empty ruled pages at the back of this handout. Use them for\nyour own personal notes or for write specific comments/feedback about\nthe workshop as it progresses.\n\n\n\n\n\n\nAn online post-workshop evaluation survey. We\u2019ll ask you to complete\nthis before you leave. If you\u2019ve used the blank pages at the back of\nthis handout to make feedback notes, you\u2019ll be able to provide more\nspecific and helpful feedback with the least amount of brain-drain!\n\n\n\n\n\n\nDocument Structure\n\n\nWe have provided you with an electronic copy of the workshop\u2019s hands-on\ntutorial documents. We have done this for two reasons: 1) you will have\nsomething to take away with you at the end of the workshop, and 2) you\ncan save time (mis)typing commands on the command line by using\ncopy-and-paste.\n\n\n\n\nWhile you could fly through the hands-on sessions doing copy-and-paste\nyou will learn more if you take the time, saved from not having to type\nall those commands, to understand what each command is doing!\n\n\n\n\nThe commands to enter at a terminal look something like this:\n\n\n1\ntophat --solexa-quals -g 2 --library-type fr-unstranded -j annotation/Danio_rerio.Zv9.66.spliceSites -o tophat/ZV9_2cells genome/ZV9 data/2cells_1.fastq data/2cells_2.fastq\n\n\n\n\n\n\nThe following styled code is not to be entered at a terminal, it is\nsimply to show you the syntax of the command. You must use your own\njudgement to substitute in the correct arguments, options, filenames etc\n\n\n1\ntophat [options]* \nindex_base\n \nreads_1\n \nreads_2\n\n\n\n\n\n\n\nThe following is an example how of R commands are styled:\n\n\n1\n2\n3\n4\n5\nR \n--\nno\n-\nsave\n\n\nlibrary\n(\nplotrix\n)\n\ndata \n-\n read.table\n(\nrun_25/stats.txt\n,\n header\n=\nTRUE\n)\n\nweighted.hist\n(\ndata\n$\nshort1_cov\n+\ndata\n$\nshort2_cov\n,\n data\n$\nlgth\n,\n breaks\n=\n0\n:\n70\n)\n\n\nq\n()\n\n\n\n\n\n\n\nThe following icons are used throughout the documentation\nto help you navigate around the document more easily:\n\n\n\n\nQuestion\n\n\nQuestions to answer.\n\n\n\n\n\n\nAnswer\n\n\nAnswers will be provided at the end of the workskop.\n\n\n\n\n\n\nImportant\n\n\nThis is important. \n\n\n\n\n\n\nSTOP\n\n\nWarning - STOP and read.\n\n\n\n\n\n\nBonus exercise\n\n\nBonus exercise for fast learners.\n\n\n\n\n\n\nAdvanced exercise\n\n\nAdvanced exercise for super-fast learners\n\n\n\n\nResources Used\n\n\nWe have provided you with an environment which contains all the tools\nand data you need for the duration of this workshop. However, we also\nprovide details about the tools and data used by each module at the\nstart of the respective module documentation.", 
            "title": "Workshop Information"
        }, 
        {
            "location": "/preamble/#providing-feedback", 
            "text": "While we endeavour to deliver a workshop with quality content and\ndocumentation in a venue conducive to an exciting, well run hands-on\nworkshop with a bunch of knowledgeable and likable trainers, we know\nthere are things we could do better.  Whilst we want to know what didn\u2019t quite hit the mark for you, what\nwould be most helpful and least depressing, would be for you to provide\nways to improve the workshop. i.e. constructive feedback. After all, if\nwe knew something wasn\u2019t going to work, we wouldn\u2019t have done it or put\nit into the workshop in the first place! Remember, we\u2019re experts in the\nfield of bioinformatics not experts in the field of biology!  Clearly, we also want to know what we did well! This gives us that \u201cfeel\ngood\u201d factor which will see us through those long days and nights in the\nlead up to such hands-on workshops!  With that in mind, we\u2019ll provide three really high tech mechanism\nthrough which you can provide anonymous feedback during the workshop:    A sheet of paper, from a flip-chart, sporting a \u201chappy\u201d face and a\n\u201cnot so happy\u201d face. Armed with a stack of colourful post-it notes, your\nmission is to see how many comments you can stick on the \u201chappy\u201d side!    Some empty ruled pages at the back of this handout. Use them for\nyour own personal notes or for write specific comments/feedback about\nthe workshop as it progresses.    An online post-workshop evaluation survey. We\u2019ll ask you to complete\nthis before you leave. If you\u2019ve used the blank pages at the back of\nthis handout to make feedback notes, you\u2019ll be able to provide more\nspecific and helpful feedback with the least amount of brain-drain!", 
            "title": "Providing Feedback"
        }, 
        {
            "location": "/preamble/#document-structure", 
            "text": "We have provided you with an electronic copy of the workshop\u2019s hands-on\ntutorial documents. We have done this for two reasons: 1) you will have\nsomething to take away with you at the end of the workshop, and 2) you\ncan save time (mis)typing commands on the command line by using\ncopy-and-paste.   While you could fly through the hands-on sessions doing copy-and-paste\nyou will learn more if you take the time, saved from not having to type\nall those commands, to understand what each command is doing!   The commands to enter at a terminal look something like this:  1 tophat --solexa-quals -g 2 --library-type fr-unstranded -j annotation/Danio_rerio.Zv9.66.spliceSites -o tophat/ZV9_2cells genome/ZV9 data/2cells_1.fastq data/2cells_2.fastq   The following styled code is not to be entered at a terminal, it is\nsimply to show you the syntax of the command. You must use your own\njudgement to substitute in the correct arguments, options, filenames etc  1 tophat [options]*  index_base   reads_1   reads_2    The following is an example how of R commands are styled:  1\n2\n3\n4\n5 R  -- no - save  library ( plotrix ) \ndata  -  read.table ( run_25/stats.txt ,  header = TRUE ) \nweighted.hist ( data $ short1_cov + data $ short2_cov ,  data $ lgth ,  breaks = 0 : 70 )  q ()    The following icons are used throughout the documentation\nto help you navigate around the document more easily:   Question  Questions to answer.    Answer  Answers will be provided at the end of the workskop.    Important  This is important.     STOP  Warning - STOP and read.    Bonus exercise  Bonus exercise for fast learners.    Advanced exercise  Advanced exercise for super-fast learners", 
            "title": "Document Structure"
        }, 
        {
            "location": "/preamble/#resources-used", 
            "text": "We have provided you with an environment which contains all the tools\nand data you need for the duration of this workshop. However, we also\nprovide details about the tools and data used by each module at the\nstart of the respective module documentation.", 
            "title": "Resources Used"
        }, 
        {
            "location": "/modules/btp-module-chip-seq/chip-seq-draft-SL_ST/", 
            "text": "Key Learning Outcomes\n\n\nAfter completing this practical the trainee should be able to:\n\n\n\n\n\n\nPerform ChIP-Seq analysis, e.g. the detection of immuno-enriched\n    areas using the chosen R package: ChIP-seq processing pipeline (SPP)\n\n\n\n\n\n\nVisualize the peak regions through a genome browser, e.g. IGV or\n    Ensembl, and identify the real peak regions\n\n\n\n\n\n\nPerform functional annotation using biomaRt R package and detect\n    potential binding sites (motif) in the predicted binding regions\n    using motif discovery tool, e.g. Trawler or MEME.\n\n\n\n\n\n\nResources You\u2019ll be Using\n\n\nTools Used\n\n\nSPP: \n\n\nhttp://compbio.med.harvard.edu/Supplements/ChIP-seq/\n\n\nIGV:\n\n\nhttp://software.broadinstitute.org/software/igv/\n\n\nEnsembl:\n\n\nhttp://www.ensembl.org\n\n\nTrawler:\n\n\nhttps://trawler.erc.monash.edu.au/index.html\n\n\nMEME:\n\n\nhttp://meme.ebi.edu.au/meme/cgi-bin/meme.cgi\n\n\nTOMTOM:\n\n\nhttp://meme.ebi.edu/meme/cgi-bin/tomtom.cgi\n\n\nDAVID:\n\n\nhttp://david.abcc.ncifcrf.gov\n\n\nGOstat:\n\n\nhttp://gostat.wehi.edu.au\n\n\nSources of Data\n\n\nhttp://www.ebi.ac.uk/arrayexpress/experiments/E-GEOD-11431\n\n\nIntroduction\n\n\nThe goal of this hands-on session is to perform some basic tasks in the\nanalysis of ChIP-seq data. In fact, you already performed the first\nstep, alignment of the reads to the genome, in the previous session. We\nstart from the aligned reads and we will find immuno-enriched areas\nusing SPP. We will visualize the identified regions in a genome browser\nand perform functional annotation and motif analysis on the predicted\nbinding regions.\n\n\nPrepare the Environment\n\n\nThe material for this practical can be found in the \nChIP-seq\n directory\non your desktop. Please make sure that this directory also contains the\nSAM/BAM files you produced during the alignment practical.\n\n\nIf you didn\u2019t have time to align the control file called \ngfp.fastq\n\nduring the alignment practical, please do it now. Follow the same steps,\nfrom the bowtie alignment step, as for the \nOct4.fastq\n file.\n\n\nIn ChIP-seq analysis (unlike in other applications such as RNA-seq) it\ncan be useful to exclude all reads that map to more than one location in\nthe genome. When using Bowtie, this can be done using the \n-m 1\n option,\nwhich tells it to report only unique matches (See \nbowtie \u2013help\n for\nmore details).\n\n\nOpen the Terminal and go to the \nChIP-seq\n directory:\n\n\n1\n2\n3\ncd /home/trainee/ChIP-seq\nls\nR\n\n\n\n\n\n\nFinding enriched areas using SPP\n\n\nTerminology used in the tutorial: \nfragment:\n overlapping fragments\nobtaining in the IP (immuno precipitation) experiments. \ntag:\n sequenced\npart of the fragment which could be from one end (in case of single end\nsequencing ) or both ends in the paired end data. \nalignment:\n a process\nto determine the position of the tags, which typically should be around\nthe binding site. \npeaks:\n spatial distribution of the tags densities\naround the binding sites on the genome. You would see two separate peaks\nof tags on the positive and negative strand around the binding site. The\ndistance between the two peaks should reflect the size of the protected\nregion.\n\n\nSPP is a Chip-seq processing pipeline implemented using R.\n\n\nThe main functions of SPP include locating quality tag alignment by\nscreening overall DNA-binding signals, removing or restricting certain\npositions with extremely high number of tags, estimating significant\nenrichment regions through genome-wide profiling, providing appropriate\noutputs for visualization, and determining statistically significant\nbinding positions with saturation criteria assessment. Moreover, the\nprocessing of ChIP-seq data can require considerable amount of CPU time,\nit is often necessary to make use of parallel processing. SPP supports\nparallel processing if the cluster option is configured. Since our\nexample data is relatively small, we will use single CPU and omit the\ncluster parameters for simplicity. The following steps will work you\nthrough the SPP pipeline.\n\n\nIn your R terminal, load spp and biomaRt packages and make sure to set\nyour working directory correctly:\n\n\n1\n2\n3\nlibrary(spp);\nlibrary(biomaRt);\nsetwd(\n/home/trainee/ChIP-seq\n);\n\n\n\n\n\n\n1. Loading tag data, selecting choosing alignment quality, removing\nanomalies\n\\\nThe first stage in SPP are 1) load input data; 2) choose alignment\nquality and 3) remove anomalies. SPP can read output from the following\naligners and file formats: ELAND, MAQ, bowtie, Arachne, tagAlign format\nand BAM format (Note: because BAM standard doesn\u2019t specify a flag for a\nuniquely-mapped read, the aligner has to generate a BAM file that would\ncontain only unique reads.)\n\n\nSTEP1 Loading data and quality filter the informative tags\n\n\nFirst load Oct4 and gfp bam files. Here GFP are the control or input\nsamples, these are usually mock IP DNA where you do not expect to see\nany binding peaks.\n\n\n1\n2\noct4.data\n-\nread.bam.tags\n(\nOct4.sorted.bam\n);\n\ngfp.data\n-\nread.bam.tags\n(\ngfp.sorted.bam\n);\n\n\n\n\n\n\n\nThe statistical significance of tags clustering observed for a putative\nprotein binding site depends on the expected background. Therefore, use\nof a input or control DNA is highly recommended in the experiment\ndesign. This provides an experimental assessment of the background tag\ndistribution.\n\n\nThe next step uses cross-correlation profile to calculate binding peak\nseparation distance, and assess whether inclusion of tags with\nnon-perfect alignment quality improves the cross-correlation peak. This\nis done by shifting the strands relative to each other by increasing\ndistance within a given range. cross-correlation of the positive and\nnegative strand tag densities is plotted. The cross-correlation plot\nshould show the predominant size of the protected region.\n\n\n1\nbinding.characteristics \n-\n get.binding.characteristics\n(\noct4.data\n,\nsrange\n=\nc\n(\n50\n,\n500\n),\nbin\n=\n5\n);\n\n\n\n\n\n\n\nThe binding.characteristics provides the estimate of the binding peak\nseparation distance, cross-correlation profile itself and tag quality\nbin acceptance information. The srange parameter defines the possible\nrange for the size of the protected region. It is supposed to be higher\nthan tag length. However, the upper boundary (500) cannot be too high,\nwhich will increase the running time. The bin parameter tags within the\nspecified number of base pairs to speed up calculation. The increase of\nbin size will decrease the accuracy of the determined parameters.\n\n\nThen, print out binding peak separation distance and we can plot\ncross-correlation profile:\n\n\n1\n2\n3\n4\n5\n6\nprint(paste(\nbinding peak separation distance=\n,binding.characteristics$peak$x));\npdf(file=\noct4.crosscorrelation.pdf\n,width=5,height=5);\npar(mar = c(3.5,3.5,1.0,0.5), mgp = c(2,0.65,0), cex = 0.8);\nplot(binding.characteristics$cross.correlation,type=\nl\n,xlab=\nstrand shift\n,ylab=\ncross-correlation\n);\nabline(v=binding.characteristics$peak$x,lty=2,col=2);\ndev.off();\n\n\n\n\n\n\nA set of tags informative about the binding positions should increase\ncross correlation magnitude whereas a randonmly mapped set of tags\nshould decrease it. The following calls will select tags with acceptable\nalignment quality based on the binding characteristics:\n\n\n1\n2\nchip.data \n-\n select.informative.tags\n(\noct4.data\n,\nbinding.characteristics\n);\n\ngfpcontrol.data \n-\n select.informative.tags\n(\ngfp.data\n,\nbinding.characteristics\n);\n\n\n\n\n\n\n\nThe last step below will scan along the chromosomes calculating local\ndensity of region (can be specified using window.size parameter, default\nis 200bp), removing or restricting singular positions with extremely\nhigh tag count relative to the neighborhood:\n\n\n1\n2\nchip.data \n-\n remove.local.tag.anomalies\n(\nchip.data\n);\n\ngfpcontrol.data \n-\n remove.local.tag.anomalies\n(\ngfpcontrol.data\n);\n\n\n\n\n\n\n\nSTEP2 Calculating genome-wide tag density and tag enrichment/depletion profiles\n\n\nThe following commands will calculate smoothed tag density and output it\ninto a WIG file that can be read with genome browsers, such as IGV\n(Note: the tags are shifted by half of the peak separation distance):\n\n\n1\n2\n3\n4\n5\ntag.shift \n-\n \nround\n(\nbinding.characteristics\n$\npeak\n$\nx\n/\n2\n)\n\nsmoothed.density \n-\n get.smoothed.tag.density\n\n(\nchip.data\n,\ncontrol.tags\n=\ngfpcontrol.data\n,\nbandwidth\n=\n200\n,\nstep\n=\n100\n,\ntag.shift\n=\ntag.shift\n);\n\nwritewig\n(\nsmoothed.density\n,\noct4.density.wig\n,\nSmoothed, background-subtracted tag density\n);\n\n\nrm\n(\nsmoothed.density\n);\n\n\n\n\n\n\n\nTo provide a rough estimate of the enrichment profile (i.e. ChIP signal\nover input), we can use the get.smoothed.enrichment.mle() method:\n\n\n1\n2\n3\nsmoothed.enrichment.estimate \n-\n get.smoothed.enrichment.mle\n\n(\nchip.data\n,\ngfpcontrol.data\n,\nbandwidth\n=\n200\n,\nstep\n=\n100\n,\ntag.shift\n=\ntag.shift\n);\n\nwritewig\n(\nsmoothed.enrichment.estimate\n,\noct4.enrichment.wig\n,\nSmoothed maximum likelihood log2 enrichment estimate\n);\n\n\n\n\n\n\n\nNext, we will scan ChIP and signal tag density to estimate lower bounds\nof tag enrichment (and upper bound of tag depletion if it is\nsignificant) along the genome. The resulting profile gives conservative\nstatistical estimates of log2 fold-enrichment ratios along the genome.\nThe example below uses a window of 500bp (and background windows of 1,\n5, 25 and 50 times that size) and a confidence interval corresponding to\n1%.\n\n\n1\n2\n3\nenrichment.estimates \n-\n get.conservative.fold.enrichment.profile\n(\nchip.data\n,\ngfpcontrol.data\n,\nfws\n=\n500\n,\nstep\n=\n100\n,\nalpha\n=\n0.01\n);\n\nwritewig\n(\nenrichment.estimates\n,\noct4.Enrichment.estimates.wig\n,\nConservative fold-enrichment/depletion estimates shown on log2 scale\n);\n\n\nrm\n(\nenrichment.estimates\n);\n\n\n\n\n\n\n\nAlso, broad regions of enrichment for a specified scale can be quickly\nidentified and output in broadPeak format using the following commands:\n\n\n1\n2\nbroad.clusters \n-\n get.broad.enrichment.clusters\n(\nchip.data\n,\ngfpcontrol.data\n,\nwindow.size\n=\n1e3\n,\nz.thr\n=\n3\n,\ntag.shift\n=\nround\n(\nbinding.characteristics\n$\npeak\n$\nx\n/\n2\n));\n\nwrite.broadpeak.info\n(\nbroad.clusters\n,\noct4.broadPeak\n);\n\n\n\n\n\n\n\nwrite out in bed format\n\n\n1\nwrite.table(cbind(rep(\n1\n, length(broad.clusters$chr1$s)), broad.clusters$chr1$s, broad.clusters$chr1$e), file = paste0(\noct4\n,\n_enrich_broad_chr1.bed\n),quote = FALSE, row.names = FALSE, col.names = FALSE, sep = \n\\t\n);\n\n\n\n\n\n\nThe tasks below will use window tag density (WTD) method to call binding\npositions, using FDR of 1% and a window size estimated by the\nbinding.characteristics.\n\n\nWe set the binding detection parameters: FDR (1%) (Note: we can use an\nE-value to the method calls below instead of the fdr), the\nbinding.characteristics contains the optimized half-size for binding\ndetection window:\n\n\n1\n2\nfdr \n-\n \n1e-2\n;\n\ndetection.window.halfsize \n-\n binding.characteristics\n$\nwhs\n;\n\n\n\n\n\n\n\nIdentify binding positions using WTD method and write narrow peaks in\nBED format:\n\n\n1\n2\n3\n4\nbp \n-\n find.binding.positions\n(\nsignal.data\n=\nchip.data\n,\ncontrol.data\n=\ngfpcontrol.data\n,\nfdr\n=\nfdr\n,\nwhs\n=\ndetection.window.halfsize\n);\n\n\nprint\n(\npaste\n(\ndetected\n,\nsum\n(\nunlist\n(\nlapply\n(\nbp\n$\nnpl\n,\nfunction\n(\nd\n)\n \nlength\n(\nd\n$\nx\n)))),\npeaks\n));\n\nbp.short \n-\n add.broad.peak.regions\n(\nchip.data\n,\ngfpcontrol.data\n,\nbp\n,\nwindow.size\n=\n500\n,\nz.thr\n=\n3\n);\n \n//\nset the window size to \n500.\n\nwrite.table\n(\nna.omit\n(\ndata.frame\n(\ncbind\n(\nrep\n(\n1\n,\n \nlength\n(\nbp.short\n$\nnpl\n$\nchr1\n$\nrs\n)),\n bp.short\n$\nnpl\n$\nchr1\n$\nrs\n,\n bp.short\n$\nnpl\n$\nchr1\n$\nre\n))),\n file \n=\n \npaste0\n(\noct4\n,\n_enrich_narrow_chr1.bed\n),\nquote \n=\n \nFALSE\n,\n row.names \n=\n \nFALSE\n,\n col.names \n=\n \nFALSE\n,\n sep \n=\n \n\\t\n);\n\n\n\n\n\n\n\nSTEP3 Comparing Binding Sites to Annotations Using the biomaRt package\n\n\nIn order to biologically interpret the results of ChIP-seq experiments,\nit is usually recommended to look at the genes and other annotated\nelements that are located in proximity to the identified enriched\nregions. This can be easily done using the R biomaRt package, which\nserves as an interface to perform comprehensive data analysis from gene\nannotation to data mining through wealth number of biological databases\nintegrated by the BioMart software suite (\nhttp://www.biomart.org\n). It\nprovides fast access to large amount of data without touching the\nunderlying database or using complex database queries. These major\ndatabases including Ensembl, COSMIC, HGNC, Gramene, Wormbase and dbSNP\nmapped to Emsembl.\n\n\nyou should make sure that ensembl has the same version of reference as\nyou used in bowtie aligner.\n\n\nWe will download the ENSEMBLE mouse genome annotations and generate a\nlist of ENSEMBLE gene information on chromosome 1 including start\nposition, end position, strand and description\n\n\n1\n2\nensembl = useMart(host=\nasia.ensembl.org\n, \nENSEMBL_MART_ENSEMBL\n, dataset = \nmmusculus_gene_ensembl\n);\ngenes.chr1 = getBM(attributes = c(\nchromosome_name\n, \nstart_position\n, \nend_position\n, \nstrand\n, \ndescription\n), filters = \nchromosome_name\n, values= \n1\n, mart = ensembl);\n\n\n\n\n\n\nNext, we\u2019re going to take our binding sites from the bp list and use it\nto determine the set of genes that contain significantly enriched Pol II\nwithin 2kb of their TSS.\n\n\nIn order to compare PolII sites to TSS sites, we need to write an\noverlap function where bs represents a binding site position, ts is the\nannotated TSS and l is the allowed distance of the binding site from the\nTSS.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\noverlap = function(bs, ts, l)\n{\n    if ((bs \n ts - l) \n (bs \n ts + l)) {\n        TRUE;\n    } else {\n        FALSE;\n    }\n}\n\n\n\n\n\n\nNow we\u2019ll write a function that takes a vector of binding site values,\nstart positions, end positions and strands of the genes on chromosome X\nas well as our distance cutoff. l and outputs a logical vector of the\ngenes that contain a Pol II site within l bp (i.e., TRUE value) or do\nnot contain a Pol II site (i.e., FALSE value).\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\nfivePrimeGenes = function(bs, ts, te, s, l) {\n    fivePrimeVec = logical();\n    for (i in 1:length(ts)) {\n            fivePrime = FALSE;\n            for (j in 1:length(bs)) {\n                if (s[i] == 1) {\n                    fivePrime = fivePrime || overlap(bs[j], ts[i], l);\n                } else {\n                    fivePrime = fivePrime || overlap(bs[j], te[i], l);\n                }\n             }\n            fivePrimeVec = c(fivePrimeVec, fivePrime);\n    }\n     fivePrimeVec;\n}\n\n\n\n\n\n\nUsing the fivePrimeGenes function, generate a vector of the TSSs and\ngenes that contain Pol II within .2kb of their TSS (i.e., l = 2000).\n\n\n1\nfivePrimeGenesLogical = fivePrimeGenes(bp$npl$chr1$x, genes.chr1$start_position, genes.chr1$end_position, genes.chr1$strand, 2000);\n\n\n\n\n\n\nFind the gene located on the plus strand\n\n\n1\nfivePrimeStartsPlus = genes.chr1$start_position[fivePrimeGenesLogical \n genes.chr1$strand == 1];\n\n\n\n\n\n\nFind the gene located on the minus strand\n\n\n1\nfivePrimeStartsMinus = genes.chr1$end_position[fivePrimeGenesLogical \n genes.chr1$strand == -1];\n\n\n\n\n\n\nCombine the start positions together\n\n\n1\nfivePrimeStarts = sort(c(fivePrimeStartsPlus, fivePrimeStartsMinus))\n\n\n\n\n\n\nGet all the gene names\n\n\n1\nfivePrimeGenes = genes.chr1$description[fivePrimeGenesLogical]\n\n\n\n\n\n\nViewing results with the Genome browser\n\n\nIt is often instructive to look at your data in a genome browser, which\nwill allow you to get a \u2018feel\u2019 for the data, as well as detecting\nabnormalities and problems. Also, exploring the data in such a way may\ngive you ideas for further analyses. Well known web-based genome\nbrowsers, like Ensembl or the UCSC browser do not only allow for more\npolished and flexible visualization, but also provide access to a wealth\nof annotations and external data sources. This makes it straightforward\nto relate your data with information about repeat regions, known genes,\nepigenetic features or areas of cross-species conservation, to name just\na few. As such, they are useful tools for exploratory analysis, even\nthough could be relatively slow. In this section, we will guide you\nthough using IGV, a stand-alone browser, which has the advantage of\nbeing installed locally, easy to use and fast access to visualize your\nin-house data. We alo provide the workflow of how to use Ensembl for\nvisualization. You can practise after the workshop.\n\n\nIGV Visualization\n\n\nDouble click the IGV 2.3 icon on your Desktop. Ignore any warnings and\nwhen it opens you have to load the genome of interest. On the top left\nof your screen choose from the drop down menu Mouse (mm10). If it\ndoesn\u2019t appear in list, click More .., type mm10 in the Filter section,\nchoose the mouse genome and press OK.\n\n\nWe have generated bigWig files in advance for you. Instead of choosing\nthe \u2019Load from File\u2019 option, we are going to use \u2019Load from URL\u2019 to\nupload to IGV. The first file is at the following URL:\n\nhttp://www.ebi.ac.uk/~remco/ChIP-Seq_course/Oct4.bw\n\n\nTo visualise the data:\n\n\n\n\n\n\nSelect chr1 in the chromosome drop-down box next to the \u2019Mouse mm10\u2019\n    box.\n\n\n\n\n\n\nClick \nFile\n then choose \nLoad from URL\n\n\n\n\n\n\nPaste the location above in the field \nFile URL\n.\n\n\n\n\n\n\nClick \nOK\n and close the window to return to the genome browser.\n\n\n\n\n\n\nYou should see Oct4.bw has been loaded in the track region below the\n    genome region.\n\n\n\n\n\n\nMove the mouse to track region over Oct4.bw.\n\n\n\n\n\n\nRight click the mouse, Change the track colour on your own\n    perference.\n\n\n\n\n\n\nRight click again, in the \nWindowing Function\n, choose \nMaxmum\n\n    and set to \nAutoscale\n.\n\n\n\n\n\n\nRepeat the process for the gfp control sample, located at:\n\n\nhttp://www.ebi.ac.uk/~remco/ChIP-Seq_course/gfp.bw\n.\n\n\nGo to a region on chromosome 1 (e.g. \n1\n:\n34823162\n-\n35323161\n), and zoom in\nand out to view the signal and peak regions. Be aware that the y-axis of\neach track is auto-scaled independently of each other, so bigger-looking\npeaks may not actually be bigger! Always look at the values on the left\nhand side axis.\n\n\nWhat can you say about the profile of Oct4 peaks in this region?\n\n\nThere are no significant Oct4 peaks over the selected region.\n\n\nCompare it with H3K4me3 histone modification wig file we have generated\nat \nhttp://www.ebi.ac.uk/~remco/ChIP-Seq_course/H3K4me3.bw\n.\n\n\nH3K4me3 has a region that contains relatively high peaks than Oct4.\n\n\nJump to \n1\n:\n36066594\n-\n36079728\n for a sample peak. Do you think H3K4me3\npeaks regions contain one or more modification sites? What about Oct4?\n\n\nYes. There are roughly three peaks, which indicate the possibility of\nhaving more than one modification sites in this region.\n\n\nFor Oct4, no peak can be observed.\n\n\nAdvanced Session\n\n\nEnsembl Visualization\n\n\nLaunch a web browser and go to the Ensembl website at\n\nhttp://www.ensembl.org/index.html\n. Choose the genome of interest (in\nthis case, mouse) on the left side of the page, browse to any location\nin the genome or click one of the demo links provided on the web page.\nClick on the \nAdd your data\n link on the left, then choose \nAttach\nremote file\n.\n\n\nWig files are large so are inconvenient for uploading directly to the\nEnsemble Genome browser. Instead, we will convert it to an indexed\nbinary format and put this into a web accessible place such as on a\nHTTP, HTTPS, or FTP server. This makes all the browsing process much\nfaster. Detailed instructions for generating a bigWig from a wig type\nfile can be found at:\n\n\nhttp://genome.ucsc.edu/goldenPath/help/bigWig.html\n.\n\n\nWe have generated bigWig files in advance for you to upload to the\nEnsembl browser. They are at the following URL:\n\nhttp://www.ebi.ac.uk/~remco/ChIP-Seq_course/Oct4.bw\n\n\nTo visualise the data:\n\n\n\n\n\n\nPaste the location above in the field File URL.\n\n\n\n\n\n\nChoose data format bigWig.\n\n\n\n\n\n\nChoose some informative name and in the next window choose the\n    colour of your preference.\n\n\n\n\n\n\nClick \nSave\n and close the window to return to the genome browser.\n\n\n\n\n\n\nRepeat the process for the gfp control sample, located at:\n\n\nhttp://www.ebi.ac.uk/~remco/ChIP-Seq_course/gfp.bw\n.\n\n\nIf can not see your tracks: Click on \u2019Configure this page\u2019 in left\npanel. In \u2019Configure region Overview\u2019 tab click on \u2019Ypur data\u2019 in left\npanel. Check the boxes in \u2019Enable/Disable all tracks\u2019 for you *.bw\nfiles by selecting \u2019wiggle plot in the pop up menu.\n\n\nAfter uploading, choose \nConfigure this page\n, and under \nYour data\n\ntick both boxes. Closing the window will save these changes.\n\n\nGo to a region on chromosome 1 (e.g. \n1\n:\n34823162\n-\n35323161\n), and zoom in\nand out to view the signal and peak regions. Be aware that the y-axis of\neach track is auto-scaled independently of each other, so bigger-looking\npeaks may not actually be bigger! Always look at the values on the left\nhand side axis.\n\n\nMACS generates its peak files in a file format called bed file. This is\na simple text format containing genomic locations, specified by\nchromosome, begin and end positions, and some more optional information.\n\n\nSee \nhttp://genome.ucsc.edu/FAQ/FAQformat.html#format1\n for details.\n\n\nBed files can also be uploaded to the Ensembl browser.\n\n\nMotif analysis\n\n\nIt is often interesting to find out whether we can identify\ntranscription factor binding sites (TFBSs) from the input DNA sequences.\nTFBSs which share a similar sequence pattern (motif) are presumed to\nhave biological functions. Here we introduce three motif discovery tools\nnamed Trawler, RSAT peak-motifs, and MEME-ChIP, which use different\nsearching algorithms. This might lead to varying results. Hence, it is\ngenerally a good idea to use several tools to identify TFBSs. The more\ntools confirm the same result, the better, which is also called an\northogonal approach. Eventually, you probably want to validate your in\nsilico findings in vivo or in vitro.\n\n\nMotif discovery with Trawler\n\n\nTrawler is a fast, yet accurate motif discovery tool that accepts both,\nBED and FASTA files as input file formats. BED files are generated when\nyou process and analyse your NGS data. Thus, it is handy to use them\ndirectly in Trawler. Other tools do not accept BED files as input. With\nTrawler, BED files can be converted into FASTA files that can then be\nused for other motif discovery tools (e.g. RSAT peak-motifs and MEME\nChIP).\n\n\nRunning Trawler\n\n\n\n\n\n\nGo to the website \nhttps://trawler.erc.monash.edu.au\n\n\n\n\n\n\nRun Trawler with BED file as input, and wait for the results\n    \n \n\n\n\n\n\n\nDownload both sample and background files in FASTA format. Right\n    click and choose: \u2019Save the link as\n\u2019 \n\n\n\n\n\n\nWhich motif was found to be the most similar to your motif?\n\n\nSox2\n\n\nOptional: Motif discovery with RSAT peak-motif\n\n\nThe motif discovery tool RSAT peak-motifs uses FASTA files as input. An\noptional background can be uploaded in FASTA format. RSAT peak-motifs\nautomatically outputs motifs of 6 and 7 nucleotides length (two separate\nfiles). While still accurate, the running time is longer compared to\nTrawler (up to 20 minutes depending on the size of the files).\n\n\nRunning RSAT peak-motifs\n\n\n\n\n\n\nGo to the website\n    \nhttp://floresta.eead.csic.es/rsat/peak-motifs~f~orm.cgi\n\n\n\n\n\n\nUpload input and background FASTA files just downloaded from Trawler\n    \n\n\n\n\n\n\nWait until the discovery finishes.\n\n\n\n\n\n\nOptional: Motif discovery with MEME-ChIP\n\n\nMEME-ChIP is a popular motif discovery tool and part of MEME Suite.\nMEME-ChIP accepts input files in FASTA format. It is not necessary to\nupload your own background because MEME-ChIP uses its own. Although\nMEME-ChIP is one of the most popular motif discovery tools, the\nidentified motifs are not very accurate and the motif search might take\nup to one hour. MEME-ChIP outputs the three motifs with the lowest\nE-Value.\n\n\nRunning MEME-ChIP\n\n\n\n\n\n\nGo to the website \nhttp://meme-suite.org/tools/meme-chip\n\n\n\n\n\n\nUpload input and background FASTA files just downloaded from Trawler\n    \n\n\n\n\n\n\nWait until the discovery finishes.\n\n\n\n\n\n\nReference\n\n\nChen, X et al.: Integration of external signaling pathways with the core\ntranscriptional network in embryonic stem cells. Cell 133:6, 1106-17\n(2008).", 
            "title": "ChIP-seq"
        }, 
        {
            "location": "/modules/btp-module-chip-seq/chip-seq-draft-SL_ST/#key-learning-outcomes", 
            "text": "After completing this practical the trainee should be able to:    Perform ChIP-Seq analysis, e.g. the detection of immuno-enriched\n    areas using the chosen R package: ChIP-seq processing pipeline (SPP)    Visualize the peak regions through a genome browser, e.g. IGV or\n    Ensembl, and identify the real peak regions    Perform functional annotation using biomaRt R package and detect\n    potential binding sites (motif) in the predicted binding regions\n    using motif discovery tool, e.g. Trawler or MEME.", 
            "title": "Key Learning Outcomes"
        }, 
        {
            "location": "/modules/btp-module-chip-seq/chip-seq-draft-SL_ST/#resources-youll-be-using", 
            "text": "", 
            "title": "Resources You\u2019ll be Using"
        }, 
        {
            "location": "/modules/btp-module-chip-seq/chip-seq-draft-SL_ST/#tools-used", 
            "text": "SPP:   http://compbio.med.harvard.edu/Supplements/ChIP-seq/  IGV:  http://software.broadinstitute.org/software/igv/  Ensembl:  http://www.ensembl.org  Trawler:  https://trawler.erc.monash.edu.au/index.html  MEME:  http://meme.ebi.edu.au/meme/cgi-bin/meme.cgi  TOMTOM:  http://meme.ebi.edu/meme/cgi-bin/tomtom.cgi  DAVID:  http://david.abcc.ncifcrf.gov  GOstat:  http://gostat.wehi.edu.au", 
            "title": "Tools Used"
        }, 
        {
            "location": "/modules/btp-module-chip-seq/chip-seq-draft-SL_ST/#sources-of-data", 
            "text": "http://www.ebi.ac.uk/arrayexpress/experiments/E-GEOD-11431", 
            "title": "Sources of Data"
        }, 
        {
            "location": "/modules/btp-module-chip-seq/chip-seq-draft-SL_ST/#introduction", 
            "text": "The goal of this hands-on session is to perform some basic tasks in the\nanalysis of ChIP-seq data. In fact, you already performed the first\nstep, alignment of the reads to the genome, in the previous session. We\nstart from the aligned reads and we will find immuno-enriched areas\nusing SPP. We will visualize the identified regions in a genome browser\nand perform functional annotation and motif analysis on the predicted\nbinding regions.", 
            "title": "Introduction"
        }, 
        {
            "location": "/modules/btp-module-chip-seq/chip-seq-draft-SL_ST/#prepare-the-environment", 
            "text": "The material for this practical can be found in the  ChIP-seq  directory\non your desktop. Please make sure that this directory also contains the\nSAM/BAM files you produced during the alignment practical.  If you didn\u2019t have time to align the control file called  gfp.fastq \nduring the alignment practical, please do it now. Follow the same steps,\nfrom the bowtie alignment step, as for the  Oct4.fastq  file.  In ChIP-seq analysis (unlike in other applications such as RNA-seq) it\ncan be useful to exclude all reads that map to more than one location in\nthe genome. When using Bowtie, this can be done using the  -m 1  option,\nwhich tells it to report only unique matches (See  bowtie \u2013help  for\nmore details).  Open the Terminal and go to the  ChIP-seq  directory:  1\n2\n3 cd /home/trainee/ChIP-seq\nls\nR", 
            "title": "Prepare the Environment"
        }, 
        {
            "location": "/modules/btp-module-chip-seq/chip-seq-draft-SL_ST/#finding-enriched-areas-using-spp", 
            "text": "Terminology used in the tutorial:  fragment:  overlapping fragments\nobtaining in the IP (immuno precipitation) experiments.  tag:  sequenced\npart of the fragment which could be from one end (in case of single end\nsequencing ) or both ends in the paired end data.  alignment:  a process\nto determine the position of the tags, which typically should be around\nthe binding site.  peaks:  spatial distribution of the tags densities\naround the binding sites on the genome. You would see two separate peaks\nof tags on the positive and negative strand around the binding site. The\ndistance between the two peaks should reflect the size of the protected\nregion.  SPP is a Chip-seq processing pipeline implemented using R.  The main functions of SPP include locating quality tag alignment by\nscreening overall DNA-binding signals, removing or restricting certain\npositions with extremely high number of tags, estimating significant\nenrichment regions through genome-wide profiling, providing appropriate\noutputs for visualization, and determining statistically significant\nbinding positions with saturation criteria assessment. Moreover, the\nprocessing of ChIP-seq data can require considerable amount of CPU time,\nit is often necessary to make use of parallel processing. SPP supports\nparallel processing if the cluster option is configured. Since our\nexample data is relatively small, we will use single CPU and omit the\ncluster parameters for simplicity. The following steps will work you\nthrough the SPP pipeline.  In your R terminal, load spp and biomaRt packages and make sure to set\nyour working directory correctly:  1\n2\n3 library(spp);\nlibrary(biomaRt);\nsetwd( /home/trainee/ChIP-seq );   1. Loading tag data, selecting choosing alignment quality, removing\nanomalies \\\nThe first stage in SPP are 1) load input data; 2) choose alignment\nquality and 3) remove anomalies. SPP can read output from the following\naligners and file formats: ELAND, MAQ, bowtie, Arachne, tagAlign format\nand BAM format (Note: because BAM standard doesn\u2019t specify a flag for a\nuniquely-mapped read, the aligner has to generate a BAM file that would\ncontain only unique reads.)", 
            "title": "Finding enriched areas using SPP"
        }, 
        {
            "location": "/modules/btp-module-chip-seq/chip-seq-draft-SL_ST/#step1-loading-data-and-quality-filter-the-informative-tags", 
            "text": "First load Oct4 and gfp bam files. Here GFP are the control or input\nsamples, these are usually mock IP DNA where you do not expect to see\nany binding peaks.  1\n2 oct4.data - read.bam.tags ( Oct4.sorted.bam ); \ngfp.data - read.bam.tags ( gfp.sorted.bam );    The statistical significance of tags clustering observed for a putative\nprotein binding site depends on the expected background. Therefore, use\nof a input or control DNA is highly recommended in the experiment\ndesign. This provides an experimental assessment of the background tag\ndistribution.  The next step uses cross-correlation profile to calculate binding peak\nseparation distance, and assess whether inclusion of tags with\nnon-perfect alignment quality improves the cross-correlation peak. This\nis done by shifting the strands relative to each other by increasing\ndistance within a given range. cross-correlation of the positive and\nnegative strand tag densities is plotted. The cross-correlation plot\nshould show the predominant size of the protected region.  1 binding.characteristics  -  get.binding.characteristics ( oct4.data , srange = c ( 50 , 500 ), bin = 5 );    The binding.characteristics provides the estimate of the binding peak\nseparation distance, cross-correlation profile itself and tag quality\nbin acceptance information. The srange parameter defines the possible\nrange for the size of the protected region. It is supposed to be higher\nthan tag length. However, the upper boundary (500) cannot be too high,\nwhich will increase the running time. The bin parameter tags within the\nspecified number of base pairs to speed up calculation. The increase of\nbin size will decrease the accuracy of the determined parameters.  Then, print out binding peak separation distance and we can plot\ncross-correlation profile:  1\n2\n3\n4\n5\n6 print(paste( binding peak separation distance= ,binding.characteristics$peak$x));\npdf(file= oct4.crosscorrelation.pdf ,width=5,height=5);\npar(mar = c(3.5,3.5,1.0,0.5), mgp = c(2,0.65,0), cex = 0.8);\nplot(binding.characteristics$cross.correlation,type= l ,xlab= strand shift ,ylab= cross-correlation );\nabline(v=binding.characteristics$peak$x,lty=2,col=2);\ndev.off();   A set of tags informative about the binding positions should increase\ncross correlation magnitude whereas a randonmly mapped set of tags\nshould decrease it. The following calls will select tags with acceptable\nalignment quality based on the binding characteristics:  1\n2 chip.data  -  select.informative.tags ( oct4.data , binding.characteristics ); \ngfpcontrol.data  -  select.informative.tags ( gfp.data , binding.characteristics );    The last step below will scan along the chromosomes calculating local\ndensity of region (can be specified using window.size parameter, default\nis 200bp), removing or restricting singular positions with extremely\nhigh tag count relative to the neighborhood:  1\n2 chip.data  -  remove.local.tag.anomalies ( chip.data ); \ngfpcontrol.data  -  remove.local.tag.anomalies ( gfpcontrol.data );", 
            "title": "STEP1 Loading data and quality filter the informative tags"
        }, 
        {
            "location": "/modules/btp-module-chip-seq/chip-seq-draft-SL_ST/#step2-calculating-genome-wide-tag-density-and-tag-enrichmentdepletion-profiles", 
            "text": "The following commands will calculate smoothed tag density and output it\ninto a WIG file that can be read with genome browsers, such as IGV\n(Note: the tags are shifted by half of the peak separation distance):  1\n2\n3\n4\n5 tag.shift  -   round ( binding.characteristics $ peak $ x / 2 ) \nsmoothed.density  -  get.smoothed.tag.density ( chip.data , control.tags = gfpcontrol.data , bandwidth = 200 , step = 100 , tag.shift = tag.shift ); \nwritewig ( smoothed.density , oct4.density.wig , Smoothed, background-subtracted tag density );  rm ( smoothed.density );    To provide a rough estimate of the enrichment profile (i.e. ChIP signal\nover input), we can use the get.smoothed.enrichment.mle() method:  1\n2\n3 smoothed.enrichment.estimate  -  get.smoothed.enrichment.mle ( chip.data , gfpcontrol.data , bandwidth = 200 , step = 100 , tag.shift = tag.shift ); \nwritewig ( smoothed.enrichment.estimate , oct4.enrichment.wig , Smoothed maximum likelihood log2 enrichment estimate );    Next, we will scan ChIP and signal tag density to estimate lower bounds\nof tag enrichment (and upper bound of tag depletion if it is\nsignificant) along the genome. The resulting profile gives conservative\nstatistical estimates of log2 fold-enrichment ratios along the genome.\nThe example below uses a window of 500bp (and background windows of 1,\n5, 25 and 50 times that size) and a confidence interval corresponding to\n1%.  1\n2\n3 enrichment.estimates  -  get.conservative.fold.enrichment.profile ( chip.data , gfpcontrol.data , fws = 500 , step = 100 , alpha = 0.01 ); \nwritewig ( enrichment.estimates , oct4.Enrichment.estimates.wig , Conservative fold-enrichment/depletion estimates shown on log2 scale );  rm ( enrichment.estimates );    Also, broad regions of enrichment for a specified scale can be quickly\nidentified and output in broadPeak format using the following commands:  1\n2 broad.clusters  -  get.broad.enrichment.clusters ( chip.data , gfpcontrol.data , window.size = 1e3 , z.thr = 3 , tag.shift = round ( binding.characteristics $ peak $ x / 2 )); \nwrite.broadpeak.info ( broad.clusters , oct4.broadPeak );    write out in bed format  1 write.table(cbind(rep( 1 , length(broad.clusters$chr1$s)), broad.clusters$chr1$s, broad.clusters$chr1$e), file = paste0( oct4 , _enrich_broad_chr1.bed ),quote = FALSE, row.names = FALSE, col.names = FALSE, sep =  \\t );   The tasks below will use window tag density (WTD) method to call binding\npositions, using FDR of 1% and a window size estimated by the\nbinding.characteristics.  We set the binding detection parameters: FDR (1%) (Note: we can use an\nE-value to the method calls below instead of the fdr), the\nbinding.characteristics contains the optimized half-size for binding\ndetection window:  1\n2 fdr  -   1e-2 ; \ndetection.window.halfsize  -  binding.characteristics $ whs ;    Identify binding positions using WTD method and write narrow peaks in\nBED format:  1\n2\n3\n4 bp  -  find.binding.positions ( signal.data = chip.data , control.data = gfpcontrol.data , fdr = fdr , whs = detection.window.halfsize );  print ( paste ( detected , sum ( unlist ( lapply ( bp $ npl , function ( d )   length ( d $ x )))), peaks )); \nbp.short  -  add.broad.peak.regions ( chip.data , gfpcontrol.data , bp , window.size = 500 , z.thr = 3 );   // set the window size to  500. \nwrite.table ( na.omit ( data.frame ( cbind ( rep ( 1 ,   length ( bp.short $ npl $ chr1 $ rs )),  bp.short $ npl $ chr1 $ rs ,  bp.short $ npl $ chr1 $ re ))),  file  =   paste0 ( oct4 , _enrich_narrow_chr1.bed ), quote  =   FALSE ,  row.names  =   FALSE ,  col.names  =   FALSE ,  sep  =   \\t );", 
            "title": "STEP2 Calculating genome-wide tag density and tag enrichment/depletion profiles"
        }, 
        {
            "location": "/modules/btp-module-chip-seq/chip-seq-draft-SL_ST/#step3-comparing-binding-sites-to-annotations-using-the-biomart-package", 
            "text": "In order to biologically interpret the results of ChIP-seq experiments,\nit is usually recommended to look at the genes and other annotated\nelements that are located in proximity to the identified enriched\nregions. This can be easily done using the R biomaRt package, which\nserves as an interface to perform comprehensive data analysis from gene\nannotation to data mining through wealth number of biological databases\nintegrated by the BioMart software suite ( http://www.biomart.org ). It\nprovides fast access to large amount of data without touching the\nunderlying database or using complex database queries. These major\ndatabases including Ensembl, COSMIC, HGNC, Gramene, Wormbase and dbSNP\nmapped to Emsembl.  you should make sure that ensembl has the same version of reference as\nyou used in bowtie aligner.  We will download the ENSEMBLE mouse genome annotations and generate a\nlist of ENSEMBLE gene information on chromosome 1 including start\nposition, end position, strand and description  1\n2 ensembl = useMart(host= asia.ensembl.org ,  ENSEMBL_MART_ENSEMBL , dataset =  mmusculus_gene_ensembl );\ngenes.chr1 = getBM(attributes = c( chromosome_name ,  start_position ,  end_position ,  strand ,  description ), filters =  chromosome_name , values=  1 , mart = ensembl);   Next, we\u2019re going to take our binding sites from the bp list and use it\nto determine the set of genes that contain significantly enriched Pol II\nwithin 2kb of their TSS.  In order to compare PolII sites to TSS sites, we need to write an\noverlap function where bs represents a binding site position, ts is the\nannotated TSS and l is the allowed distance of the binding site from the\nTSS.  1\n2\n3\n4\n5\n6\n7\n8 overlap = function(bs, ts, l)\n{\n    if ((bs   ts - l)   (bs   ts + l)) {\n        TRUE;\n    } else {\n        FALSE;\n    }\n}   Now we\u2019ll write a function that takes a vector of binding site values,\nstart positions, end positions and strands of the genes on chromosome X\nas well as our distance cutoff. l and outputs a logical vector of the\ngenes that contain a Pol II site within l bp (i.e., TRUE value) or do\nnot contain a Pol II site (i.e., FALSE value).   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15 fivePrimeGenes = function(bs, ts, te, s, l) {\n    fivePrimeVec = logical();\n    for (i in 1:length(ts)) {\n            fivePrime = FALSE;\n            for (j in 1:length(bs)) {\n                if (s[i] == 1) {\n                    fivePrime = fivePrime || overlap(bs[j], ts[i], l);\n                } else {\n                    fivePrime = fivePrime || overlap(bs[j], te[i], l);\n                }\n             }\n            fivePrimeVec = c(fivePrimeVec, fivePrime);\n    }\n     fivePrimeVec;\n}   Using the fivePrimeGenes function, generate a vector of the TSSs and\ngenes that contain Pol II within .2kb of their TSS (i.e., l = 2000).  1 fivePrimeGenesLogical = fivePrimeGenes(bp$npl$chr1$x, genes.chr1$start_position, genes.chr1$end_position, genes.chr1$strand, 2000);   Find the gene located on the plus strand  1 fivePrimeStartsPlus = genes.chr1$start_position[fivePrimeGenesLogical   genes.chr1$strand == 1];   Find the gene located on the minus strand  1 fivePrimeStartsMinus = genes.chr1$end_position[fivePrimeGenesLogical   genes.chr1$strand == -1];   Combine the start positions together  1 fivePrimeStarts = sort(c(fivePrimeStartsPlus, fivePrimeStartsMinus))   Get all the gene names  1 fivePrimeGenes = genes.chr1$description[fivePrimeGenesLogical]", 
            "title": "STEP3 Comparing Binding Sites to Annotations Using the biomaRt package"
        }, 
        {
            "location": "/modules/btp-module-chip-seq/chip-seq-draft-SL_ST/#viewing-results-with-the-genome-browser", 
            "text": "It is often instructive to look at your data in a genome browser, which\nwill allow you to get a \u2018feel\u2019 for the data, as well as detecting\nabnormalities and problems. Also, exploring the data in such a way may\ngive you ideas for further analyses. Well known web-based genome\nbrowsers, like Ensembl or the UCSC browser do not only allow for more\npolished and flexible visualization, but also provide access to a wealth\nof annotations and external data sources. This makes it straightforward\nto relate your data with information about repeat regions, known genes,\nepigenetic features or areas of cross-species conservation, to name just\na few. As such, they are useful tools for exploratory analysis, even\nthough could be relatively slow. In this section, we will guide you\nthough using IGV, a stand-alone browser, which has the advantage of\nbeing installed locally, easy to use and fast access to visualize your\nin-house data. We alo provide the workflow of how to use Ensembl for\nvisualization. You can practise after the workshop.  IGV Visualization  Double click the IGV 2.3 icon on your Desktop. Ignore any warnings and\nwhen it opens you have to load the genome of interest. On the top left\nof your screen choose from the drop down menu Mouse (mm10). If it\ndoesn\u2019t appear in list, click More .., type mm10 in the Filter section,\nchoose the mouse genome and press OK.  We have generated bigWig files in advance for you. Instead of choosing\nthe \u2019Load from File\u2019 option, we are going to use \u2019Load from URL\u2019 to\nupload to IGV. The first file is at the following URL: http://www.ebi.ac.uk/~remco/ChIP-Seq_course/Oct4.bw  To visualise the data:    Select chr1 in the chromosome drop-down box next to the \u2019Mouse mm10\u2019\n    box.    Click  File  then choose  Load from URL    Paste the location above in the field  File URL .    Click  OK  and close the window to return to the genome browser.    You should see Oct4.bw has been loaded in the track region below the\n    genome region.    Move the mouse to track region over Oct4.bw.    Right click the mouse, Change the track colour on your own\n    perference.    Right click again, in the  Windowing Function , choose  Maxmum \n    and set to  Autoscale .    Repeat the process for the gfp control sample, located at:  http://www.ebi.ac.uk/~remco/ChIP-Seq_course/gfp.bw .  Go to a region on chromosome 1 (e.g.  1 : 34823162 - 35323161 ), and zoom in\nand out to view the signal and peak regions. Be aware that the y-axis of\neach track is auto-scaled independently of each other, so bigger-looking\npeaks may not actually be bigger! Always look at the values on the left\nhand side axis.  What can you say about the profile of Oct4 peaks in this region?  There are no significant Oct4 peaks over the selected region.  Compare it with H3K4me3 histone modification wig file we have generated\nat  http://www.ebi.ac.uk/~remco/ChIP-Seq_course/H3K4me3.bw .  H3K4me3 has a region that contains relatively high peaks than Oct4.  Jump to  1 : 36066594 - 36079728  for a sample peak. Do you think H3K4me3\npeaks regions contain one or more modification sites? What about Oct4?  Yes. There are roughly three peaks, which indicate the possibility of\nhaving more than one modification sites in this region.  For Oct4, no peak can be observed.", 
            "title": "Viewing results with the Genome browser"
        }, 
        {
            "location": "/modules/btp-module-chip-seq/chip-seq-draft-SL_ST/#advanced-session", 
            "text": "Ensembl Visualization  Launch a web browser and go to the Ensembl website at http://www.ensembl.org/index.html . Choose the genome of interest (in\nthis case, mouse) on the left side of the page, browse to any location\nin the genome or click one of the demo links provided on the web page.\nClick on the  Add your data  link on the left, then choose  Attach\nremote file .  Wig files are large so are inconvenient for uploading directly to the\nEnsemble Genome browser. Instead, we will convert it to an indexed\nbinary format and put this into a web accessible place such as on a\nHTTP, HTTPS, or FTP server. This makes all the browsing process much\nfaster. Detailed instructions for generating a bigWig from a wig type\nfile can be found at:  http://genome.ucsc.edu/goldenPath/help/bigWig.html .  We have generated bigWig files in advance for you to upload to the\nEnsembl browser. They are at the following URL: http://www.ebi.ac.uk/~remco/ChIP-Seq_course/Oct4.bw  To visualise the data:    Paste the location above in the field File URL.    Choose data format bigWig.    Choose some informative name and in the next window choose the\n    colour of your preference.    Click  Save  and close the window to return to the genome browser.    Repeat the process for the gfp control sample, located at:  http://www.ebi.ac.uk/~remco/ChIP-Seq_course/gfp.bw .  If can not see your tracks: Click on \u2019Configure this page\u2019 in left\npanel. In \u2019Configure region Overview\u2019 tab click on \u2019Ypur data\u2019 in left\npanel. Check the boxes in \u2019Enable/Disable all tracks\u2019 for you *.bw\nfiles by selecting \u2019wiggle plot in the pop up menu.  After uploading, choose  Configure this page , and under  Your data \ntick both boxes. Closing the window will save these changes.  Go to a region on chromosome 1 (e.g.  1 : 34823162 - 35323161 ), and zoom in\nand out to view the signal and peak regions. Be aware that the y-axis of\neach track is auto-scaled independently of each other, so bigger-looking\npeaks may not actually be bigger! Always look at the values on the left\nhand side axis.  MACS generates its peak files in a file format called bed file. This is\na simple text format containing genomic locations, specified by\nchromosome, begin and end positions, and some more optional information.  See  http://genome.ucsc.edu/FAQ/FAQformat.html#format1  for details.  Bed files can also be uploaded to the Ensembl browser.", 
            "title": "Advanced Session"
        }, 
        {
            "location": "/modules/btp-module-chip-seq/chip-seq-draft-SL_ST/#motif-analysis", 
            "text": "It is often interesting to find out whether we can identify\ntranscription factor binding sites (TFBSs) from the input DNA sequences.\nTFBSs which share a similar sequence pattern (motif) are presumed to\nhave biological functions. Here we introduce three motif discovery tools\nnamed Trawler, RSAT peak-motifs, and MEME-ChIP, which use different\nsearching algorithms. This might lead to varying results. Hence, it is\ngenerally a good idea to use several tools to identify TFBSs. The more\ntools confirm the same result, the better, which is also called an\northogonal approach. Eventually, you probably want to validate your in\nsilico findings in vivo or in vitro.", 
            "title": "Motif analysis"
        }, 
        {
            "location": "/modules/btp-module-chip-seq/chip-seq-draft-SL_ST/#motif-discovery-with-trawler", 
            "text": "Trawler is a fast, yet accurate motif discovery tool that accepts both,\nBED and FASTA files as input file formats. BED files are generated when\nyou process and analyse your NGS data. Thus, it is handy to use them\ndirectly in Trawler. Other tools do not accept BED files as input. With\nTrawler, BED files can be converted into FASTA files that can then be\nused for other motif discovery tools (e.g. RSAT peak-motifs and MEME\nChIP).  Running Trawler    Go to the website  https://trawler.erc.monash.edu.au    Run Trawler with BED file as input, and wait for the results\n          Download both sample and background files in FASTA format. Right\n    click and choose: \u2019Save the link as \u2019     Which motif was found to be the most similar to your motif?  Sox2", 
            "title": "Motif discovery with Trawler"
        }, 
        {
            "location": "/modules/btp-module-chip-seq/chip-seq-draft-SL_ST/#optional-motif-discovery-with-rsat-peak-motif", 
            "text": "The motif discovery tool RSAT peak-motifs uses FASTA files as input. An\noptional background can be uploaded in FASTA format. RSAT peak-motifs\nautomatically outputs motifs of 6 and 7 nucleotides length (two separate\nfiles). While still accurate, the running time is longer compared to\nTrawler (up to 20 minutes depending on the size of the files).  Running RSAT peak-motifs    Go to the website\n     http://floresta.eead.csic.es/rsat/peak-motifs~f~orm.cgi    Upload input and background FASTA files just downloaded from Trawler\n        Wait until the discovery finishes.", 
            "title": "Optional: Motif discovery with RSAT peak-motif"
        }, 
        {
            "location": "/modules/btp-module-chip-seq/chip-seq-draft-SL_ST/#optional-motif-discovery-with-meme-chip", 
            "text": "MEME-ChIP is a popular motif discovery tool and part of MEME Suite.\nMEME-ChIP accepts input files in FASTA format. It is not necessary to\nupload your own background because MEME-ChIP uses its own. Although\nMEME-ChIP is one of the most popular motif discovery tools, the\nidentified motifs are not very accurate and the motif search might take\nup to one hour. MEME-ChIP outputs the three motifs with the lowest\nE-Value.  Running MEME-ChIP    Go to the website  http://meme-suite.org/tools/meme-chip    Upload input and background FASTA files just downloaded from Trawler\n        Wait until the discovery finishes.", 
            "title": "Optional: Motif discovery with MEME-ChIP"
        }, 
        {
            "location": "/modules/btp-module-chip-seq/chip-seq-draft-SL_ST/#reference", 
            "text": "Chen, X et al.: Integration of external signaling pathways with the core\ntranscriptional network in embryonic stem cells. Cell 133:6, 1106-17\n(2008).", 
            "title": "Reference"
        }, 
        {
            "location": "/modules/btp-module-denovo-canu/denovo_canu/", 
            "text": "Pacbio reads: assembly with command line tools\n\n\nKeywords: de novo assembly, PacBio, PacificBiosciences, Illumina, command line, Canu, Circlator, BWA, Spades, Pilon, Microbial Genomics Virtual Laboratory\n\n\nThis tutorial demonstrates how to use long Pacbio sequence reads to assemble a bacterial genome, including correcting the assembly with short Illumina reads.\n\n\nResources\n\n\nTools (and versions) used in this tutorial include:\n\n\n\n\ncanu 1.5 [recently updated]\n\n\ninfoseq and sizeseq (part of EMBOSS) 6.6.0.0\n\n\ncirclator 1.5.1 [recently updated]\n\n\nbwa 0.7.15\n\n\nsamtools 1.3.1\n\n\nmakeblastdb and blastn (part of blast) 2.4.0+\n\n\npilon 1.20\n\n\n\n\nLearning objectives\n\n\nAt the end of this tutorial, be able to:\n\n\n\n\nAssemble and circularise a bacterial genome from PacBio sequence data.\n\n\nRecover small plasmids missed by long read sequencing, using Illumina data\n\n\nExplore the effect of polishing assembled sequences with a different data set.\n\n\n\n\nOverview\n\n\nSimplified version of workflow:\n\n\n\n\nGet data\n\n\nThe files we need are:\n\n\n\n\npacbio.fastq.gz\n : the PacBio reads\n\n\nillumina_R1.fastq.gz\n: the Illumina forward reads\n\n\nillumina_R2.fastq.gz\n: the Illumina reverse reads\n\n\n\n\nIf you already have the files, skip forward to next section, \nAssemble\n.\n\n\nOtherwise, this section has information about how to find and move the files:\n\n\nPacBio files\n\n\n\n\nOpen the command line. \n\n\nNavigate to or create the directory in which you want to work.\n\n\nIf the files are already on your server, you can symlink by using\n\n\n\n\n1\nln -s real_file_path [e.g. data/sample_name/pacbio1.fastq.gz] chosen_symlink_name [e.g. pacbio1.fastq.gz]\n\n\n\n\n\n\n\n\n\n\nAlternatively, obtain the input files from elsewhere, e.g. from the BPA portal. (You will need a password.)\n\n\n\n\n\n\nPacbio files are often stored in the format:\n\n\n\n\nSample_name/Cell_name/Analysis_Results/long_file_name_1.fastq.gz\n\n\n\n\n\n\n\n\nWe will use the \nlongfilename.subreads.fastq.gz\n files.\n\n\n\n\n\n\nThe reads are usually split into three separate files because they are so large.\n\n\n\n\n\n\nRight click on the first \nsubreads.fastq.gz\n file and \ncopy link address\n.\n\n\n\n\n\n\nIn the command line, type:\n\n\n\n\n\n\n1\nwget --user username --password password [paste link URL for file]\n\n\n\n\n- Repeat for the other two \nsubreads.fastq.gz\n files.\n- Join the files:\n\n1\ncat pacbio*.fastq.gz \n pacbio.fastq.gz\n\n\n\n\n- If the files are not gzipped, type:\n\n1\ncat pacbio*.fastq | gzip \n pacbio.fastq.gz\n\n\n\n\n\nIllumina files\n\n\n\n\nWe will also use 2 x Illumina (Miseq) fastq.gz files.\n\n\nThese are the \nR1.fastq.gz\n and \nR2.fastq.gz\n files.\n\n\nSymlink or \nwget\n these files as described above for PacBio files.\n\n\nShorten the name of each of these files:\n\n\n\n\n1\n2\nmv longfilename_R1.fastq.gz illumina_R1.fastq.gz\nmv longfilename_R2.fastq.gz illumina_R2.fastq.gz\n\n\n\n\n\n\nSample information\n\n\nThe sample used in this tutorial is a gram-positive bacteria called \nStaphylococcus aureus\n (sample number 25747). This particular sample is from a strain that is resistant to the antibiotic methicillin (a type of penicillin). It is also called MRSA: methicillin-resistant \nStaphylococcus aureus\n. It was isolated from (human) blood and caused bacteraemia, an infection of the bloodstream.\n\n\nAssemble\n\n\n\n\nWe will use the assembly software called \nCanu\n.\n\n\nRun Canu with these commands:\n\n\n\n\n1\ncanu -p canu -d canu_outdir genomeSize=2.8m -pacbio-raw pacbio.fastq.gz\n\n\n\n\n\n\n\n\nthe first \ncanu\n tells the program to run\n\n\n-p canu\n names prefix for output files (\ncanu\n)\n\n\n-d canu_outdir\n names output directory (\ncanu_outdir\n)\n\n\n\n\ngenomeSize\n only has to be approximate.\n\n\n\n\ne.g. \nStaphylococcus aureus\n, 2.8m\n\n\ne.g. \nStreptococcus pyogenes\n, 1.8m\n\n\n\n\n\n\n\n\nCanu will correct, trim and assemble the reads.\n\n\n\n\nVarious output will be displayed on the screen.\n\n\n\n\nCheck the output\n\n\nMove into \ncanu_outdir\n and \nls\n to see the output files.\n\n\n\n\nThe \ncanu.contigs.fasta\n are the assembled sequences.\n\n\nThe \ncanu.unassembled.fasta\n are the reads that could not be assembled.\n\n\nThe \ncanu.correctedReads.fasta.gz\n are the corrected Pacbio reads that were used in the assembly.\n\n\nThe \ncanu.file.gfa\n is the graph of the assembly.\n\n\nDisplay summary information about the contigs: (\ninfoseq\n is a tool from \nEMBOSS\n)\n\n\n\n\n1\ninfoseq canu.contigs.fasta\n\n\n\n\n\n\n\n\nThis will show the contigs found by Canu. e.g.,\n\n\n\n\n1\n    - tig00000001   2851805\n\n\n\n\n\n\nThis looks like a chromosome of approximately 2.8 million bases.\n\n\nThis matches what we would expect for this sample. For other data, Canu may not be able to join all the reads into one contig, so there may be several contigs in the output. Also, the sample may contain some plasmids and these may be found full or partially by Canu as additional contigs.  \n\n\nChange Canu parameters if required\n\n\nIf the assembly is poor with many contigs, re-run Canu with extra sensitivity parameters; e.g.\n\n1\ncanu -p prefix -d outdir corMhapSensitivity=high corMinCoverage=0 genomeSize=2.8m -pacbio-raw pacbio.fastq.gz\n\n\n\n\n\nQuestions\n\n\nQ: How do long- and short-read assembly methods differ? A: short reads: De Bruijn graphs; long reads: a move back towards simpler overlap-layout-consensus methods.\n\n\nQ: Where can we find out the what the approximate genome size should be for the species being assembled? A: NCBI Genomes - enter species name - click on Genome Assembly and Annotation report - sort table by clicking on the column header Size (Mb) - look at range of sizes in this column.\n\n\nQ: In the assembly output, what are the unassembled reads? Why are they there?\n\n\nQ: What are the corrected reads? How did canu correct the reads?\n\n\nQ: Where could you view the output .gfa and what would it show?\n\n\nTrim and circularise\n\n\nRun Circlator\n\n\nCirclator identifies and trims overhangs (on chromosomes and plasmids) and orients the start position at an appropriate gene (e.g. dnaA). It takes in the assembled contigs from Canu, as well as the corrected reads prepared by Canu.\n\n\nOverhangs are shown in blue:\n\n\n\n\nAdapted from Figure 1. Hunt et al. Genome Biology 2015\n\n\nMove back into your main analysis folder.\n\n\nRun Circlator:\n\n\n1\ncirclator all --threads 8 --verbose canu_outdir/canu.contigs.fasta canu_outdir/canu.correctedReads.fasta.gz circlator_outdir\n\n\n\n\n\n\n\n\n--threads\n is the number of cores: change this to an appropriate number\n\n\n--verbose\n prints progress information to the screen\n\n\ncanu_outdir/canu.contigs.fasta\n is the file path to the input Canu assembly\n\n\ncanu_outdir/canu.correctedReads.fasta.gz\n is the file path to the corrected Pacbio reads - note, fastA not fastQ\n\n\ncirclator_outdir\n is the name of the output directory.\n\n\n\n\nSome output will print to screen. When finished, it should say \nCircularized x of x contig(s)\n.\n\n\nCheck the output\n\n\nMove into the \ncirclator_outdir\n directory and \nls\n to list files.\n\n\nWere the contigs circularised?\n :\n\n\n1\nless 04.merge.circularise.log\n\n\n\n\n\n\n\n\nYes, the contig was circularised (last column).\n\n\nType \nq\n to exit.\n\n\n\n\nWhere were the contigs oriented (which gene)?\n :\n\n\n1\nless 06.fixstart.log\n\n\n\n\n- Look in the \ngene_name\n column.\n- The contig has been oriented at tr|A0A090N2A8|A0A090N2A8_STAAU, which is another name for dnaA. \n This is typically used as the start of bacterial chromosome sequences.\n\n\nWhat are the trimmed contig sizes?\n :\n\n\n1\ninfoseq 06.fixstart.fasta\n\n\n\n\n\n\n\n\ntig00000001 2823331 (28564 bases trimmed)\n\n\n\n\nThis trimmed part is the overlap.\n\n\nRe-name the contigs file\n:\n\n\n\n\nThe trimmed contigs are in the file called \n06.fixstart.fasta\n.\n\n\nRe-name it \ncontig1.fasta\n:\n\n\n\n\n1\nmv 06.fixstart.fasta contig1.fasta\n\n\n\n\n\n\nOpen this file in a text editor (e.g. nano: \nnano contig1.fasta\n) and change the header to \nchromosome\n.\n\n\nMove the file back into the main folder (\nmv contig1.fasta ../\n).\n\n\nOptions\n\n\nIf all the contigs have not circularised with Circlator, an option is to change the \n--b2r_length_cutoff\n setting to approximately 2X the average read depth.\n\n\nQuestions\n\n\nQ: Were all the contigs circularised? Why/why not?\n\n\nQ: Circlator can set the start of the sequence at a particular gene. Which gene does it use? Is this appropriate for all contigs? A: Uses dnaA for the chromosomal contig. For other contigs, uses a centrally-located gene. However, ideally, plasmids would be oriented on a gene such as repA. It is possible to provide a file to Circlator to do this.\n\n\nFind smaller plasmids\n\n\nPacbio reads are long, and may have been longer than small plasmids. We will look for any small plasmids using the Illumina reads.\n\n\nThis section involves several steps:\n\n\n\n\nUse the Canu+Circlator output of a trimmed assembly contig.\n\n\nMap all the Illumina reads against this Pacbio-assembled contig.\n\n\nExtract any reads that \ndidn\nt\n map and assemble them together: this could be a plasmid, or part of a plasmid.\n\n\nLook for overhang: if found, trim.\n\n\n\n\nAlign Illumina reads to the PacBio contig\n\n\n\n\nIndex the contigs file:\n\n\n\n\n1\nbwa index contig1.fasta\n\n\n\n\n\n\n\n\nAlign Illumina reads using using bwa mem:\n\n\n\n\n1\nbwa mem -t 8 contig1.fasta illumina_R1.fastq.gz illumina_R2.fastq.gz | samtools sort \n aln.bam\n\n\n\n\n\n\n\n\nbwa mem\n is the alignment tool\n\n\n-t 8\n is the number of cores: choose an appropriate number\n\n\ncontig1.fasta\n is the input assembly file\n\n\nillumina_R1.fastq.gz illumina_R2.fastq.gz\n are the Illumina reads\n\n\n| samtools sort\n pipes the output to samtools to sort\n\n\n aln.bam\n sends the alignment to the file \naln.bam\n\n\n\n\nExtract unmapped Illumina reads\n\n\n\n\nIndex the alignment file:\n\n\n\n\n1\nsamtools index aln.bam\n\n\n\n\n\n\n\n\nExtract the fastq files from the bam alignment - those reads that were unmapped to the Pacbio alignment - and save them in various \nunmapped\n files:\n\n\n\n\n1\nsamtools fastq -f 4 -1 unmapped.R1.fastq -2 unmapped.R2.fastq -s unmapped.RS.fastq aln.bam\n\n\n\n\n\n\n\n\nfastq\n is a command that coverts a \n.bam\n file into fastq format\n\n\n-f 4\n : only output unmapped reads\n\n\n-1\n : put R1 reads into a file called \nunmapped.R1.fastq\n\n\n-2\n : put R2 reads into a file called \nunmapped.R2.fastq\n\n\n-s\n : put singleton reads into a file called \nunmapped.RS.fastq\n\n\naln.bam\n : input alignment file\n\n\n\n\nWe now have three files of the unampped reads: \n unmapped.R1.fastq\n, \n unmapped.R2.fastq\n, \n unmapped.RS.fastq\n.\n\n\nAssemble the unmapped reads\n\n\n\n\nAssemble with Spades:\n\n\n\n\n1\nspades.py -1 unmapped.R1.fastq -2 unmapped.R2.fastq -s unmapped.RS.fastq --careful --cov-cutoff auto -o spades_assembly\n\n\n\n\n\n\n\n\n-1\n is input file forward\n\n\n-2\n is input file reverse\n\n\n-s\n is unpaired\n\n\n--careful\n minimizes mismatches and short indels\n\n\n--cov-cutoff auto\n computes the coverage threshold (rather than the default setting, \noff\n)\n\n\n-o\n is the output directory\n\n\n\n\nMove into the output directory (\nspades_assembly\n) and look at the contigs:\n\n\n1\ninfoseq contigs.fasta\n\n\n\n\n- 78 contigs were assembled, with the max length of 2250 (the first contig).\n\n- All other nodes are \n 650kb so we will disregard as they are unlikely to be plasmids.\n- Type \nq\n to exit.\n- We will extract the first sequence (NODE_1):\n\n\n1\nsamtools faidx contigs.fasta\n\n\n\n\n\n\n1\nsamtools faidx contigs.fasta NODE_1_length_2550_cov_496.613 \n contig2.fasta\n\n\n\n\n\n\n\n\nThis is now saved as \ncontig2.fasta\n\n\nOpen in nano and change header to \nplasmid\n.\n\n\n\n\nTrim the plasmid\n\n\nTo trim any overhang on this plasmid, we will blast the start of contig2 against itself.\n\n\n\n\nTake the start of the contig:\n\n\n\n\n1\nhead -n 10 contig2.fasta \n contig2.fa.head\n\n\n\n\n\n\n\n\nWe want to see if it matches the end (overhang).\n\n\nFormat the assembly file for blast:\n\n\n\n\n1\nmakeblastdb -in contig2.fasta -dbtype nucl\n\n\n\n\n\n\n\n\n\n\nBlast the start of the assembly (.head file) against all of the assembly:\n\n1\nblastn -query contig2.fa.head -db contig2.fasta -evalue 1e-3 -dust no -out contig2.bls\n\n\n\n\n\n\n\n\n\nLook at \ncontig2.bls\n to see hits:\n\n1\nless contig2.bls\n\n\n\n\n\n\n\n\n\nThe first hit is at start, as expected.\n\n\n\n\nThe second hit is at 2474 all the way to the end - 2550.\n\n\nThis is the overhang.\n\n\nTrim to position 2473.\n\n\nIndex the plasmid.fa file:\n\n\n\n\n1\nsamtools faidx contig2.fasta\n\n\n\n\n\n\n\n\nTrim:\n\n1\nsamtools faidx contig2.fasta plasmid:1-2473 \n plasmid.fa.trimmed\n\n\n\n\n\n\n\nplasmid\n is the name of the contig, and we want the sequence from 1-2473.\n\n\n\n\n\n\nOpen this file in nano (\nnano plasmid.fa.trimmed\n) and change the header to \nplasmid\n, save.\n\n\n\n\nWe now have a trimmed plasmid.\n\n\nMove file back into main folder:\n\n\n\n\n1\ncp plasmid.fa.trimmed ../\n\n\n\n\n\n\n\n\nMove into the main folder.\n\n\n\n\nPlasmid contig orientation\n\n\nThe bacterial chromosome was oriented at the gene dnaA. Plasmids are often oriented at the replication gene, but this is highly variable and there is no established convention. Here we will orient the plasmid at a gene found by Prodigal, in Circlator:\n\n\n1\ncirclator fixstart plasmid.fa.trimmed plasmid_fixstart\n\n\n\n\n\n\n\n\nfixstart\n is an option in Circlator just to orient a sequence.\n\n\nplasmid.fa.trimmed\n is our small plasmid.\n\n\nplasmid_fixstart\n is the prefix for the output files.\n\n\n\n\nView the output:\n\n\n1\nless plasmid_fixstart.log\n\n\n\n\n\n\n\n\nThe plasmid has been oriented at a gene predicted by Prodigal, and the break-point is at position 1200.\n\n\nChange the file name:\n\n\n\n\n1\ncp plasmid_fixstart.fasta contig2.fasta\n\n\n\n\n\n\n\n\n\nCollect contigs\n\n\n1\ncat contig1.fasta contig2.fasta \n genome.fasta\n\n\n\n\n\n\n\n\n\n\nSee the contigs and sizes:\n\n1\ninfoseq genome.fasta\n\n\n\n\n\n\n\n\n\nchromosome: 2823331\n\n\n\n\nplasmid: 2473\n\n\n\n\nQuestions\n\n\nQ: Why is this section so complicated? A: Finding small plasmids is difficult for many reasons! This paper has a nice summary: On the (im)possibility to reconstruct plasmids from whole genome short-read sequencing data. doi: \nhttps://doi.org/10.1101/086744\n\n\nQ: Why can PacBio sequencing miss small plasmids? A: Library prep size selection\n\n\nQ: We extract unmapped Illumina reads and assemble these to find small plasmids. What could they be missing? A: Repeats that have mapped to the PacBio assembly.\n\n\nQ: How do you find a plasmid in a Bandage graph? A: It is probably circular, matches the size of a known plasmid, has a rep gene\n\n\nQ: Are there easier ways to find plasmids? A: Possibly. One option is the program called Unicycler which may automate many of these steps. \nhttps://github.com/rrwick/Unicycler\n\n\nCorrect\n\n\nWe will correct the Pacbio assembly with Illumina reads.\n\n\nMake an alignment file\n\n\n\n\nAlign the Illumina reads (R1 and R2) to the draft PacBio assembly, e.g. \ngenome.fasta\n:\n\n\n\n\n1\n2\nbwa index genome.fasta\nbwa mem -t 32 genome.fasta illumina_R1.fastq.gz illumina_R2.fastq.gz | samtools sort \n aln.bam\n\n\n\n\n\n\n\n\n\n\n-t\n is the number of cores: set this to an appropriate number. (To find out how many you have, \ngrep -c processor /proc/cpuinfo\n).\n\n\n\n\n\n\nIndex the files:\n\n\n\n\n\n\n1\n2\nsamtools index aln.bam\nsamtools faidx genome.fasta\n\n\n\n\n\n\n\n\nNow we have an alignment file to use in Pilon: \naln.bam\n\n\n\n\nRun Pilon\n\n\n\n\nRun:\n\n\n\n\n1\npilon --genome genome.fasta --frags aln.bam --output pilon1 --fix all --mindepth 0.5 --changes --verbose --threads 32\n\n\n\n\n\n\n\n\n--genome\n is the name of the input assembly to be corrected\n\n\n--frags\n is the alignment of the reads against the assembly\n\n\n--output\n is the name of the output prefix\n\n\n--fix\n is an option for types of corrections\n\n\n--mindepth\n gives a minimum read depth to use\n\n\n--changes\n produces an output file of the changes made\n\n\n--verbose\n prints information to the screen during the run\n\n\n--threads\n : set this to an appropriate number\n\n\n\n\nLook at the changes file:\n\n\n1\nless pilon1.changes\n\n\n\n\n\n\nExample:\n\n\n\n\nLook at the details of the fasta file:\n\n\n1\ninfoseq pilon1.fasta\n\n\n\n\n\n\n\n\nchromosome - 2823340 (net +9 bases)\n\n\nplasmid - 2473 (no change)\n\n\n\n\nOption:\n\n\nIf there are many changes, run Pilon again, using the \npilon1.fasta\n file as the input assembly, and the Illumina reads to correct.\n\n\nGenome output\n\n\n\n\nChange the file name:\n\n\n\n\n1\ncp pilon1.fasta assembly.fasta\n\n\n\n\n\n\n\n\nWe now have the corrected genome assembly of \nStaphylococcus aureus\n in .fasta format, containing a chromosome and a small plasmid.  \n\n\n\n\nQuestions\n\n\nQ: Why don\nt we correct earlier in the assembly process? A: We need to circularise the contigs and trim overhangs first.\n\n\nQ: Why can we use some reads (Illumina) to correct other reads (PacBio) ? A: Illumina reads have higher accuracy\n\n\nQ: Could we just use PacBio reads to assemble the genome? A: Yes, if accuracy adequate.\n\n\nShort-read assembly: a comparison\n\n\nSo far, we have assembled the long PacBio reads into one contig (the chromosome) and found an additional plasmid in the Illumina short reads.\n\n\nIf we only had Illumina reads, we could also assemble these using the tool Spades.\n\n\nYou can try this here or try it later on your own data.\n\n\nGet data\n\n\nWe will use the same Illumina data as we used above:\n\n\n\n\nillumina_R1.fastq.gz\n: the Illumina forward reads\n\n\nillumina_R2.fastq.gz\n: the Illumina reverse reads\n\n\n\n\nAssemble\n\n\nRun Spades:\n\n\n1\nspades.py -1 illumina_R1.fastq.gz -2 illumina_R2.fastq.gz --careful --cov-cutoff auto -o spades_assembly_all_illumina\n\n\n\n\n\n\n\n\n-1\n is input file of forward reads\n\n\n-2\n is input file of reverse reads\n\n\n--careful\n minimizes mismatches and short indels\n\n\n--cov-cutoff auto\n computes the coverage threshold (rather than the default setting, \noff\n)\n\n\n-o\n is the output directory\n\n\n\n\nResults\n\n\nMove into the output directory and look at the contigs:\n\n\n1\ninfoseq contigs.fasta\n\n\n\n\n\n\nQuestions\n\n\nHow many contigs were found by Spades?\n\n\n\n\nmany\n\n\n\n\nHow does this compare to the number of contigs found by assembling the long read data with Canu?\n\n\n\n\nmany more.\n\n\n\n\nDoes it matter that an assembly is in many contigs? \n\n\n\n\n\n\nYes\n\n\n\n\n\n\nbroken genes =\n missing/incorrect annotations\n\n\n\n\n\n\nless information about structure: e.g. number of plasmids\n\n\n\n\n\n\nNo\n\n\n\n\n\n\nMany or all genes may still be annotated\n\n\n\n\nGene location is useful (e.g. chromosome, plasmid1) but not always essential (e.g. presence/absence of particular resistance genes)\n\n\n\n\nHow can we get more information about the assembly from Spades?\n\n\n\n\nLook at the assembly graph \nassembly_graph.fastg\n, e.g. in the program Bandage. This shows how contigs are related, albeit with ambiguity in some places. \n\n\n\n\nNext\n\n\nFurther analyses\n\n\n\n\nAnnotate with Prokka.\n\n\nComparative genomics, e.g. with Roary.\n\n\n\n\nLinks\n\n\n\n\nDetails of bas.h5 files\n\n\nCanu \nmanual\n and \ngitub repository\n\n\nCirclator \narticle\n and \ngithub repository\n\n\nPilon \narticle\n and \ngithub repository\n\n\nNotes on \nfinishing\n and \nevaluating\n assemblies.", 
            "title": "DeNovo Canu"
        }, 
        {
            "location": "/modules/btp-module-denovo-canu/denovo_canu/#pacbio-reads-assembly-with-command-line-tools", 
            "text": "Keywords: de novo assembly, PacBio, PacificBiosciences, Illumina, command line, Canu, Circlator, BWA, Spades, Pilon, Microbial Genomics Virtual Laboratory  This tutorial demonstrates how to use long Pacbio sequence reads to assemble a bacterial genome, including correcting the assembly with short Illumina reads.", 
            "title": "Pacbio reads: assembly with command line tools"
        }, 
        {
            "location": "/modules/btp-module-denovo-canu/denovo_canu/#resources", 
            "text": "Tools (and versions) used in this tutorial include:   canu 1.5 [recently updated]  infoseq and sizeseq (part of EMBOSS) 6.6.0.0  circlator 1.5.1 [recently updated]  bwa 0.7.15  samtools 1.3.1  makeblastdb and blastn (part of blast) 2.4.0+  pilon 1.20", 
            "title": "Resources"
        }, 
        {
            "location": "/modules/btp-module-denovo-canu/denovo_canu/#learning-objectives", 
            "text": "At the end of this tutorial, be able to:   Assemble and circularise a bacterial genome from PacBio sequence data.  Recover small plasmids missed by long read sequencing, using Illumina data  Explore the effect of polishing assembled sequences with a different data set.", 
            "title": "Learning objectives"
        }, 
        {
            "location": "/modules/btp-module-denovo-canu/denovo_canu/#overview", 
            "text": "Simplified version of workflow:", 
            "title": "Overview"
        }, 
        {
            "location": "/modules/btp-module-denovo-canu/denovo_canu/#get-data", 
            "text": "The files we need are:   pacbio.fastq.gz  : the PacBio reads  illumina_R1.fastq.gz : the Illumina forward reads  illumina_R2.fastq.gz : the Illumina reverse reads   If you already have the files, skip forward to next section,  Assemble .  Otherwise, this section has information about how to find and move the files:", 
            "title": "Get data"
        }, 
        {
            "location": "/modules/btp-module-denovo-canu/denovo_canu/#pacbio-files", 
            "text": "Open the command line.   Navigate to or create the directory in which you want to work.  If the files are already on your server, you can symlink by using   1 ln -s real_file_path [e.g. data/sample_name/pacbio1.fastq.gz] chosen_symlink_name [e.g. pacbio1.fastq.gz]     Alternatively, obtain the input files from elsewhere, e.g. from the BPA portal. (You will need a password.)    Pacbio files are often stored in the format:   Sample_name/Cell_name/Analysis_Results/long_file_name_1.fastq.gz     We will use the  longfilename.subreads.fastq.gz  files.    The reads are usually split into three separate files because they are so large.    Right click on the first  subreads.fastq.gz  file and  copy link address .    In the command line, type:    1 wget --user username --password password [paste link URL for file]  \n- Repeat for the other two  subreads.fastq.gz  files.\n- Join the files: 1 cat pacbio*.fastq.gz   pacbio.fastq.gz  \n- If the files are not gzipped, type: 1 cat pacbio*.fastq | gzip   pacbio.fastq.gz", 
            "title": "PacBio files"
        }, 
        {
            "location": "/modules/btp-module-denovo-canu/denovo_canu/#illumina-files", 
            "text": "We will also use 2 x Illumina (Miseq) fastq.gz files.  These are the  R1.fastq.gz  and  R2.fastq.gz  files.  Symlink or  wget  these files as described above for PacBio files.  Shorten the name of each of these files:   1\n2 mv longfilename_R1.fastq.gz illumina_R1.fastq.gz\nmv longfilename_R2.fastq.gz illumina_R2.fastq.gz", 
            "title": "Illumina files"
        }, 
        {
            "location": "/modules/btp-module-denovo-canu/denovo_canu/#sample-information", 
            "text": "The sample used in this tutorial is a gram-positive bacteria called  Staphylococcus aureus  (sample number 25747). This particular sample is from a strain that is resistant to the antibiotic methicillin (a type of penicillin). It is also called MRSA: methicillin-resistant  Staphylococcus aureus . It was isolated from (human) blood and caused bacteraemia, an infection of the bloodstream.", 
            "title": "Sample information"
        }, 
        {
            "location": "/modules/btp-module-denovo-canu/denovo_canu/#assemble", 
            "text": "We will use the assembly software called  Canu .  Run Canu with these commands:   1 canu -p canu -d canu_outdir genomeSize=2.8m -pacbio-raw pacbio.fastq.gz    the first  canu  tells the program to run  -p canu  names prefix for output files ( canu )  -d canu_outdir  names output directory ( canu_outdir )   genomeSize  only has to be approximate.   e.g.  Staphylococcus aureus , 2.8m  e.g.  Streptococcus pyogenes , 1.8m     Canu will correct, trim and assemble the reads.   Various output will be displayed on the screen.", 
            "title": "Assemble"
        }, 
        {
            "location": "/modules/btp-module-denovo-canu/denovo_canu/#check-the-output", 
            "text": "Move into  canu_outdir  and  ls  to see the output files.   The  canu.contigs.fasta  are the assembled sequences.  The  canu.unassembled.fasta  are the reads that could not be assembled.  The  canu.correctedReads.fasta.gz  are the corrected Pacbio reads that were used in the assembly.  The  canu.file.gfa  is the graph of the assembly.  Display summary information about the contigs: ( infoseq  is a tool from  EMBOSS )   1 infoseq canu.contigs.fasta    This will show the contigs found by Canu. e.g.,   1     - tig00000001   2851805   This looks like a chromosome of approximately 2.8 million bases.  This matches what we would expect for this sample. For other data, Canu may not be able to join all the reads into one contig, so there may be several contigs in the output. Also, the sample may contain some plasmids and these may be found full or partially by Canu as additional contigs.", 
            "title": "Check the output"
        }, 
        {
            "location": "/modules/btp-module-denovo-canu/denovo_canu/#change-canu-parameters-if-required", 
            "text": "If the assembly is poor with many contigs, re-run Canu with extra sensitivity parameters; e.g. 1 canu -p prefix -d outdir corMhapSensitivity=high corMinCoverage=0 genomeSize=2.8m -pacbio-raw pacbio.fastq.gz", 
            "title": "Change Canu parameters if required"
        }, 
        {
            "location": "/modules/btp-module-denovo-canu/denovo_canu/#questions", 
            "text": "Q: How do long- and short-read assembly methods differ? A: short reads: De Bruijn graphs; long reads: a move back towards simpler overlap-layout-consensus methods.  Q: Where can we find out the what the approximate genome size should be for the species being assembled? A: NCBI Genomes - enter species name - click on Genome Assembly and Annotation report - sort table by clicking on the column header Size (Mb) - look at range of sizes in this column.  Q: In the assembly output, what are the unassembled reads? Why are they there?  Q: What are the corrected reads? How did canu correct the reads?  Q: Where could you view the output .gfa and what would it show?", 
            "title": "Questions"
        }, 
        {
            "location": "/modules/btp-module-denovo-canu/denovo_canu/#trim-and-circularise", 
            "text": "", 
            "title": "Trim and circularise"
        }, 
        {
            "location": "/modules/btp-module-denovo-canu/denovo_canu/#run-circlator", 
            "text": "Circlator identifies and trims overhangs (on chromosomes and plasmids) and orients the start position at an appropriate gene (e.g. dnaA). It takes in the assembled contigs from Canu, as well as the corrected reads prepared by Canu.  Overhangs are shown in blue:   Adapted from Figure 1. Hunt et al. Genome Biology 2015  Move back into your main analysis folder.  Run Circlator:  1 circlator all --threads 8 --verbose canu_outdir/canu.contigs.fasta canu_outdir/canu.correctedReads.fasta.gz circlator_outdir    --threads  is the number of cores: change this to an appropriate number  --verbose  prints progress information to the screen  canu_outdir/canu.contigs.fasta  is the file path to the input Canu assembly  canu_outdir/canu.correctedReads.fasta.gz  is the file path to the corrected Pacbio reads - note, fastA not fastQ  circlator_outdir  is the name of the output directory.   Some output will print to screen. When finished, it should say  Circularized x of x contig(s) .", 
            "title": "Run Circlator"
        }, 
        {
            "location": "/modules/btp-module-denovo-canu/denovo_canu/#check-the-output_1", 
            "text": "Move into the  circlator_outdir  directory and  ls  to list files.  Were the contigs circularised?  :  1 less 04.merge.circularise.log    Yes, the contig was circularised (last column).  Type  q  to exit.   Where were the contigs oriented (which gene)?  :  1 less 06.fixstart.log  \n- Look in the  gene_name  column.\n- The contig has been oriented at tr|A0A090N2A8|A0A090N2A8_STAAU, which is another name for dnaA.   This is typically used as the start of bacterial chromosome sequences.  What are the trimmed contig sizes?  :  1 infoseq 06.fixstart.fasta    tig00000001 2823331 (28564 bases trimmed)   This trimmed part is the overlap.  Re-name the contigs file :   The trimmed contigs are in the file called  06.fixstart.fasta .  Re-name it  contig1.fasta :   1 mv 06.fixstart.fasta contig1.fasta   Open this file in a text editor (e.g. nano:  nano contig1.fasta ) and change the header to  chromosome .  Move the file back into the main folder ( mv contig1.fasta ../ ).", 
            "title": "Check the output"
        }, 
        {
            "location": "/modules/btp-module-denovo-canu/denovo_canu/#options", 
            "text": "If all the contigs have not circularised with Circlator, an option is to change the  --b2r_length_cutoff  setting to approximately 2X the average read depth.", 
            "title": "Options"
        }, 
        {
            "location": "/modules/btp-module-denovo-canu/denovo_canu/#questions_1", 
            "text": "Q: Were all the contigs circularised? Why/why not?  Q: Circlator can set the start of the sequence at a particular gene. Which gene does it use? Is this appropriate for all contigs? A: Uses dnaA for the chromosomal contig. For other contigs, uses a centrally-located gene. However, ideally, plasmids would be oriented on a gene such as repA. It is possible to provide a file to Circlator to do this.", 
            "title": "Questions"
        }, 
        {
            "location": "/modules/btp-module-denovo-canu/denovo_canu/#find-smaller-plasmids", 
            "text": "Pacbio reads are long, and may have been longer than small plasmids. We will look for any small plasmids using the Illumina reads.  This section involves several steps:   Use the Canu+Circlator output of a trimmed assembly contig.  Map all the Illumina reads against this Pacbio-assembled contig.  Extract any reads that  didn t  map and assemble them together: this could be a plasmid, or part of a plasmid.  Look for overhang: if found, trim.", 
            "title": "Find smaller plasmids"
        }, 
        {
            "location": "/modules/btp-module-denovo-canu/denovo_canu/#align-illumina-reads-to-the-pacbio-contig", 
            "text": "Index the contigs file:   1 bwa index contig1.fasta    Align Illumina reads using using bwa mem:   1 bwa mem -t 8 contig1.fasta illumina_R1.fastq.gz illumina_R2.fastq.gz | samtools sort   aln.bam    bwa mem  is the alignment tool  -t 8  is the number of cores: choose an appropriate number  contig1.fasta  is the input assembly file  illumina_R1.fastq.gz illumina_R2.fastq.gz  are the Illumina reads  | samtools sort  pipes the output to samtools to sort   aln.bam  sends the alignment to the file  aln.bam", 
            "title": "Align Illumina reads to the PacBio contig"
        }, 
        {
            "location": "/modules/btp-module-denovo-canu/denovo_canu/#extract-unmapped-illumina-reads", 
            "text": "Index the alignment file:   1 samtools index aln.bam    Extract the fastq files from the bam alignment - those reads that were unmapped to the Pacbio alignment - and save them in various  unmapped  files:   1 samtools fastq -f 4 -1 unmapped.R1.fastq -2 unmapped.R2.fastq -s unmapped.RS.fastq aln.bam    fastq  is a command that coverts a  .bam  file into fastq format  -f 4  : only output unmapped reads  -1  : put R1 reads into a file called  unmapped.R1.fastq  -2  : put R2 reads into a file called  unmapped.R2.fastq  -s  : put singleton reads into a file called  unmapped.RS.fastq  aln.bam  : input alignment file   We now have three files of the unampped reads:   unmapped.R1.fastq ,   unmapped.R2.fastq ,   unmapped.RS.fastq .", 
            "title": "Extract unmapped Illumina reads"
        }, 
        {
            "location": "/modules/btp-module-denovo-canu/denovo_canu/#assemble-the-unmapped-reads", 
            "text": "Assemble with Spades:   1 spades.py -1 unmapped.R1.fastq -2 unmapped.R2.fastq -s unmapped.RS.fastq --careful --cov-cutoff auto -o spades_assembly    -1  is input file forward  -2  is input file reverse  -s  is unpaired  --careful  minimizes mismatches and short indels  --cov-cutoff auto  computes the coverage threshold (rather than the default setting,  off )  -o  is the output directory   Move into the output directory ( spades_assembly ) and look at the contigs:  1 infoseq contigs.fasta  \n- 78 contigs were assembled, with the max length of 2250 (the first contig). \n- All other nodes are   650kb so we will disregard as they are unlikely to be plasmids.\n- Type  q  to exit.\n- We will extract the first sequence (NODE_1):  1 samtools faidx contigs.fasta   1 samtools faidx contigs.fasta NODE_1_length_2550_cov_496.613   contig2.fasta    This is now saved as  contig2.fasta  Open in nano and change header to  plasmid .", 
            "title": "Assemble the unmapped reads"
        }, 
        {
            "location": "/modules/btp-module-denovo-canu/denovo_canu/#trim-the-plasmid", 
            "text": "To trim any overhang on this plasmid, we will blast the start of contig2 against itself.   Take the start of the contig:   1 head -n 10 contig2.fasta   contig2.fa.head    We want to see if it matches the end (overhang).  Format the assembly file for blast:   1 makeblastdb -in contig2.fasta -dbtype nucl     Blast the start of the assembly (.head file) against all of the assembly: 1 blastn -query contig2.fa.head -db contig2.fasta -evalue 1e-3 -dust no -out contig2.bls     Look at  contig2.bls  to see hits: 1 less contig2.bls     The first hit is at start, as expected.   The second hit is at 2474 all the way to the end - 2550.  This is the overhang.  Trim to position 2473.  Index the plasmid.fa file:   1 samtools faidx contig2.fasta    Trim: 1 samtools faidx contig2.fasta plasmid:1-2473   plasmid.fa.trimmed    plasmid  is the name of the contig, and we want the sequence from 1-2473.    Open this file in nano ( nano plasmid.fa.trimmed ) and change the header to  plasmid , save.   We now have a trimmed plasmid.  Move file back into main folder:   1 cp plasmid.fa.trimmed ../    Move into the main folder.", 
            "title": "Trim the plasmid"
        }, 
        {
            "location": "/modules/btp-module-denovo-canu/denovo_canu/#plasmid-contig-orientation", 
            "text": "The bacterial chromosome was oriented at the gene dnaA. Plasmids are often oriented at the replication gene, but this is highly variable and there is no established convention. Here we will orient the plasmid at a gene found by Prodigal, in Circlator:  1 circlator fixstart plasmid.fa.trimmed plasmid_fixstart    fixstart  is an option in Circlator just to orient a sequence.  plasmid.fa.trimmed  is our small plasmid.  plasmid_fixstart  is the prefix for the output files.   View the output:  1 less plasmid_fixstart.log    The plasmid has been oriented at a gene predicted by Prodigal, and the break-point is at position 1200.  Change the file name:   1 cp plasmid_fixstart.fasta contig2.fasta", 
            "title": "Plasmid contig orientation"
        }, 
        {
            "location": "/modules/btp-module-denovo-canu/denovo_canu/#collect-contigs", 
            "text": "1 cat contig1.fasta contig2.fasta   genome.fasta     See the contigs and sizes: 1 infoseq genome.fasta     chromosome: 2823331   plasmid: 2473", 
            "title": "Collect contigs"
        }, 
        {
            "location": "/modules/btp-module-denovo-canu/denovo_canu/#questions_2", 
            "text": "Q: Why is this section so complicated? A: Finding small plasmids is difficult for many reasons! This paper has a nice summary: On the (im)possibility to reconstruct plasmids from whole genome short-read sequencing data. doi:  https://doi.org/10.1101/086744  Q: Why can PacBio sequencing miss small plasmids? A: Library prep size selection  Q: We extract unmapped Illumina reads and assemble these to find small plasmids. What could they be missing? A: Repeats that have mapped to the PacBio assembly.  Q: How do you find a plasmid in a Bandage graph? A: It is probably circular, matches the size of a known plasmid, has a rep gene  Q: Are there easier ways to find plasmids? A: Possibly. One option is the program called Unicycler which may automate many of these steps.  https://github.com/rrwick/Unicycler", 
            "title": "Questions"
        }, 
        {
            "location": "/modules/btp-module-denovo-canu/denovo_canu/#correct", 
            "text": "We will correct the Pacbio assembly with Illumina reads.", 
            "title": "Correct"
        }, 
        {
            "location": "/modules/btp-module-denovo-canu/denovo_canu/#make-an-alignment-file", 
            "text": "Align the Illumina reads (R1 and R2) to the draft PacBio assembly, e.g.  genome.fasta :   1\n2 bwa index genome.fasta\nbwa mem -t 32 genome.fasta illumina_R1.fastq.gz illumina_R2.fastq.gz | samtools sort   aln.bam     -t  is the number of cores: set this to an appropriate number. (To find out how many you have,  grep -c processor /proc/cpuinfo ).    Index the files:    1\n2 samtools index aln.bam\nsamtools faidx genome.fasta    Now we have an alignment file to use in Pilon:  aln.bam", 
            "title": "Make an alignment file"
        }, 
        {
            "location": "/modules/btp-module-denovo-canu/denovo_canu/#run-pilon", 
            "text": "Run:   1 pilon --genome genome.fasta --frags aln.bam --output pilon1 --fix all --mindepth 0.5 --changes --verbose --threads 32    --genome  is the name of the input assembly to be corrected  --frags  is the alignment of the reads against the assembly  --output  is the name of the output prefix  --fix  is an option for types of corrections  --mindepth  gives a minimum read depth to use  --changes  produces an output file of the changes made  --verbose  prints information to the screen during the run  --threads  : set this to an appropriate number   Look at the changes file:  1 less pilon1.changes   Example:   Look at the details of the fasta file:  1 infoseq pilon1.fasta    chromosome - 2823340 (net +9 bases)  plasmid - 2473 (no change)   Option:  If there are many changes, run Pilon again, using the  pilon1.fasta  file as the input assembly, and the Illumina reads to correct.", 
            "title": "Run Pilon"
        }, 
        {
            "location": "/modules/btp-module-denovo-canu/denovo_canu/#genome-output", 
            "text": "Change the file name:   1 cp pilon1.fasta assembly.fasta    We now have the corrected genome assembly of  Staphylococcus aureus  in .fasta format, containing a chromosome and a small plasmid.", 
            "title": "Genome output"
        }, 
        {
            "location": "/modules/btp-module-denovo-canu/denovo_canu/#questions_3", 
            "text": "Q: Why don t we correct earlier in the assembly process? A: We need to circularise the contigs and trim overhangs first.  Q: Why can we use some reads (Illumina) to correct other reads (PacBio) ? A: Illumina reads have higher accuracy  Q: Could we just use PacBio reads to assemble the genome? A: Yes, if accuracy adequate.", 
            "title": "Questions"
        }, 
        {
            "location": "/modules/btp-module-denovo-canu/denovo_canu/#short-read-assembly-a-comparison", 
            "text": "So far, we have assembled the long PacBio reads into one contig (the chromosome) and found an additional plasmid in the Illumina short reads.  If we only had Illumina reads, we could also assemble these using the tool Spades.  You can try this here or try it later on your own data.", 
            "title": "Short-read assembly: a comparison"
        }, 
        {
            "location": "/modules/btp-module-denovo-canu/denovo_canu/#get-data_1", 
            "text": "We will use the same Illumina data as we used above:   illumina_R1.fastq.gz : the Illumina forward reads  illumina_R2.fastq.gz : the Illumina reverse reads", 
            "title": "Get data"
        }, 
        {
            "location": "/modules/btp-module-denovo-canu/denovo_canu/#assemble_1", 
            "text": "Run Spades:  1 spades.py -1 illumina_R1.fastq.gz -2 illumina_R2.fastq.gz --careful --cov-cutoff auto -o spades_assembly_all_illumina    -1  is input file of forward reads  -2  is input file of reverse reads  --careful  minimizes mismatches and short indels  --cov-cutoff auto  computes the coverage threshold (rather than the default setting,  off )  -o  is the output directory", 
            "title": "Assemble"
        }, 
        {
            "location": "/modules/btp-module-denovo-canu/denovo_canu/#results", 
            "text": "Move into the output directory and look at the contigs:  1 infoseq contigs.fasta", 
            "title": "Results"
        }, 
        {
            "location": "/modules/btp-module-denovo-canu/denovo_canu/#questions_4", 
            "text": "How many contigs were found by Spades?   many   How does this compare to the number of contigs found by assembling the long read data with Canu?   many more.   Does it matter that an assembly is in many contigs?     Yes    broken genes =  missing/incorrect annotations    less information about structure: e.g. number of plasmids    No    Many or all genes may still be annotated   Gene location is useful (e.g. chromosome, plasmid1) but not always essential (e.g. presence/absence of particular resistance genes)   How can we get more information about the assembly from Spades?   Look at the assembly graph  assembly_graph.fastg , e.g. in the program Bandage. This shows how contigs are related, albeit with ambiguity in some places.", 
            "title": "Questions"
        }, 
        {
            "location": "/modules/btp-module-denovo-canu/denovo_canu/#next", 
            "text": "", 
            "title": "Next"
        }, 
        {
            "location": "/modules/btp-module-denovo-canu/denovo_canu/#further-analyses", 
            "text": "Annotate with Prokka.  Comparative genomics, e.g. with Roary.", 
            "title": "Further analyses"
        }, 
        {
            "location": "/modules/btp-module-denovo-canu/denovo_canu/#links", 
            "text": "Details of bas.h5 files  Canu  manual  and  gitub repository  Circlator  article  and  github repository  Pilon  article  and  github repository  Notes on  finishing  and  evaluating  assemblies.", 
            "title": "Links"
        }, 
        {
            "location": "/modules/btp-module-ngs-cli/commandline/", 
            "text": "Key Learning Outcomes\n\n\nAfter completing this practical the trainee should be able to:\n\n\n\n\n\n\nFamiliarise yourself with the command line environment on a Linux\n    operating system.\n\n\n\n\n\n\nRun some basic linux system and file operation commands\n\n\n\n\n\n\nNavigration of biological data files structure and manipulation\n\n\n\n\n\n\nResources You\u2019ll be Using\n\n\nTools Used\n\n\n[style=multiline,labelindent=0cm,align=left,leftmargin=0.5cm]\n\n\nBasic Linux system commands on an Ubuntu OS.\n\n\nBasic file operation commands\n\n\nUseful Links\n\n\nSoftware Carpentry:\n\n\nhttps://software-carpentry.org\n\n\n1000Genome Project data for example:\n\n\nhttp://www.1000genomes.org\n\n\nShell Exercise\n\n\nLet\u2019s try out your new shell skills on some real data.\n\n\nThe file \n1000gp.vcf\n is a small sample (1%) of a very large text file\ncontaining human genetics data. Specifically, it describes genetic\nvariation in three African individuals sequenced as part of the 1000\nGenomes Project (\nhttp://www.1000genomes.org\n). The \u2019vcf\u2019 extension lets\nus know that it\u2019s in a specific text format, namely \u2019Variant Call\nFormat\u2019. The file starts with a bunch of comment lines (they start with\n\u2019#\u2019 or \u2019##\u2019), and then a large number of data lines. This VCF file\nlists the differences between the three African individuals and a\nstandard \u2019individual\u2019 called the reference (actually based upon a few\ndifferent people). Each line in the file corresponds to a difference.\nThe line tells us the position of the difference (chromosome and\nposition), the genetic sequence in the reference, and the corresponding\nsequence in each of the three Africans. Before we start processing the\nfile, let\u2019s get a high-level view of the file that we\u2019re about to work\nwith.\n\n\nOpen the Terminal and go to the directory where the data are stored:\n\n\n1\n2\n3\n4\n5\ncd /home/trainee/cli\nls\npwd\nls -lh 1000gp.vcf\nwc -l 1000gp.vcf\n\n\n\n\n\n\nWhat\u2019s the file size (in kilo-bytes), and how many lines are in the\nfile?. (Hint: \nman ls\n, \nman wc\n)\\\n\n\n3.6M\\\n45034 lines\\\n\n\nBecause this file is so large, you\u2019re going to almost always want to\npipe (\n`) the result of any command to less (a simple text viewer, type\u2018\nq`\u2019 to exit) or head (to print the first 10 lines) so that you don\u2019t\naccidentally print 45,000 lines to the screen.\n\n\nLet\u2019s start by printing the first 5 lines to see what it looks like.\n\n\n1\nhead -5 1000gp.vcf\n\n\n\n\n\n\nThat isn\u2019t very interesting; it\u2019s just a bunch of the comments at the\nbeginning of the file (they all start with \u2019#\u2019)!\n\n\nPrint the first 20 lines to see more of the file.\n\n\n1\nhead -20 1000gp.vcf\n\n\n\n\n\n\nOkay, so now we can see the basic structure of the file. A few comment\nlines that start with \u2019#\u2019 or \u2019##\u2019 and then a bunch of lines of data\nthat contain all the data and are pretty hard to understand. Each line\nof data contains the same number of fields, and all fields are separated\nwith TABs. These fields are:\n\n\n\n\n\n\nthe chromosome (which volume the difference is in)\n\n\n\n\n\n\nthe position (which character in the volume the difference starts\n    at)\n\n\n\n\n\n\nthe ID of the difference\n\n\n\n\n\n\nthe sequence in the reference human(s)\n\n\n\n\n\n\nThe rest of the columns tell us, in a rather complex way, a bunch of\nadditional information about that position, including: the predicted\nsequence for each of the three Africans and how confident the scientists\nare that these sequences are correct.\n\n\nTo start analyzing the actual data, we have to remove the header.\n\n\nHow can we print the first 10 non-header lines (those that don\u2019t start\nwith a \u2019#\u2019)?(Hint: \nman grep\n (remember to use pipes ``))\n\n\n1\ngrep -v \n^\\#\n 1000gp.vcf | head\n\n\n\n\n\n\nHow many lines of data are in the file (rather than counting the number\nof header lines and subtracting, try just counting the number of data\nlines)?\\\n\n\n1\ngrep -v \n^\\#\n 1000gp.vcf | wc -l (should print 45024)\n\n\n\n\n\n\nWhere these differences are located can be important. If all the\ndifferences between two encyclopedias were in just the first volume,\nthat would be interesting. The first field of each data line is the name\nof the chromosome that the difference occurs on (which volume we\u2019re on).\n\n\nPrint the first 10 chromosomes, one per line.\n\n\nHint: \nman cut\n (remember to remove header lines first)\n\n\n1\ngrep -v \n\\^\\#\n 1000gp.vcf | cut -f 1 | head\n\n\n\n\n\n\nAs you should have observed, the first 10 lines are on numbered\nchromosomes. Every normal cell in your body has 23 pairs of chromosomes,\n22 pairs of \u2018autosomal\u2019 chromosomes (these are numbered 1-22) and a pair\nof sex chromosomes (two Xs if you\u2019re female, an X and a Y if you\u2019re\nmale).\n\n\nLet\u2019s look at which chromosomes these variations are on.\n\n\nPrint a list of the chromosomes that are in the file (each chromosome\nname should only be printed once, so you should only print 23 lines).\n\n\nHint: remove all duplicates from your previous answer (\nman sort\n)\n\n\n1\ngrep -v \n\\^\\#\n 1000gp.vcf | cut -f 1 | sort -u\n\n\n\n\n\n\nRather than using \nsort\n to print unique results, a common pipeline is\nto first sort and then pipe to another UNIX command, \nuniq\n. The \nuniq\n\ncommand takes sorted input and prints only unique lines, but it provides\nmore flexibility than just using sort by itself. Keep in mind, if the\ninput isn\u2019t sorted, \nuniq\n won\u2019t work properly.\n\n\nUsing \nsort\n and \nuniq\n, print the number of times each chromosome\noccurs in the file.\n\n\nHint: \nman uniq\n\n\n1\ngrep -v \n\\^\\#\n 1000gp.vcf | cut -f 1 | sort | uniq -c\n\n\n\n\n\n\nAdd to your previous solution to list the chromosomes from most\nfrequently observed to least frequently observed.\n\n\nHint: Make sure you\u2019re sorting in descending order. By default, sort\nsorts in ascending order.\n\n\n1\ngrep -v \n\\^\\#\n 1000gp.vcf | cut -f 1 | sort | uniq -c | sort -n -r\n\n\n\n\n\n\nThis is great, but biologists might also like to see the chromosomes\nordered by their number (not dictionary order), since different\nchromosomes have different attributes and this ordering allows them to\nfind a specific chromosome more easily.\n\n\nSort the previous output by chromosome number\n\n\nHint: A lot of the power of sort comes from the fact that you can\nspecify which fields to sort on, and the order in which to sort them. In\nthis case you only need to sort on one field.\n\n\n1\ngrep -v \n\\^\\#\n 1000gp.vcf | cut -f 1 | sort | uniq -c | sort -k 2n", 
            "title": "Introduction to Command Line"
        }, 
        {
            "location": "/modules/btp-module-ngs-cli/commandline/#key-learning-outcomes", 
            "text": "After completing this practical the trainee should be able to:    Familiarise yourself with the command line environment on a Linux\n    operating system.    Run some basic linux system and file operation commands    Navigration of biological data files structure and manipulation", 
            "title": "Key Learning Outcomes"
        }, 
        {
            "location": "/modules/btp-module-ngs-cli/commandline/#resources-youll-be-using", 
            "text": "", 
            "title": "Resources You\u2019ll be Using"
        }, 
        {
            "location": "/modules/btp-module-ngs-cli/commandline/#tools-used", 
            "text": "[style=multiline,labelindent=0cm,align=left,leftmargin=0.5cm]  Basic Linux system commands on an Ubuntu OS.  Basic file operation commands", 
            "title": "Tools Used"
        }, 
        {
            "location": "/modules/btp-module-ngs-cli/commandline/#useful-links", 
            "text": "Software Carpentry:  https://software-carpentry.org  1000Genome Project data for example:  http://www.1000genomes.org", 
            "title": "Useful Links"
        }, 
        {
            "location": "/modules/btp-module-ngs-cli/commandline/#shell-exercise", 
            "text": "Let\u2019s try out your new shell skills on some real data.  The file  1000gp.vcf  is a small sample (1%) of a very large text file\ncontaining human genetics data. Specifically, it describes genetic\nvariation in three African individuals sequenced as part of the 1000\nGenomes Project ( http://www.1000genomes.org ). The \u2019vcf\u2019 extension lets\nus know that it\u2019s in a specific text format, namely \u2019Variant Call\nFormat\u2019. The file starts with a bunch of comment lines (they start with\n\u2019#\u2019 or \u2019##\u2019), and then a large number of data lines. This VCF file\nlists the differences between the three African individuals and a\nstandard \u2019individual\u2019 called the reference (actually based upon a few\ndifferent people). Each line in the file corresponds to a difference.\nThe line tells us the position of the difference (chromosome and\nposition), the genetic sequence in the reference, and the corresponding\nsequence in each of the three Africans. Before we start processing the\nfile, let\u2019s get a high-level view of the file that we\u2019re about to work\nwith.  Open the Terminal and go to the directory where the data are stored:  1\n2\n3\n4\n5 cd /home/trainee/cli\nls\npwd\nls -lh 1000gp.vcf\nwc -l 1000gp.vcf   What\u2019s the file size (in kilo-bytes), and how many lines are in the\nfile?. (Hint:  man ls ,  man wc )\\  3.6M\\\n45034 lines\\  Because this file is so large, you\u2019re going to almost always want to\npipe ( `) the result of any command to less (a simple text viewer, type\u2018 q`\u2019 to exit) or head (to print the first 10 lines) so that you don\u2019t\naccidentally print 45,000 lines to the screen.  Let\u2019s start by printing the first 5 lines to see what it looks like.  1 head -5 1000gp.vcf   That isn\u2019t very interesting; it\u2019s just a bunch of the comments at the\nbeginning of the file (they all start with \u2019#\u2019)!  Print the first 20 lines to see more of the file.  1 head -20 1000gp.vcf   Okay, so now we can see the basic structure of the file. A few comment\nlines that start with \u2019#\u2019 or \u2019##\u2019 and then a bunch of lines of data\nthat contain all the data and are pretty hard to understand. Each line\nof data contains the same number of fields, and all fields are separated\nwith TABs. These fields are:    the chromosome (which volume the difference is in)    the position (which character in the volume the difference starts\n    at)    the ID of the difference    the sequence in the reference human(s)    The rest of the columns tell us, in a rather complex way, a bunch of\nadditional information about that position, including: the predicted\nsequence for each of the three Africans and how confident the scientists\nare that these sequences are correct.  To start analyzing the actual data, we have to remove the header.  How can we print the first 10 non-header lines (those that don\u2019t start\nwith a \u2019#\u2019)?(Hint:  man grep  (remember to use pipes ``))  1 grep -v  ^\\#  1000gp.vcf | head   How many lines of data are in the file (rather than counting the number\nof header lines and subtracting, try just counting the number of data\nlines)?\\  1 grep -v  ^\\#  1000gp.vcf | wc -l (should print 45024)   Where these differences are located can be important. If all the\ndifferences between two encyclopedias were in just the first volume,\nthat would be interesting. The first field of each data line is the name\nof the chromosome that the difference occurs on (which volume we\u2019re on).  Print the first 10 chromosomes, one per line.  Hint:  man cut  (remember to remove header lines first)  1 grep -v  \\^\\#  1000gp.vcf | cut -f 1 | head   As you should have observed, the first 10 lines are on numbered\nchromosomes. Every normal cell in your body has 23 pairs of chromosomes,\n22 pairs of \u2018autosomal\u2019 chromosomes (these are numbered 1-22) and a pair\nof sex chromosomes (two Xs if you\u2019re female, an X and a Y if you\u2019re\nmale).  Let\u2019s look at which chromosomes these variations are on.  Print a list of the chromosomes that are in the file (each chromosome\nname should only be printed once, so you should only print 23 lines).  Hint: remove all duplicates from your previous answer ( man sort )  1 grep -v  \\^\\#  1000gp.vcf | cut -f 1 | sort -u   Rather than using  sort  to print unique results, a common pipeline is\nto first sort and then pipe to another UNIX command,  uniq . The  uniq \ncommand takes sorted input and prints only unique lines, but it provides\nmore flexibility than just using sort by itself. Keep in mind, if the\ninput isn\u2019t sorted,  uniq  won\u2019t work properly.  Using  sort  and  uniq , print the number of times each chromosome\noccurs in the file.  Hint:  man uniq  1 grep -v  \\^\\#  1000gp.vcf | cut -f 1 | sort | uniq -c   Add to your previous solution to list the chromosomes from most\nfrequently observed to least frequently observed.  Hint: Make sure you\u2019re sorting in descending order. By default, sort\nsorts in ascending order.  1 grep -v  \\^\\#  1000gp.vcf | cut -f 1 | sort | uniq -c | sort -n -r   This is great, but biologists might also like to see the chromosomes\nordered by their number (not dictionary order), since different\nchromosomes have different attributes and this ordering allows them to\nfind a specific chromosome more easily.  Sort the previous output by chromosome number  Hint: A lot of the power of sort comes from the fact that you can\nspecify which fields to sort on, and the order in which to sort them. In\nthis case you only need to sort on one field.  1 grep -v  \\^\\#  1000gp.vcf | cut -f 1 | sort | uniq -c | sort -k 2n", 
            "title": "Shell Exercise"
        }, 
        {
            "location": "/modules/btp-module-ngs-qc/ngs-qc/", 
            "text": "Key Learning Outcomes\n\n\nAfter completing this practical the trainee should be able to:\n\n\n\n\n\n\nAssess the overall quality of NGS (FastQ format) sequence reads\n\n\n\n\n\n\nVisualise the quality, and other associated matrices, of reads to\n    decide on filters and cutoffs for cleaning up data ready for\n    downstream analysis\n\n\n\n\n\n\nClean up adaptors and pre-process the sequence data for further\n    analysis\n\n\n\n\n\n\nResources You\u2019ll be Using\n\n\nTools Used\n\n\nFastQC:\n\n\nhttp://www.bioinformatics.babraham.ac.uk/projects/fastqc/\n\n\nSkewer:\n\n\nhttp://sourceforge.net/projects/skewer/\n\n\nFASTX-Toolkit:\n\n\nhttp://hannonlab.cshl.edu/fastx_toolkit/\n\n\nUseful Links\n\n\nFASTQ Encoding\n\n\nIntroduction\n\n\nGoing on a blind date with your read set? For a better understanding of\nthe consequences please check the data quality!\n\n\nFor the purpose of this tutorial we are focusing only on Illumina\nsequencing which uses \u2019sequence by synthesis\u2019 technology in a highly\nparallel fashion. Although Illumina high throughput sequencing provides\nhighly accurate sequence data, several sequence artifacts, including\nbase calling errors and small insertions/deletions, poor quality reads\nand primer/adapter contamination are quite common in the high throughput\nsequencing data. The primary errors are substitution errors. The error\nrates can vary from 0.5-2.0% with errors mainly rising in frequency at\nthe 3\u2019 ends of reads.\n\n\nOne way to investigate sequence data quality is to visualize the quality\nscores and other metrics in a compact manner to get an idea about the\nquality of a read data set. Read data sets can be improved by pre\nprocessing in different ways like trimming off low quality bases,\ncleaning up any sequencing adapters, removing PCR duplicates and\nscreening for contamination. We can also look at other statistics such\nas, sequence length distribution, base composition, sequence complexity,\npresence of ambiguous bases etc. to assess the overall quality of the\ndata set.\n\n\nHighly redundant coverage ($\n$15X) of the genome can be used to correct\nsequencing errors in the reads before assembly. Various k-mer based\nerror correction methods exist but are beyond the scope of this\ntutorial.\n\n\nQuality Value Encoding Schema\n\n\nIn order to use a single character to encode Phred qualities, ASCII\ncharacters are used\n(\nhttp://shop.alterlinks.com/ascii-table/ascii-table-us.php\n). All ASCII\ncharacters have a decimal number associated with them but the first 32\ncharacters are non-printable (e.g. backspace, shift, return, escape).\nTherefore, the first printable ASCII character is number 33, the\nexclamation mark (!). In Phred+33 encoded quality values the exclamation\nmark takes the Phred quality score of zero.\n\n\nEarly Solexa (now Illumina) sequencing needed to encode negative quality\nvalues. Because ASCII characters $\n$ 33 are non-printable, using the\nPhred+33 encoding was not possible. Therefore, they simply moved the\noffset from 33 to 64 thus inventing the Phred+64 encoded quality values.\nIn this encoding a Phred quality of zero is denoted by the ASCII number\n64 (the @ character). Since Illumina 1.8, quality values are now encoded\nusing Phred+33.\n\n\nFASTQ does not provide a way to describe what quality encoding is used\nfor the quality values. Therefore, you should find this out from your\nsequencing provider. Alternatively, you may be able to figure this out\nby determining what ASCII characters are present in the FASTQ file. E.g\nthe presence of numbers in the quality strings, can only mean the\nquality values are Phred+33 encoded. However, due to the overlapping\nnature of the Phred+33 and Phred+64 encoding schema it is not always\npossible to identify what encoding is in use. For example, if the only\ncharacters seen in the quality string are (\n@ABCDEFGHI\n), then it is\nimpossible to know if you have really good Phred+33 encoded qualities or\nreally bad Phred+64 encoded qualities.\n\n\nFor a graphical representation of the different ASCII characters used in\nthe two encoding schema see:\n\nhttp://en.wikipedia.org/wiki/FASTQ_format#Encoding\n.\n\n\nPrepare the Environment\n\n\nTo investigate sequence data quality we will demonstrate tools called\nFastQC and Skewer. FastQC will process and present the reports in a\nvisual manner. Based on the results, the sequence data can be processed\nusing the Skewer. We will use one data set in this practical, which can\nbe found in the QC directory on your desktop.\n\n\nOpen the Terminal and go to the directory where the data are stored:\n\n\n1\n2\n3\n4\ncd\nls\ncd qc\npwd\n\n\n\n\n\n\nAt any time, help can be displayed for FastQC using the following\ncommand:\n\n\n1\nfastqc -h\n\n\n\n\n\n\nLook at SYNOPSIS (Usage) and options after typing fastqc -h\n\n\nQuality Visualisation\n\n\nWe have a file for a good quality and bad quality statistics. FastQC\ngenerates results in the form of a zipped and unzipped directory for\neach input file.\n\n\nExecute the following command on the two files:\n\n\n1\n2\nfastqc -f fastq qcdemo_R1.fastq.gz\nfastqc -f fastq qcdemo_R2.fastq.gz\n\n\n\n\n\n\nView the FastQC report file of the bad data using a web browser such as\nfirefox. The \u2019\n\u2019 sign puts the job in the background.\n\n\n1\nfirefox qcdemo_R2_fastqc.html \n\n\n\n\n\n\n\nThe report file will have a Basic Statistics table and various graphs\nand tables for different quality statistics. E.g.:\n\n\n[H]\n\n\nll\n\n\nFilename \n qcdemo_R2.fastq.gz\\\nFile type \n Conventional base calls\\\nEncoding \n Sanger / Illumina 1.9\\\nTotal Sequences \n 1000000\\\nFiltered Sequences \n 0\\\nSequence length \n 150\\\n%GC \n 37\\\n\n\n[tab:badexampleuntrimmed]\n\n\n[H] \n\n[fig:bad\ne\nxample\nu\nntrimmed\np\nlot]\n\n\nA Phred quality score (or Q-score) expresses an error probability. In\nparticular, it serves as a convenient and compact way to communicate\nvery small error probabilities. The probability that base $A$ is wrong\n($P(\\sim A)$) is expressed by a quality score, $Q(A)$, according to the\nrelationship:\\\n\\\n$Q(A) =-10 log10(P(\\sim A))$\\\n\\\nThe relationship between the quality score and error probability is\ndemonstrated with the following table:\n\n\n[H]\n\n\nrrr\n\n\nQuality score, Q(A)\n \n \nError probability, P($\\sim$A)\n \n \nAccuracy\nof the base call\n\\\n10 \n 0.1 \n 90%\\\n20 \n 0.01 \n 99%\\\n30 \n 0.001 \n 99.9%\\\n40 \n 0.0001 \n 99.99%\\\n50 \n 0.00001 \n 99.999%\\\n\n\n[tab:quality\ne\nrror\np\nrobs]\n\n\nHow many sequences were there in your file? What is the read length?\n\n\n1,000,000. read length=150bp\n\n\nDoes the quality score values vary throughout the read length? (hint:\nlook at the \u2019per base sequence quality plot\u2019)\n\n\nYes. Quality scores are dropping towards the end of the reads.\n\n\nWhat is the quality score range you see?\n\n\n2-40\n\n\nAt around which position do the scores start falling below Q20 for the\n25% quartile range (25%of reads below Q20)?\n\n\nAround 30 bp position\n\n\nHow can we trim the reads to filter out the low quality data?\n\n\nBy trimming off the bases after a fixed position of the read or by\ntrimming off bases based on the quality score.\n\n\nGood Quality Data\n\n\nView the FastQC report files \nfastqc_report.html\n to see examples of a\ngood quality data and compare the quality plot with that of the\n\nbad_example_fastqc\n.\n\n\n1\nfirefox qcdemo_R1_fastqc.html \n\n\n\n\n\n\n\nSequencing errors can complicate the downstream analysis, which normally\nrequires that reads be aligned to each other (for genome assembly) or to\na reference genome (for detection of mutations). Sequence reads\ncontaining errors may lead to ambiguous paths in the assembly or\nimproper gaps. In variant analysis projects sequence reads are aligned\nagainst the reference genome. The errors in the reads may lead to more\nmismatches than expected from mutations alone. But if these errors can\nbe removed or corrected, the read alignments and hence the variant\ndetection will improve. The assemblies will also improve after\npre-processing the reads to remove errors.\n\n\nRead Trimming\n\n\nRead trimming can be done in a variety of different ways. Choose a\nmethod which best suits your data. Here we are giving examples of\nfixed-length trimming and quality-based trimming.\n\n\nQuality Based Trimming\n\n\nBase call quality scores can be used to dynamically determine the trim\npoints for each read. A quality score threshold and minimum read length\nfollowing trimming can be used to remove low quality data.\n\n\nThe previous FastQC results show R1 is fine but R2 has low quality at\nthe end. There is no adaptor contamination though. We will be using\nSkewer to perform the quality trimming.\n\n\nRun the following command to quality trim a set of paired end data.\n\n\n1\n2\ncd /home/trainee/qc\nskewer -t 4 -l 50  -q 30 -Q 25 -m pe -o qcdemo qcdemo_R1.fastq.gz qcdemo_R2.fastq.gz\n\n\n\n\n\n\n-t\n:   number of threads to use\n\n\n-l\n:   min length to keep after trimming\n\n\n-q\n:   Quality threshold used for trimming at 3\u2019 end\n\n\n-Q\n:   mean quality threshold for a read\n\n\n-m\n:   pair-end mode\n\n\nRun FastQC on the quality trimmed file and visualise the quality scores.\n\n\nLook at the last files generated, are the file names same as the input ?\n\n\n1\nls -ltr\n\n\n\n\n\n\nRun Fastqc on the quality trimmed files:\n\n\n1\n2\nfastqc -f fastq qcdemo_R1.fastq-trimmed-pair1.fastq\nfastqc -f fastq qcdemo_R1.fastq-trimmed-pair2.fastq\n\n\n\n\n\n\nVisualise the fastqc results:\n\n\n1\n2\nfirefox qcdemo_R1.fastq-trimmed-pair1_fastqc.html \n\nfirefox qcdemo_R1.fastq-trimmed-pair2_fastqc.html\n\n\n\n\n\n\n\nLet\u2019s look at the quality from the second reads. The output should look\nlike:\n\n\n[H]\n\n\nll\n\n\nFilename \n qcdemo_R1.fastq-trimmed-pair2.fastq\\\nFile type \n Conventional base calls\\\nEncoding \n Sanger / Illumina 1.9\\\nTotal Sequences \n 742262\\\nFiltered Sequences \n 0\\\nSequence length \n 50-150\\\n%GC \n 37\\\n\n\n[tab:badexamplequalitytrimmed]\n\n\n[H] \n\n[fig:bad\ne\nxample\nq\nuality\nt\nrimmed\np\nlot]\n\n\nDid the number of total reads in R1 and R2 change after trimming?\n\n\nQuality trimming discarded $\n$25000 reads. However, We retain a lot of\nmaximal length reads which have good quality all the way to the ends.\n\n\nWhat reads lengths were obtained after quality based trimming?\n\n\n50-150\n\n\nReads $\n$50 bp, following quality trimming, were discarded.\n\n\n\n\nQuestion\n\n\nDid you observe adapter sequences in the data?\n\n\n\n\nAnswer\nNo. (Hint: look at the overrepresented sequences)    \n\n\nQuestion\n\n\nHow can you use -a option with fastqc? (Hint: try fastqc -h).\n\n\n\n\nAnswer\nAdaptors can be supplied in a file for screening.\nAdapter Clipping\n\n\nSometimes sequence reads may end up getting the leftover of adapters and\nprimers used in the sequencing process. It\u2019s good practice to screen\nyour data for these possible contamination for more sensitive alignment\nand assembly based analysis.\n\n\nThis is particularly important when read lengths can be longer than the\nmolecules being sequenced. For example when sequencing miRNAs.\n\n\nVarious QC tools are available to screen and/or clip these\nadapter/primer sequences from your data. Apart from skewer which will be\nusing today the following two tools are also useful for trimming and\nremoving adapter sequence.\n\n\nCutadapt \nhttp://code.google.com/p/cutadapt/\n\n\nTrimmomatic \nhttp://www.usadellab.org/cms/?page=trimmomatic\n\n\nHere we are demonstrating \nSkewer\n to trim a given adapter sequence.\n\n\n1\n2\n3\n4\ncd /home/trainee/qc\nfastqc -f fastq  adaptorQC.fastq.gz\nfirefox adaptorQC_fastqc.html\nskewer -x TGGAATTCTCGGGTGCCAAGGT -t 20 -l 10 -L 35 -q 30 adaptorQC.fastq.gz\n\n\n\n\n\n\n-x\n:   adaptor sequence used\n\n\n-t\n:   number of threads to use\n\n\n-l\n:   min length to keep after trimming\n\n\n-L\n:   Max length to keep after trimming, in this experiment we were\n    expecting only small RNA fragments\n\n\n-Q\n:   Quality threshold used for trimming at 3\u2019 end. Use -m option to\n    control the end you want to trim\n\n\nRun FastQC on the adapter trimmed file and visualise the quality scores.\nFastqc now shows adaptor free results.\n\n\n1\n2\nfastqc adaptorQC.fastq-trimmed.fastq\nfirefox adaptorQC.fastq-trimmed_fastqc.html \n\n\n\n\n\n\n\nFixed Length Trimming\n\n\nWe will not cover Fixed Length Trimming but provide the following for\nyour information.\n Low quality read ends can be trimmed using a\nfixed-length trimming. We will use the \nfastx_trimmer\n from the\nFASTX-Toolkit. Usage message to find out various options you can use\nwith this tool. Type \nfastx_trimmer -h\n at anytime to display help.\n\n\nWe will now do fixed-length trimming of the \nbad_example.fastq\n file\nusing the following command. You should still be in the qc directory, if\nnot cd back in.\n\n\n1\n2\n3\n4\ncd /home/trainee/qc\nfastqc -f fastq bad_example.fastq\nfastx_trimmer -h\nfastx_trimmer -Q 33 -f 1 -l 80 -i bad_example.fastq -o bad_example_trimmed01.fastq\n\n\n\n\n\n\nWe used the following options in the command above:\n\n\n-Q 33\n:   Indicates the input quality scores are Phred+33 encoded\n\n\n-f\n:   First base to be retained in the output\n\n\n-l\n:   Last base to be retained in the output\n\n\n-i\n:   Input FASTQ file name\n\n\n-o\n:   Output file name\n\n\nRun FastQC on the trimmed file and visualise the quality scores of the\ntrimmed file.\n\n\n1\n2\nfastqc -f fastq bad_example_trimmed01.fastq\nfirefox bad_example_trimmed01_fastqc.html \n\n\n\n\n\n\n\nThe output should look like:\n\n\n[H]\n\n\nll\n\n\nFilename \n bad_example_trimmed01.fastq\\\nFile type \n Conventional base calls\\\nEncoding \n Sanger / Illumina 1.9\\\nTotal Sequences \n 40000\\\nFiltered Sequences \n 0\\\nSequence length \n 80\\\n%GC \n 48\\\n\n\n[tab:badexampletrimmed]\n\n\n\n[fig:bad\ne\nxample\nt\nrimmed\np\nlot]\n\n\nWhat values would you use for \n-f\n if you wanted to trim off 10 bases at\nthe 5\u2019 end of the reads?\n\n\n-f 11", 
            "title": "Data Quality"
        }, 
        {
            "location": "/modules/btp-module-ngs-qc/ngs-qc/#key-learning-outcomes", 
            "text": "After completing this practical the trainee should be able to:    Assess the overall quality of NGS (FastQ format) sequence reads    Visualise the quality, and other associated matrices, of reads to\n    decide on filters and cutoffs for cleaning up data ready for\n    downstream analysis    Clean up adaptors and pre-process the sequence data for further\n    analysis", 
            "title": "Key Learning Outcomes"
        }, 
        {
            "location": "/modules/btp-module-ngs-qc/ngs-qc/#resources-youll-be-using", 
            "text": "", 
            "title": "Resources You\u2019ll be Using"
        }, 
        {
            "location": "/modules/btp-module-ngs-qc/ngs-qc/#tools-used", 
            "text": "FastQC:  http://www.bioinformatics.babraham.ac.uk/projects/fastqc/  Skewer:  http://sourceforge.net/projects/skewer/  FASTX-Toolkit:  http://hannonlab.cshl.edu/fastx_toolkit/", 
            "title": "Tools Used"
        }, 
        {
            "location": "/modules/btp-module-ngs-qc/ngs-qc/#useful-links", 
            "text": "FASTQ Encoding", 
            "title": "Useful Links"
        }, 
        {
            "location": "/modules/btp-module-ngs-qc/ngs-qc/#introduction", 
            "text": "Going on a blind date with your read set? For a better understanding of\nthe consequences please check the data quality!  For the purpose of this tutorial we are focusing only on Illumina\nsequencing which uses \u2019sequence by synthesis\u2019 technology in a highly\nparallel fashion. Although Illumina high throughput sequencing provides\nhighly accurate sequence data, several sequence artifacts, including\nbase calling errors and small insertions/deletions, poor quality reads\nand primer/adapter contamination are quite common in the high throughput\nsequencing data. The primary errors are substitution errors. The error\nrates can vary from 0.5-2.0% with errors mainly rising in frequency at\nthe 3\u2019 ends of reads.  One way to investigate sequence data quality is to visualize the quality\nscores and other metrics in a compact manner to get an idea about the\nquality of a read data set. Read data sets can be improved by pre\nprocessing in different ways like trimming off low quality bases,\ncleaning up any sequencing adapters, removing PCR duplicates and\nscreening for contamination. We can also look at other statistics such\nas, sequence length distribution, base composition, sequence complexity,\npresence of ambiguous bases etc. to assess the overall quality of the\ndata set.  Highly redundant coverage ($ $15X) of the genome can be used to correct\nsequencing errors in the reads before assembly. Various k-mer based\nerror correction methods exist but are beyond the scope of this\ntutorial.", 
            "title": "Introduction"
        }, 
        {
            "location": "/modules/btp-module-ngs-qc/ngs-qc/#quality-value-encoding-schema", 
            "text": "In order to use a single character to encode Phred qualities, ASCII\ncharacters are used\n( http://shop.alterlinks.com/ascii-table/ascii-table-us.php ). All ASCII\ncharacters have a decimal number associated with them but the first 32\ncharacters are non-printable (e.g. backspace, shift, return, escape).\nTherefore, the first printable ASCII character is number 33, the\nexclamation mark (!). In Phred+33 encoded quality values the exclamation\nmark takes the Phred quality score of zero.  Early Solexa (now Illumina) sequencing needed to encode negative quality\nvalues. Because ASCII characters $ $ 33 are non-printable, using the\nPhred+33 encoding was not possible. Therefore, they simply moved the\noffset from 33 to 64 thus inventing the Phred+64 encoded quality values.\nIn this encoding a Phred quality of zero is denoted by the ASCII number\n64 (the @ character). Since Illumina 1.8, quality values are now encoded\nusing Phred+33.  FASTQ does not provide a way to describe what quality encoding is used\nfor the quality values. Therefore, you should find this out from your\nsequencing provider. Alternatively, you may be able to figure this out\nby determining what ASCII characters are present in the FASTQ file. E.g\nthe presence of numbers in the quality strings, can only mean the\nquality values are Phred+33 encoded. However, due to the overlapping\nnature of the Phred+33 and Phred+64 encoding schema it is not always\npossible to identify what encoding is in use. For example, if the only\ncharacters seen in the quality string are ( @ABCDEFGHI ), then it is\nimpossible to know if you have really good Phred+33 encoded qualities or\nreally bad Phred+64 encoded qualities.  For a graphical representation of the different ASCII characters used in\nthe two encoding schema see: http://en.wikipedia.org/wiki/FASTQ_format#Encoding .", 
            "title": "Quality Value Encoding Schema"
        }, 
        {
            "location": "/modules/btp-module-ngs-qc/ngs-qc/#prepare-the-environment", 
            "text": "To investigate sequence data quality we will demonstrate tools called\nFastQC and Skewer. FastQC will process and present the reports in a\nvisual manner. Based on the results, the sequence data can be processed\nusing the Skewer. We will use one data set in this practical, which can\nbe found in the QC directory on your desktop.  Open the Terminal and go to the directory where the data are stored:  1\n2\n3\n4 cd\nls\ncd qc\npwd   At any time, help can be displayed for FastQC using the following\ncommand:  1 fastqc -h   Look at SYNOPSIS (Usage) and options after typing fastqc -h", 
            "title": "Prepare the Environment"
        }, 
        {
            "location": "/modules/btp-module-ngs-qc/ngs-qc/#quality-visualisation", 
            "text": "We have a file for a good quality and bad quality statistics. FastQC\ngenerates results in the form of a zipped and unzipped directory for\neach input file.  Execute the following command on the two files:  1\n2 fastqc -f fastq qcdemo_R1.fastq.gz\nfastqc -f fastq qcdemo_R2.fastq.gz   View the FastQC report file of the bad data using a web browser such as\nfirefox. The \u2019 \u2019 sign puts the job in the background.  1 firefox qcdemo_R2_fastqc.html     The report file will have a Basic Statistics table and various graphs\nand tables for different quality statistics. E.g.:  [H]  ll  Filename   qcdemo_R2.fastq.gz\\\nFile type   Conventional base calls\\\nEncoding   Sanger / Illumina 1.9\\\nTotal Sequences   1000000\\\nFiltered Sequences   0\\\nSequence length   150\\\n%GC   37\\  [tab:badexampleuntrimmed]  [H]  \n[fig:bad e xample u ntrimmed p lot]  A Phred quality score (or Q-score) expresses an error probability. In\nparticular, it serves as a convenient and compact way to communicate\nvery small error probabilities. The probability that base $A$ is wrong\n($P(\\sim A)$) is expressed by a quality score, $Q(A)$, according to the\nrelationship:\\\n\\\n$Q(A) =-10 log10(P(\\sim A))$\\\n\\\nThe relationship between the quality score and error probability is\ndemonstrated with the following table:  [H]  rrr  Quality score, Q(A)     Error probability, P($\\sim$A)     Accuracy\nof the base call \\\n10   0.1   90%\\\n20   0.01   99%\\\n30   0.001   99.9%\\\n40   0.0001   99.99%\\\n50   0.00001   99.999%\\  [tab:quality e rror p robs]  How many sequences were there in your file? What is the read length?  1,000,000. read length=150bp  Does the quality score values vary throughout the read length? (hint:\nlook at the \u2019per base sequence quality plot\u2019)  Yes. Quality scores are dropping towards the end of the reads.  What is the quality score range you see?  2-40  At around which position do the scores start falling below Q20 for the\n25% quartile range (25%of reads below Q20)?  Around 30 bp position  How can we trim the reads to filter out the low quality data?  By trimming off the bases after a fixed position of the read or by\ntrimming off bases based on the quality score.", 
            "title": "Quality Visualisation"
        }, 
        {
            "location": "/modules/btp-module-ngs-qc/ngs-qc/#good-quality-data", 
            "text": "View the FastQC report files  fastqc_report.html  to see examples of a\ngood quality data and compare the quality plot with that of the bad_example_fastqc .  1 firefox qcdemo_R1_fastqc.html     Sequencing errors can complicate the downstream analysis, which normally\nrequires that reads be aligned to each other (for genome assembly) or to\na reference genome (for detection of mutations). Sequence reads\ncontaining errors may lead to ambiguous paths in the assembly or\nimproper gaps. In variant analysis projects sequence reads are aligned\nagainst the reference genome. The errors in the reads may lead to more\nmismatches than expected from mutations alone. But if these errors can\nbe removed or corrected, the read alignments and hence the variant\ndetection will improve. The assemblies will also improve after\npre-processing the reads to remove errors.", 
            "title": "Good Quality Data"
        }, 
        {
            "location": "/modules/btp-module-ngs-qc/ngs-qc/#read-trimming", 
            "text": "Read trimming can be done in a variety of different ways. Choose a\nmethod which best suits your data. Here we are giving examples of\nfixed-length trimming and quality-based trimming.", 
            "title": "Read Trimming"
        }, 
        {
            "location": "/modules/btp-module-ngs-qc/ngs-qc/#quality-based-trimming", 
            "text": "Base call quality scores can be used to dynamically determine the trim\npoints for each read. A quality score threshold and minimum read length\nfollowing trimming can be used to remove low quality data.  The previous FastQC results show R1 is fine but R2 has low quality at\nthe end. There is no adaptor contamination though. We will be using\nSkewer to perform the quality trimming.  Run the following command to quality trim a set of paired end data.  1\n2 cd /home/trainee/qc\nskewer -t 4 -l 50  -q 30 -Q 25 -m pe -o qcdemo qcdemo_R1.fastq.gz qcdemo_R2.fastq.gz   -t\n:   number of threads to use  -l\n:   min length to keep after trimming  -q\n:   Quality threshold used for trimming at 3\u2019 end  -Q\n:   mean quality threshold for a read  -m\n:   pair-end mode  Run FastQC on the quality trimmed file and visualise the quality scores.  Look at the last files generated, are the file names same as the input ?  1 ls -ltr   Run Fastqc on the quality trimmed files:  1\n2 fastqc -f fastq qcdemo_R1.fastq-trimmed-pair1.fastq\nfastqc -f fastq qcdemo_R1.fastq-trimmed-pair2.fastq   Visualise the fastqc results:  1\n2 firefox qcdemo_R1.fastq-trimmed-pair1_fastqc.html  \nfirefox qcdemo_R1.fastq-trimmed-pair2_fastqc.html    Let\u2019s look at the quality from the second reads. The output should look\nlike:  [H]  ll  Filename   qcdemo_R1.fastq-trimmed-pair2.fastq\\\nFile type   Conventional base calls\\\nEncoding   Sanger / Illumina 1.9\\\nTotal Sequences   742262\\\nFiltered Sequences   0\\\nSequence length   50-150\\\n%GC   37\\  [tab:badexamplequalitytrimmed]  [H]  \n[fig:bad e xample q uality t rimmed p lot]  Did the number of total reads in R1 and R2 change after trimming?  Quality trimming discarded $ $25000 reads. However, We retain a lot of\nmaximal length reads which have good quality all the way to the ends.  What reads lengths were obtained after quality based trimming?  50-150  Reads $ $50 bp, following quality trimming, were discarded.   Question  Did you observe adapter sequences in the data?   Answer No. (Hint: look at the overrepresented sequences)      Question  How can you use -a option with fastqc? (Hint: try fastqc -h).   Answer Adaptors can be supplied in a file for screening.", 
            "title": "Quality Based Trimming"
        }, 
        {
            "location": "/modules/btp-module-ngs-qc/ngs-qc/#adapter-clipping", 
            "text": "Sometimes sequence reads may end up getting the leftover of adapters and\nprimers used in the sequencing process. It\u2019s good practice to screen\nyour data for these possible contamination for more sensitive alignment\nand assembly based analysis.  This is particularly important when read lengths can be longer than the\nmolecules being sequenced. For example when sequencing miRNAs.  Various QC tools are available to screen and/or clip these\nadapter/primer sequences from your data. Apart from skewer which will be\nusing today the following two tools are also useful for trimming and\nremoving adapter sequence.  Cutadapt  http://code.google.com/p/cutadapt/  Trimmomatic  http://www.usadellab.org/cms/?page=trimmomatic  Here we are demonstrating  Skewer  to trim a given adapter sequence.  1\n2\n3\n4 cd /home/trainee/qc\nfastqc -f fastq  adaptorQC.fastq.gz\nfirefox adaptorQC_fastqc.html\nskewer -x TGGAATTCTCGGGTGCCAAGGT -t 20 -l 10 -L 35 -q 30 adaptorQC.fastq.gz   -x\n:   adaptor sequence used  -t\n:   number of threads to use  -l\n:   min length to keep after trimming  -L\n:   Max length to keep after trimming, in this experiment we were\n    expecting only small RNA fragments  -Q\n:   Quality threshold used for trimming at 3\u2019 end. Use -m option to\n    control the end you want to trim  Run FastQC on the adapter trimmed file and visualise the quality scores.\nFastqc now shows adaptor free results.  1\n2 fastqc adaptorQC.fastq-trimmed.fastq\nfirefox adaptorQC.fastq-trimmed_fastqc.html", 
            "title": "Adapter Clipping"
        }, 
        {
            "location": "/modules/btp-module-ngs-qc/ngs-qc/#fixed-length-trimming", 
            "text": "We will not cover Fixed Length Trimming but provide the following for\nyour information.  Low quality read ends can be trimmed using a\nfixed-length trimming. We will use the  fastx_trimmer  from the\nFASTX-Toolkit. Usage message to find out various options you can use\nwith this tool. Type  fastx_trimmer -h  at anytime to display help.  We will now do fixed-length trimming of the  bad_example.fastq  file\nusing the following command. You should still be in the qc directory, if\nnot cd back in.  1\n2\n3\n4 cd /home/trainee/qc\nfastqc -f fastq bad_example.fastq\nfastx_trimmer -h\nfastx_trimmer -Q 33 -f 1 -l 80 -i bad_example.fastq -o bad_example_trimmed01.fastq   We used the following options in the command above:  -Q 33\n:   Indicates the input quality scores are Phred+33 encoded  -f\n:   First base to be retained in the output  -l\n:   Last base to be retained in the output  -i\n:   Input FASTQ file name  -o\n:   Output file name  Run FastQC on the trimmed file and visualise the quality scores of the\ntrimmed file.  1\n2 fastqc -f fastq bad_example_trimmed01.fastq\nfirefox bad_example_trimmed01_fastqc.html     The output should look like:  [H]  ll  Filename   bad_example_trimmed01.fastq\\\nFile type   Conventional base calls\\\nEncoding   Sanger / Illumina 1.9\\\nTotal Sequences   40000\\\nFiltered Sequences   0\\\nSequence length   80\\\n%GC   48\\  [tab:badexampletrimmed]  \n[fig:bad e xample t rimmed p lot]  What values would you use for  -f  if you wanted to trim off 10 bases at\nthe 5\u2019 end of the reads?  -f 11", 
            "title": "Fixed Length Trimming"
        }, 
        {
            "location": "/modules/btp-module-ngs-mapping/alignment/", 
            "text": "Key Learning Outcomes\n\n\nAfter completing this practical the trainee should be able to:\n\n\n\n\n\n\nPerform a simple NGS data alignment task, with Bowtie2, against one\n    interested reference data\n\n\n\n\n\n\nInterpret and manipulate the mapping output using SAMtools\n\n\n\n\n\n\nVisualise the alignment via a standard genome browser, e.g. IGV\n    browser\n\n\n\n\n\n\nResources You\u2019ll be Using\n\n\nTools Used\n\n\nBowtie2:\n\n\nhttp://bowtie-bio.sourceforge.net/bowtie2/index.shtml\n\n\nSamtools:\n\n\nhttp://broadinstitute.github.io/picard\n\n\nBEDTools:\n\n\nhttp://code.google.com/p/bedtools/\n\n\nUCSC tools:\n\n\nhttp://hgdownload.cse.ucsc.edu/admin/exe/\n\n\nIGV genome browser:\n\n\nhttp://www.broadinstitute.org/igv/\n\n\nUseful Links\n\n\nSAM Specification:\n\n\nhttp://samtools.sourceforge.net/SAM1.pdf\n\n\nExplain SAM Flags:\n\n\nhttps://broadinstitute.github.io/picard/explain-flags.html\n\n\nSources of Data\n\n\nhttp://www.ebi.ac.uk/arrayexpress/experiments/E-GEOD-11431\n\n\nIntroduction\n\n\nThe goal of this hands-on session is to perform an unspliced alignment\nfor a small subset of raw reads. We will align raw sequencing data to\nthe mouse genome using Bowtie2 and then we will manipulate the SAM\noutput in order to visualize the alignment on the IGV browser.\n\n\nPrepare the Environment\n\n\nWe will use one data set in this practical, which can be found in the\n\nChIP-seq\n directory on your desktop.\n\n\nOpen the Terminal.\n\n\nFirst, go to the right folder, where the data are stored.\n\n\n1\ncd /home/trainee/chipseq\n\n\n\n\n\n\nThe \n.fastq\n file that we will align is called \nOct4.fastq\n. This file\nis based on Oct4 ChIP-seq data published by Chen \net al.\n (2008). For\nthe sake of time, we will align these reads to a single mouse\nchromosome.\n\n\nAlignment\n\n\nYou already know that there are a number of competing tools for short\nread alignment, each with its own set of strengths, weaknesses, and\ncaveats. Here we will try Bowtie2, a widely used ultrafast, memory\nefficient short read aligner.\n\n\nBowtie2 has a number of parameters in order to perform the alignment. To\nview them all type\n\n\n1\nbowtie2 --help\n\n\n\n\n\n\nBowtie2 uses indexed genome for the alignment in order to keep its\nmemory footprint small. Because of time constraints we will build the\nindex only for one chromosome of the mouse genome. For this we need the\nchromosome sequence in FASTA format. This is stored in a file named\n\nmm10\n, under the subdirectory \nbowtie_index\n.\n\n\nThe indexed chromosome is generated using the command:\n\n\n1\nbowtie2-build bowtie_index/mm10.fa bowtie_index/mm10\n\n\n\n\n\n\nThis command will output 6 files that constitute the index. These files\nthat have the prefix \nmm10\n are stored in the \nbowtie_index\n\nsubdirectory. To view if they files have been successfully created type:\n\n\n1\nls -l bowtie_index\n\n\n\n\n\n\nNow that the genome is indexed we can move on to the actual alignment.\nThe first argument for \nbowtie2\n is the basename of the index for the\ngenome to be searched; in our case this is \nmm10\n. We also want to make\nsure that the output is in SAM format using the \n-S\n parameter. The last\nargument is the name of the FASTQ file.\n\n\nAlign the Oct4 reads using Bowtie2:\n\n\n1\nbowtie2 -x bowtie_index/mm10 -q Oct4.fastq \n Oct4.sam\n\n\n\n\n\n\nThe above command outputs the alignment in SAM format and stores them in\nthe file \nOct4.sam\n.\n\n\nIn general before you run Bowtie2, you have to know what quality\nencoding your FASTQ files are in. The available FASTQ encodings for\nbowtie are:\n\n\n\u2013phred33-quals\n:   Input qualities are Phred+33 (default).\n\n\n\u2013phred64-quals\n:   Input qualities are Phred+64 (same as \n\u2013solexa1.3-quals\n).\n\n\n\u2013solexa-quals\n:   Input qualities are from GA Pipeline ver. $\n$ 1.3.\n\n\n\u2013solexa1.3-quals\n:   Input qualities are from GA Pipeline ver. $\\geq$ 1.3.\n\n\n\u2013integer-quals\n:   Qualities are given as space-separated integers (not ASCII).\n\n\nThe FASTQ files we are working with are Sanger encoded (Phred+33), which\nis the default for Bowtie2.\n\n\nBowtie2 will take 2-3 minutes to align the file. This is fast compared\nto other aligners which sacrifice some speed to obtain higher\nsensitivity.\n\n\nLook at the top 10 lines of the SAM file using head (record lines are\nwrapped). Then try the second command, note use arrow navigation and to\nexit type \u2019q\u2019.\n\n\n1\n2\nhead  Oct4.sam\nless -S Oct4.sam\n\n\n\n\n\n\nCan you distinguish between the header of the SAM format and the actual\nalignments?\n\n\nThe header line starts with the letter \u2018@\u2019, i.e.:\n\n\n\n\n@HD   VN:1.0       SO:unsorted             \n\n  @SQ   SN:chr1      LN:195471971            \n\n  @PG   ID:Bowtie2   PN:bowtie2     VN:2.2.4   CL:\u201c/tools/bowtie2/bowtie2-default/bowtie2-align-s \u2013wrapper basic-0 -x bowtie_index/mm10 -q Oct4.fastq\u201d\n\n\n\n\nWhile, the actual alignments start with read id, i.e.:\n\n\n\n\nSRR002012.45   0    etc  \n\n  SRR002012.48   16   chr1   etc\n\n\n\n\nWhat kind of information does the header provide?\n\n\n\n\n\n\n@HD: Header line; VN: Format version; SO: the sort order of\n    alignments.\n\n\n\n\n\n\n@SQ: Reference sequence information; SN: reference sequence name;\n    LN: reference sequence length.\n\n\n\n\n\n\n@PG: Read group information; ID: Read group identifier; VN: Program\n    version; CL: the command line that produces the alignment.\n\n\n\n\n\n\nTo which chromosome are the reads mapped?\n\n\nChromosome 1.\n\n\nManipulate SAM output\n\n\nSAM files are rather big and when dealing with a high volume of NGS\ndata, storage space can become an issue. As we have already seen, we can\nconvert SAM to BAM files (their binary equivalent that are not human\nreadable) that occupy much less space.\n\n\nConvert SAM to BAM using \nsamtools view\n and store the output in the\nfile \nOct4.bam\n. You have to instruct \nsamtools view\n that the input is\nin SAM format (\n-S\n), the output should be in BAM format (\n-b\n) and that\nyou want the output to be stored in the file specified by the \n-o\n\noption:\n\n\n1\nsamtools view -bSo Oct4.bam Oct4.sam\n\n\n\n\n\n\nCompute summary stats for the Flag values associated with the alignments\nusing:\n\n\n1\nsamtools flagstat Oct4.bam\n\n\n\n\n\n\nVisualize alignments in IGV\n\n\nIGV is a stand-alone genome browser. Please check their website\n(\nhttp://www.broadinstitute.org/igv/\n) for all the formats that IGV can\ndisplay. For our visualization purposes we will use the BAM and bigWig\nformats.\n\n\nWhen uploading a BAM file into the genome browser, the browser will look\nfor the index of the BAM file in the same folder where the BAM files is.\nThe index file should have the same name as the BAM file and the suffix\n\n.bai\n. Finally, to create the index of a BAM file you need to make sure\nthat the file is sorted according to chromosomal coordinates.\n\n\nSort alignments according to chromosomal position and store the result\nin the file with the prefix \nOct4.sorted\n:\n\n\n1\nsamtools sort Oct4.bam Oct4.sorted\n\n\n\n\n\n\nIndex the sorted file.\n\n\n1\nsamtools index Oct4.sorted.bam\n\n\n\n\n\n\nThe indexing will create a file called \nOct4.sorted.bam.bai\n. Note that\nyou don\u2019t have to specify the name of the index file when running\n\nsamtools index\n, it simply appends a \n.bai\n suffix to the input BAM\nfile.\n\n\nAnother way to visualize the alignments is to convert the BAM file into\na bigWig file. The bigWig format is for display of dense, continuous\ndata and the data will be displayed as a graph. The resulting bigWig\nfiles are in an indexed binary format.\n\n\nThe BAM to bigWig conversion takes place in two steps. Firstly, we\nconvert the BAM file into a bedgraph, called \nOct4.bedgraph\n, using the\ntool \ngenomeCoverageBed\n from BEDTools. Then we convert the bedgraph\ninto a bigWig binary file called \nOct4.bw\n, using \nbedGraphToBigWig\n\nfrom the UCSC tools:\n\n\n1\n2\ngenomeCoverageBed -bg -ibam Oct4.sorted.bam -g bowtie_index/mouse.mm10.genome \n Oct4.bedgraph\nbedGraphToBigWig Oct4.bedgraph bowtie_index/mouse.mm10.genome Oct4.bw\n\n\n\n\n\n\nBoth of the commands above take as input a file called\n\nmouse.mm10.genome\n that is stored under the subdirectory\n\nbowtie_index\n. These genome files are tab-delimited and describe the\nsize of the chromosomes for the organism of interest. When using the\nUCSC Genome Browser, Ensembl, or Galaxy, you typically indicate which\nspecies/genome build you are working with. The way you do this for\nBEDTools is to create a \u201cgenome\u201d file, which simply lists the names of\nthe chromosomes (or scaffolds, etc.) and their size (in basepairs).\n\n\nBEDTools includes pre-defined genome files for human and mouse in the\n\ngenomes\n subdirectory included in the BEDTools distribution.\n\n\nNow we will load the data into the IGV browser for visualization. In\norder to launch IGV double click on the \nIGV 2.3\n icon on your Desktop.\nIgnore any warnings and when it opens you have to load the genome of\ninterest.\n\n\nOn the top left of your screen choose from the drop down menu\n\nMouse (mm10)\n. If it doesn\u2019t appear in list, click \nMore ..\n, type\n\nmm10\n in the Filter section, choose the mouse genome and press OK. Then\nin order to load the desire files go to:\n\n\n1\nFile \n Load from File\n\n\n\n\n\n\nOn the pop up window navigate to Desktop $\n$ chipseq folder and select\nthe file \nOct4.sorted.bam\n.\n\n\nRepeat these steps in order to load \nOct4.bw\n as well.\n\n\nSelect \nchr1\n from the drop down menu on the top left. Right click on\nthe name of \nOct4.bw\n and choose Maximum under the Windowing Function.\nRight click again and select Autoscale.\n\n\nIn order to see the aligned reads of the BAM file, you need to zoom in\nto a specific region. For example, look for gene \nLemd1\n in the search\nbox.\n\n\nWhat is the main difference between the visualization of BAM and bigWig\nfiles?\n\n\nThe actual alignment of reads that stack to a particular region can be\ndisplayed using the information stored in a BAM format. The bigWig\nformat is for display of dense, continuous data that will be displayed\nin the Genome Browser as a graph.\n\n\nUsing the \n+\n button on the top right, zoom in to see more of the\ndetails of the alignments.\n\n\nWhat do you think the different colors mean?\n\n\nThe different color represents four nucleotides, e.g. blue is Cytidine\n(C), red is Thymidine (T).\n\n\nPractice Makes Perfect!\n\n\nIn the chipseq folder you will find the file \ngfp.fastq\n. Follow the\nabove described analysis, from the bowtie2 alignment step, for this\ndataset as well. You will need these files for the ChIP-Seq module.", 
            "title": "Read Alignment"
        }, 
        {
            "location": "/modules/btp-module-ngs-mapping/alignment/#key-learning-outcomes", 
            "text": "After completing this practical the trainee should be able to:    Perform a simple NGS data alignment task, with Bowtie2, against one\n    interested reference data    Interpret and manipulate the mapping output using SAMtools    Visualise the alignment via a standard genome browser, e.g. IGV\n    browser", 
            "title": "Key Learning Outcomes"
        }, 
        {
            "location": "/modules/btp-module-ngs-mapping/alignment/#resources-youll-be-using", 
            "text": "", 
            "title": "Resources You\u2019ll be Using"
        }, 
        {
            "location": "/modules/btp-module-ngs-mapping/alignment/#tools-used", 
            "text": "Bowtie2:  http://bowtie-bio.sourceforge.net/bowtie2/index.shtml  Samtools:  http://broadinstitute.github.io/picard  BEDTools:  http://code.google.com/p/bedtools/  UCSC tools:  http://hgdownload.cse.ucsc.edu/admin/exe/  IGV genome browser:  http://www.broadinstitute.org/igv/", 
            "title": "Tools Used"
        }, 
        {
            "location": "/modules/btp-module-ngs-mapping/alignment/#useful-links", 
            "text": "SAM Specification:  http://samtools.sourceforge.net/SAM1.pdf  Explain SAM Flags:  https://broadinstitute.github.io/picard/explain-flags.html", 
            "title": "Useful Links"
        }, 
        {
            "location": "/modules/btp-module-ngs-mapping/alignment/#sources-of-data", 
            "text": "http://www.ebi.ac.uk/arrayexpress/experiments/E-GEOD-11431", 
            "title": "Sources of Data"
        }, 
        {
            "location": "/modules/btp-module-ngs-mapping/alignment/#introduction", 
            "text": "The goal of this hands-on session is to perform an unspliced alignment\nfor a small subset of raw reads. We will align raw sequencing data to\nthe mouse genome using Bowtie2 and then we will manipulate the SAM\noutput in order to visualize the alignment on the IGV browser.", 
            "title": "Introduction"
        }, 
        {
            "location": "/modules/btp-module-ngs-mapping/alignment/#prepare-the-environment", 
            "text": "We will use one data set in this practical, which can be found in the ChIP-seq  directory on your desktop.  Open the Terminal.  First, go to the right folder, where the data are stored.  1 cd /home/trainee/chipseq   The  .fastq  file that we will align is called  Oct4.fastq . This file\nis based on Oct4 ChIP-seq data published by Chen  et al.  (2008). For\nthe sake of time, we will align these reads to a single mouse\nchromosome.", 
            "title": "Prepare the Environment"
        }, 
        {
            "location": "/modules/btp-module-ngs-mapping/alignment/#alignment", 
            "text": "You already know that there are a number of competing tools for short\nread alignment, each with its own set of strengths, weaknesses, and\ncaveats. Here we will try Bowtie2, a widely used ultrafast, memory\nefficient short read aligner.  Bowtie2 has a number of parameters in order to perform the alignment. To\nview them all type  1 bowtie2 --help   Bowtie2 uses indexed genome for the alignment in order to keep its\nmemory footprint small. Because of time constraints we will build the\nindex only for one chromosome of the mouse genome. For this we need the\nchromosome sequence in FASTA format. This is stored in a file named mm10 , under the subdirectory  bowtie_index .  The indexed chromosome is generated using the command:  1 bowtie2-build bowtie_index/mm10.fa bowtie_index/mm10   This command will output 6 files that constitute the index. These files\nthat have the prefix  mm10  are stored in the  bowtie_index \nsubdirectory. To view if they files have been successfully created type:  1 ls -l bowtie_index   Now that the genome is indexed we can move on to the actual alignment.\nThe first argument for  bowtie2  is the basename of the index for the\ngenome to be searched; in our case this is  mm10 . We also want to make\nsure that the output is in SAM format using the  -S  parameter. The last\nargument is the name of the FASTQ file.  Align the Oct4 reads using Bowtie2:  1 bowtie2 -x bowtie_index/mm10 -q Oct4.fastq   Oct4.sam   The above command outputs the alignment in SAM format and stores them in\nthe file  Oct4.sam .  In general before you run Bowtie2, you have to know what quality\nencoding your FASTQ files are in. The available FASTQ encodings for\nbowtie are:  \u2013phred33-quals\n:   Input qualities are Phred+33 (default).  \u2013phred64-quals\n:   Input qualities are Phred+64 (same as  \u2013solexa1.3-quals ).  \u2013solexa-quals\n:   Input qualities are from GA Pipeline ver. $ $ 1.3.  \u2013solexa1.3-quals\n:   Input qualities are from GA Pipeline ver. $\\geq$ 1.3.  \u2013integer-quals\n:   Qualities are given as space-separated integers (not ASCII).  The FASTQ files we are working with are Sanger encoded (Phred+33), which\nis the default for Bowtie2.  Bowtie2 will take 2-3 minutes to align the file. This is fast compared\nto other aligners which sacrifice some speed to obtain higher\nsensitivity.  Look at the top 10 lines of the SAM file using head (record lines are\nwrapped). Then try the second command, note use arrow navigation and to\nexit type \u2019q\u2019.  1\n2 head  Oct4.sam\nless -S Oct4.sam   Can you distinguish between the header of the SAM format and the actual\nalignments?  The header line starts with the letter \u2018@\u2019, i.e.:   @HD   VN:1.0       SO:unsorted              \n  @SQ   SN:chr1      LN:195471971             \n  @PG   ID:Bowtie2   PN:bowtie2     VN:2.2.4   CL:\u201c/tools/bowtie2/bowtie2-default/bowtie2-align-s \u2013wrapper basic-0 -x bowtie_index/mm10 -q Oct4.fastq\u201d   While, the actual alignments start with read id, i.e.:   SRR002012.45   0    etc   \n  SRR002012.48   16   chr1   etc   What kind of information does the header provide?    @HD: Header line; VN: Format version; SO: the sort order of\n    alignments.    @SQ: Reference sequence information; SN: reference sequence name;\n    LN: reference sequence length.    @PG: Read group information; ID: Read group identifier; VN: Program\n    version; CL: the command line that produces the alignment.    To which chromosome are the reads mapped?  Chromosome 1.", 
            "title": "Alignment"
        }, 
        {
            "location": "/modules/btp-module-ngs-mapping/alignment/#manipulate-sam-output", 
            "text": "SAM files are rather big and when dealing with a high volume of NGS\ndata, storage space can become an issue. As we have already seen, we can\nconvert SAM to BAM files (their binary equivalent that are not human\nreadable) that occupy much less space.  Convert SAM to BAM using  samtools view  and store the output in the\nfile  Oct4.bam . You have to instruct  samtools view  that the input is\nin SAM format ( -S ), the output should be in BAM format ( -b ) and that\nyou want the output to be stored in the file specified by the  -o \noption:  1 samtools view -bSo Oct4.bam Oct4.sam   Compute summary stats for the Flag values associated with the alignments\nusing:  1 samtools flagstat Oct4.bam", 
            "title": "Manipulate SAM output"
        }, 
        {
            "location": "/modules/btp-module-ngs-mapping/alignment/#visualize-alignments-in-igv", 
            "text": "IGV is a stand-alone genome browser. Please check their website\n( http://www.broadinstitute.org/igv/ ) for all the formats that IGV can\ndisplay. For our visualization purposes we will use the BAM and bigWig\nformats.  When uploading a BAM file into the genome browser, the browser will look\nfor the index of the BAM file in the same folder where the BAM files is.\nThe index file should have the same name as the BAM file and the suffix .bai . Finally, to create the index of a BAM file you need to make sure\nthat the file is sorted according to chromosomal coordinates.  Sort alignments according to chromosomal position and store the result\nin the file with the prefix  Oct4.sorted :  1 samtools sort Oct4.bam Oct4.sorted   Index the sorted file.  1 samtools index Oct4.sorted.bam   The indexing will create a file called  Oct4.sorted.bam.bai . Note that\nyou don\u2019t have to specify the name of the index file when running samtools index , it simply appends a  .bai  suffix to the input BAM\nfile.  Another way to visualize the alignments is to convert the BAM file into\na bigWig file. The bigWig format is for display of dense, continuous\ndata and the data will be displayed as a graph. The resulting bigWig\nfiles are in an indexed binary format.  The BAM to bigWig conversion takes place in two steps. Firstly, we\nconvert the BAM file into a bedgraph, called  Oct4.bedgraph , using the\ntool  genomeCoverageBed  from BEDTools. Then we convert the bedgraph\ninto a bigWig binary file called  Oct4.bw , using  bedGraphToBigWig \nfrom the UCSC tools:  1\n2 genomeCoverageBed -bg -ibam Oct4.sorted.bam -g bowtie_index/mouse.mm10.genome   Oct4.bedgraph\nbedGraphToBigWig Oct4.bedgraph bowtie_index/mouse.mm10.genome Oct4.bw   Both of the commands above take as input a file called mouse.mm10.genome  that is stored under the subdirectory bowtie_index . These genome files are tab-delimited and describe the\nsize of the chromosomes for the organism of interest. When using the\nUCSC Genome Browser, Ensembl, or Galaxy, you typically indicate which\nspecies/genome build you are working with. The way you do this for\nBEDTools is to create a \u201cgenome\u201d file, which simply lists the names of\nthe chromosomes (or scaffolds, etc.) and their size (in basepairs).  BEDTools includes pre-defined genome files for human and mouse in the genomes  subdirectory included in the BEDTools distribution.  Now we will load the data into the IGV browser for visualization. In\norder to launch IGV double click on the  IGV 2.3  icon on your Desktop.\nIgnore any warnings and when it opens you have to load the genome of\ninterest.  On the top left of your screen choose from the drop down menu Mouse (mm10) . If it doesn\u2019t appear in list, click  More .. , type mm10  in the Filter section, choose the mouse genome and press OK. Then\nin order to load the desire files go to:  1 File   Load from File   On the pop up window navigate to Desktop $ $ chipseq folder and select\nthe file  Oct4.sorted.bam .  Repeat these steps in order to load  Oct4.bw  as well.  Select  chr1  from the drop down menu on the top left. Right click on\nthe name of  Oct4.bw  and choose Maximum under the Windowing Function.\nRight click again and select Autoscale.  In order to see the aligned reads of the BAM file, you need to zoom in\nto a specific region. For example, look for gene  Lemd1  in the search\nbox.  What is the main difference between the visualization of BAM and bigWig\nfiles?  The actual alignment of reads that stack to a particular region can be\ndisplayed using the information stored in a BAM format. The bigWig\nformat is for display of dense, continuous data that will be displayed\nin the Genome Browser as a graph.  Using the  +  button on the top right, zoom in to see more of the\ndetails of the alignments.  What do you think the different colors mean?  The different color represents four nucleotides, e.g. blue is Cytidine\n(C), red is Thymidine (T).", 
            "title": "Visualize alignments in IGV"
        }, 
        {
            "location": "/modules/btp-module-ngs-mapping/alignment/#practice-makes-perfect", 
            "text": "In the chipseq folder you will find the file  gfp.fastq . Follow the\nabove described analysis, from the bowtie2 alignment step, for this\ndataset as well. You will need these files for the ChIP-Seq module.", 
            "title": "Practice Makes Perfect!"
        }, 
        {
            "location": "/modules/btp-module-rna-seq/rna-seq/", 
            "text": "Key Learning Outcomes\n\n\nAfter completing this practical the trainee should be able to:\n\n\n\n\n\n\nUnderstand and perform a simple RNA-Seq analysis workflow.\n\n\n\n\n\n\nPerform spliced alignments to an indexed reference genome using\n    TopHat.\n\n\n\n\n\n\nVisualize spliced transcript alignments in a genome browser such as\n    IGV.\n\n\n\n\n\n\nBe able to identify differential gene expression between two\n    experimental conditions.\n\n\n\n\n\n\nBe familiar with R environment and be able to run R based RNA-seq\n    packages.\n\n\n\n\n\n\nWe also have bonus exercises where you can learn to:\n\n\n\n\n\n\nPerform transcript assembly using Cufflinks.\n\n\n\n\n\n\nRun cuffdiff, a Cufflinks utility for differential expression\n    analysis.\n\n\n\n\n\n\nVisualize transcript alignments and annotation in a genome browser\n    such as IGV.\n\n\n\n\n\n\nResources You\u2019ll be Using\n\n\nTools Used\n\n\nTophat:\n\n\nhttps://ccb.jhu.edu/software/tophat/index.shtml\n\n\nCufflinks:\n\n\nhttp://cole-trapnell-lab.github.io/cufflinks/\n\n\nSamtools:\n\n\nhttp://www.htslib.org/\n\n\nBEDTools:\n\n\nhttps://github.com/arq5x/bedtools2\n\n\nUCSC tools:\n\n\nhttp://hgdownload.cse.ucsc.edu/admin/exe/\n\n\nIGV:\n\n\nhttp://www.broadinstitute.org/igv/\n\n\nFeatureCount:\n\n\nhttp://subread.sourceforge.net/\n\n\nedgeR package:\n\n\nhttps://bioconductor.org/packages/release/bioc/html/edgeR.html\n\n\nCummeRbund manual:\n\n\nhttp://www.bioconductor.org/packages/release/bioc/vignettes/cummeRbund/inst/doc/cummeRbund-manual.pdf\n\n\nSources of Data\n\n\nhttp://www.ebi.ac.uk/ena/data/view/ERR022484\n\n\nhttp://www.ebi.ac.uk/ena/data/view/ERR022485\n\n\nhttp://www.pnas.org/content/suppl/2008/12/16/0807121105.DCSupplemental\n\n\nIntroduction\n\n\nThe goal of this hands-on session is to perform some basic tasks in the\ndownstream analysis of RNA-seq data.\\\n\n\nFirst we will use RNA-seq data from zebrafish. You will align one set of\nreads to the zebrafish using Tophat2. You will then view the aligned\nreads using the IGV viewer. We will also demonstrate how gene counts can\nbe derived from this data. You will go on to assembly a transcriptome\nfrom the read data using cufflinks. We will show you how this type of\ndata may be analysed for differential expression.\n\n\nThe second part of the tutorial will focus on RNA-seq data from a human\nexperiment (cancer cell line versus normal cells). You will use the\nBioconductor packages edgeR and voom (limma) to determine differential\ngene expression. The results from this analysis will then be used in the\nfinal session which introduces you to some of the tools used to gain\nbiological insight into the results of a differential expression\nanalysis\n\n\nPrepare the Environment\n\n\nWe will use a dataset derived from sequencing of mRNA from \nDanio rerio\n\nembryos in two different developmental stages. Sequencing was performed\non the Illumina platform and generated 76bp paired-end sequence data\nusing polyA selected RNA. Due to the time constraints of the practical\nwe will only use a subset of the reads.\n\n\nThe data files are contained in the subdirectory called \ndata\n and are\nthe following:\n\n\n2cells_1.fastq\n and \n2cells_2.fastq\n\n:   \\\n    These files are based on RNA-seq data of a 2-cell zebrafish embryo\n\n\n6h_1.fastq\n and \n6h_2.fastq\n\n:   \\\n    These files are based on RNA-seq data of zebrafish embryos 6h post\n    fertilization\n\n\nOpen the Terminal and go to the \nrnaseq\n working directory:\n\n\n1\ncd /home/trainee/rnaseq/\n\n\n\n\n\n\nAll commands entered into the terminal for this tutorial should be from\nwithin the \n/home/trainee/rnaseq\n directory.\n\n\nCheck that the \ndata\n directory contains the above-mentioned files by\ntyping:\n\n\n1\nls data\n\n\n\n\n\n\nAlignment\n\n\nThere are numerous tools for performing short read alignment and the\nchoice of aligner should be carefully made according to the analysis\ngoals/requirements. Here we will use Tophat2, a widely used ultrafast\naligner that performs spliced alignments.\n\n\nTophat2 is based on the Bowtie2 aligner and uses an indexed genome for\nthe alignment to speed up the alignment and keep its memory footprint\nsmall. The the index for the \nDanio rerio\n genome has been created for\nyou.\n\n\nThe command to create an index is as follows. You DO NOT need to run\nthis command yourself - we have done this for you.\n\n\n1\nbowtie2-build genome/Danio_rerio.Zv9.66.dna.fa genome/ZV9\n\n\n\n\n\n\nTophat2 has a number of parameters in order to perform the alignment. To\nview them all type:\n\n\n1\ntophat2 --help\n\n\n\n\n\n\nThe general format of the tophat2 command is:\n\n\n1\ntophat2 [options]* \nindex_base\n \nreads_1\n \nreads_2\n\n\n\n\n\n\n\nWhere the last two arguments are the \n.fastq\n files of the paired end\nreads, and the argument before is the basename of the indexed genome.\n\n\nThe quality values in the FASTQ files used in this hands-on session are\nPhred+33 encoded. We explicitly tell tophat of this fact by using the\ncommand line argument \n\u2013solexa-quals\n.\n\n\nYou can look at the first few reads in the file \ndata/2cells_1.fastq\n\nwith:\n\n\n1\nhead -n 20 data/2cells_1.fastq\n\n\n\n\n\n\nSome other parameters that we are going to use to run Tophat are listed\nbelow:\n\n\n-g\n:   Maximum number of multihits allowed. Short reads are likely to map\n    to more than one location in the genome even though these reads can\n    have originated from only one of these regions. In RNA-seq we allow\n    for a limited number of multihits, and in this case we ask Tophat to\n    report only reads that map at most onto 2 different loci.\n\n\n\u2013library-type\n:   Before performing any type of RNA-seq analysis you need to know a\n    few things about the library preparation. Was it done using a\n    strand-specific protocol or not? If yes, which strand? In our data\n    the protocol was NOT strand specific.\n\n\n-j\n:   Improve spliced alignment by providing Tophat with annotated splice\n    junctions. Pre-existing genome annotation is an advantage when\n    analysing RNA-seq data. This file contains the coordinates of\n    annotated splice junctions from Ensembl. These are stored under the\n    sub-directory \nannotation\n in a file called \nZV9.spliceSites\n.\n\n\n-o\n:   This specifies in which subdirectory Tophat should save the output\n    files. Given that for every run the name of the output files is the\n    same, we specify different directories for each run.\n\n\nIt takes some time (approx. 20 min) to perform tophat spliced\nalignments, even for this subset of reads. Therefore, we have\npre-aligned the \n2cells\n data for you using the following command:\n\n\nYou DO NOT need to run this command yourself - we have done this for\nyou.\n\n\n1\ntophat2 --solexa-quals -g 2 --library-type fr-unstranded -j annotation/Danio_rerio.Zv9.66.spliceSites -o tophat/ZV9_2cells genome/ZV9 data/2cells_1.fastq data/2cells_2.fastq\n\n\n\n\n\n\nAlign the \n6h\n data yourself using the following command:\n\n\n1\n2\n# Takes approx. 20mins\ntophat2 --solexa-quals -g 2 --library-type fr-unstranded -j annotation/Danio_rerio.Zv9.66.spliceSites -o tophat/ZV9_6h genome/ZV9 data/6h_1.fastq data/6h_2.fastq\n\n\n\n\n\n\nThe \n6h\n read alignment will take approx. 20 min to complete. Therefore,\nwe\u2019ll take a look at some of the files, generated by tophat, for the\npre-computed \n2cells\n data.\n\n\nTophat generates several files in the specified output directory. The\nmost important files are listed below.\n\n\naccepted_hits.bam\n:   This file contains the list of read alignments in BAM format.\n\n\nalign_summary.txt\n:   Provides a detailed summary of the read-alignments.\n\n\nunmapped.bam\n:   This file contains the unmapped reads.\n\n\nThe complete documentation can be found at:\n\nhttps://ccb.jhu.edu/software/tophat/manual.shtml\n\n\nAlignment Visualisation in IGV\n\n\nThe Integrative Genomics Viewer (IGV) is able to provide a visualisation\nof read alignments given a reference sequence and a BAM file. We\u2019ll\nvisualise the information contained in the \naccepted_hits.bam\n and\n\njunctions.bed\n files for the pre-computed \n2cells\n data. The former,\ncontains the tophat spliced alignments of the reads to the reference\nwhile the latter stores the coordinates of the splice junctions present\nin the data set.\n\n\nOpen the \nrnaseq\n directory on your Desktop and double-click the\n\ntophat\n subdirectory and then the \nZV9_2cells\n directory.\n\n\n\n\n\n\nLaunch IGV by double-clicking the \u201cIGV 2.3.*\u201d icon on the Desktop\n    (ignore any warnings that you may get as it opens). \nNOTE: IGV may\n    take several minutes to load for the first time, please be patient.\n\n\n\n\n\n\nChoose \u201cZebrafish (Zv9)\u201d from the drop-down box in the top left of\n    the IGV window. Else you can also load the genome fasta file.\n\n\n\n\n\n\nLoad the \naccepted_hits.sorted.bam\n file by clicking the \u201cFile\u201d\n    menu, selecting \u201cLoad from File\u201d and navigating to the\n    \nDesktop/rnaseq/tophat/ZV9_2cells\n directory.\n\n\n\n\n\n\nRename the track by right-clicking on its name and choosing \u201cRename\n    Track\u201d. Give it a meaningful name like \u201c2cells BAM\u201d.\n\n\n\n\n\n\nLoad the \njunctions.bed\n from the same directory and rename the\n    track \u201c2cells Junctions BED\u201d.\n\n\n\n\n\n\nLoad the Ensembl annotations file \nDanio_rerio.Zv9.66.gtf\n stored in\n    the \nrnaseq/annotation\n directory.\n\n\n\n\n\n\nNavigate to a region on chromosome 12 by typing\n    \nchr12\n:\n20\n,\n270\n,\n921\n-\n20\n,\n300\n,\n943\n into the search box at the top of the\n    IGV window.\n\n\n\n\n\n\nKeep zooming to view the bam file alignments\n\n\nSome useful IGV manuals can be found below\n\n\nhttp://www.broadinstitute.org/software/igv/interpreting_insert_size\n\n\nhttp://www.broadinstitute.org/software/igv/alignmentdata\n  \n\n\nDoes the file \u2019align_summary.txt\u2019 look interesting? What information\ndoes it provide?\n\n\nAs the name suggests, the file provides a details summary of the\nalignment statistics.\n\n\nOne other important file is \u2019unmapped.bam\u2019. This file contains the\nunampped reads.\n\n\nCan you identify the splice junctions from the BAM file?\n\n\nSplice junctions can be identified in the alignment BAM files. These are\nthe aligned RNA-Seq reads that have skipped-bases from the reference\ngenome (most likely introns).\n\n\nAre the junctions annotated for \nCBY1\n consistent with the annotation?\n\n\nRead alignment supports an extended length in exon 5 to the gene model\n(cby1-001)\n\n\nOnce tophat finishes aligning the 6h data you will need to sort the\nalignments found in the BAM file and then index the sorted BAM file.\n\n\n1\n2\nsamtools sort tophat/ZV9_6h/accepted_hits.bam tophat/ZV9_6h/accepted_hits.sorted\nsamtools index tophat/ZV9_6h/accepted_hits.sorted.bam\n\n\n\n\n\n\nLoad the sorted BAM file into IGV, as described previously, and rename\nthe track appropriately.\n\n\nGenerating Gene Counts\n\n\nIn RNAseq experiments the digital gene expression is recorded as the\ngene counts or number of reads aligning to a known gene feature. If you\nhave a well annotated genome, you can use the gene structure file in a\nstandard gene annotation format (GTF or GFF)) along with the spliced\nalignment file to quantify the known genes. We will demonstrate a\nutility called \nFeatureCounts\n that comes with the \nSubread\n package.\n\n\n1\n2\nmkdir gene_counts\nfeatureCounts -a annotation/Danio_rerio.Zv9.66.gtf -t exon -g gene_id -o gene_counts/gene_counts.txt tophat/ZV9_6h/accepted_hits.sorted.bam tophat/ZV9_2cells/accepted_hits.sorted.bam\n\n\n\n\n\n\nIsoform Expression and Transcriptome Assembly\n\n\nFor non-model organisms and genomes with draft assemblies and incomplete\nannotations, it is a common practice to take and assembly based approach\nto generate gene structures followed by the quantification step. There\nare a number of reference based transcript assemblers available that can\nbe used for this purpose such as, cufflinks, stringy. These assemblers\ncan give gene or isoform level assemblies that can be used to perform a\ngene/isoform level quantification. These assemblers require an alignment\nof reads with a reference genome or transcriptome as an input. The\nsecond optional input is a known gene structure in \nGTF\n or \nGFF\n\nformat.\n\n\nThere are a number of tools that perform reconstruction of the\ntranscriptome and for this workshop we are going to use Cufflinks.\nCufflinks can do transcriptome assembly either \nab initio\n or using a\nreference annotation. It also quantifies the isoform expression in\nFragments Per Kilobase of exon per Million fragments mapped (FPKM).\n\n\nCufflinks has a number of parameters in order to perform transcriptome\nassembly and quantification. To view them all type:\n\n\n1\ncufflinks --help\n\n\n\n\n\n\nWe aim to reconstruct the transcriptome for both samples by using the\nEnsembl annotation both strictly and as a guide. In the first case\nCufflinks will only report isoforms that are included in the annotation,\nwhile in the latter case it will report novel isoforms as well.\n\n\nThe Ensembl annotation for \nDanio rerio\n is available in\n\nannotation/Danio_rerio.Zv9.66.gtf\n.\n\n\nThe general format of the \ncufflinks\n command is:\n\n\n1\ncufflinks [options]* \naligned_reads.(sam|bam)\n\n\n\n\n\n\n\nWhere the input is the aligned reads (either in SAM or BAM format).\n\n\nSome of the available parameters for Cufflinks that we are going to use\nto run Cufflinks are listed below:\n\n\n-o\n:   Output directory.\n\n\n-G\n:   Tells Cufflinks to use the supplied GTF annotations strictly in\n    order to estimate isoform annotation.\n\n\n-b\n:   Instructs Cufflinks to run a bias detection and correction algorithm\n    which can significantly improve accuracy of transcript abundance\n    estimates. To do this Cufflinks requires a multi-fasta file with the\n    genomic sequences against which we have aligned the reads.\n\n\n-u\n:   Tells Cufflinks to do an initial estimation procedure to more\n    accurately weight reads mapping to multiple locations in the genome\n    (multi-hits).\n\n\n\u2013library-type\n:   Before performing any type of RNA-seq analysis you need to know a\n    few things about the library preparation. Was it done using a\n    strand-specific protocol or not? If yes, which strand? In our data\n    the protocol was NOT strand specific.\n\n\nPerform transcriptome assembly, strictly using the supplied GTF\nannotations, for the \n2cells\n and \n6h\n data using cufflinks:\n\n\n1\n2\n3\n4\n# 2cells data (takes approx. 5mins):\ncufflinks -o cufflinks/ZV9_2cells_gtf -G annotation/Danio_rerio.Zv9.66.gtf -b genome/Danio_rerio.Zv9.66.dna.fa -u --library-type fr-unstranded tophat/ZV9_2cells/accepted_hits.bam\n# 6h data (takes approx. 5mins):\ncufflinks -o cufflinks/ZV9_6h_gtf -G annotation/Danio_rerio.Zv9.66.gtf -b genome/Danio_rerio.Zv9.66.dna.fa -u --library-type fr-unstranded tophat/ZV9_6h/accepted_hits.bam\n\n\n\n\n\n\nCufflinks generates several files in the specified output directory.\nHere\u2019s a short description of these files:\n\n\ngenes.fpkm_tracking\n:   Contains the estimated gene-level expression values.\n\n\nisoforms.fpkm_tracking\n:   Contains the estimated isoform-level expression values.\n\n\nskipped.gtf\n:   Contains loci skipped as a result of exceeding the maximum number of\n    fragments.\n\n\ntranscripts.gtf\n:   This GTF file contains Cufflinks\u2019 assembled isoforms.\n\n\nThe complete documentation can be found at:\n\n\nhttp://cole-trapnell-lab.github.io/cufflinks/file_formats/#output-formats-used-in-the-cufflinks-suite\n\n\nSo far we have forced cufflinks, by using the \n-G\n option, to strictly\nuse the GTF annotations provided and thus novel transcripts will not be\nreported. We can get cufflinks to perform a GTF-guided transcriptome\nassembly by using the \n-g\n option instead. Thus, novel transcripts will\nbe reported.\n\n\nGTF-guided transcriptome assembly is more computationally intensive than\nstrictly using the GTF annotations. Therefore, we have pre-computed\nthese GTF-guided assemblies for you and have placed the results under\nsubdirectories:\n\n\ncufflinks/ZV9_2cells_gtf_guided\n and \ncufflinks/ZV9_6h_gft_guided\n.\n\n\nYou DO NOT need to run these commands. We provide them so you know how\nwe generated the the GTF-guided transcriptome assemblies:\n\n\n1\n2\n3\n4\n# 2cells guided transcriptome assembly (takes approx. 30mins):\ncufflinks -o cufflinks/ZV9_2cells_gtf_guided -g annotation/Danio_rerio.Zv9.66.gtf -b genome/Danio_rerio.Zv9.66.dna.fa -u --library-type fr-unstranded tophat/ZV9_2cells/accepted_hits.bam\n# 6h guided transcriptome assembly (takes approx. 30mins):\ncufflinks -o cufflinks/ZV9_6h_gtf_guided -g annotation/Danio_rerio.Zv9.66.gtf -b genome/Danio_rerio.Zv9.66.dna.fa -u --library-type fr-unstranded tophat/ZV9_6h/accepted_hits.bam\n\n\n\n\n\n\n\n\n\n\nGo back to IGV and load the pre-computed, GTF-guided transcriptome\n    assembly for the \n2cells\n data\n    (\ncufflinks/ZV9_2cells_gtf_guided/transcripts.gtf\n).\n\n\n\n\n\n\nRename the track as \u201c2cells GTF-Guided Transcripts\u201d.\n\n\n\n\n\n\nIn the search box type \nENSDART00000082297\n in order for the browser\n    to zoom in to the gene of interest.\n\n\n\n\n\n\nDo you observe any difference between the Ensembl GTF annotations and\nthe GTF-guided transcripts assembled by cufflinks (the \u201c2cells\nGTF-Guided Transcripts\u201d track)?\n\n\nYes. It appears that the Ensembl annotations may have truncated the last\nexon. However, our data also doesn\u2019t contain reads that span between the\nlast two exons.\n\n\nDifferential Gene Expression Analysis using edgeR\n\n\nExperiment Design\n\n\nThe example we are working through today follows a case Study set out in\nthe edgeR Users Guide (4.3 Androgen-treated prostate cancer cells\n(RNA-Seq, two groups) which is based on an experiment conducted by Li et\nal. (2008, Proc Natl Acad Sci USA, 105, 20179-84).\n\n\nThe researches used a prostate cancer cell line (LNCaP cells). These\ncells are sensitive to stimulation by male hormones (androgens). Three\nreplicate RNA samples were collected from LNCaP cells treated with an\nandrogen hormone (DHT). Four replicates were collected from cells\ntreated with an inactive compound. Each of the seven samples was run on\na lane (7 lanes) of an Illumina flow cell to produce 35 bp reads. The\nexperimental design was therefore:\n\n\n[H]\n\n\nrrr\n\n\nLane\n \n \nTreatment\n \n \nLabel\n\\\n1 \n Control \n Con1\\\n2 \n Control \n Con2\\\n3 \n Control \n Con3\\\n4 \n Control \n Con4\\\n5 \n DHT \n DHT1\\\n6 \n DHT \n DHT2\\\n7 \n DHT \n DHT3\\\n\n\n[tab:experimental\nd\nesign]\n\n\nThis workflow requires raw gene count files and these can be generated\nusing a utility called featureCounts as demonstrated above. We are using\na pre-computed gene counts data (stored in \npnas_expression.txt\n) for\nthis exercise.\n\n\nPrepare the Environment\n\n\nPrepare the environment and load R:\n\n\n1\n2\ncd /home/trainee/rnaseq/edgeR\nR (press enter)\n\n\n\n\n\n\nOnce on the R prompt. Load libraries:\n\n\n1\n2\n3\n4\n5\n6\nlibrary(edgeR)\nlibrary(biomaRt)\nlibrary(gplots)\nlibrary(\nlimma\n)\nlibrary(\nRColorBrewer\n)\nlibrary(\norg.Hs.eg.db\n)\n\n\n\n\n\n\nRead in Data\n\n\nRead in count table and experimental design:\n\n\n1\n2\n3\n4\ndata \n-\n read.delim\n(\npnas_expression.txt\n,\n row.names\n=\n1\n,\n header\n=\nT\n)\n\ntargets \n-\n read.delim\n(\nTargets.txt\n,\n header\n=\nT\n)\n\n\ncolnames\n(\ndata\n)\n \n-\ntargets\n$\nLabel\n\nhead\n(\ndata\n,\n n\n=\n20\n)\n\n\n\n\n\n\n\nAdd Gene annotation\n\n\nThe data set only includes the Ensembl gene id and the counts. It is\nuseful to have other annotations such as the gene symbol and entrez id.\nNext we will add in these annotations. We will use the BiomaRt package\nto do this.\n\n\nWe start by using the useMart function of BiomaRt to access the human\ndata base of ensemble gene ids.\n\n\n1\nhuman\n-\nuseMart\n(\nhost\n=\nwww.ensembl.org\n,\n \nENSEMBL_MART_ENSEMBL\n,\n dataset\n=\nhsapiens_gene_ensembl\n)\n attributes\n=\nc\n(\nensembl_gene_id\n,\n \nentrezgene\n,\nhgnc_symbol\n)\n\n\n\n\n\n\n\nWe create a vector of our ensemble gene ids.\n\n\n1\n2\nensembl_names\n-\nrownames\n(\ndata\n)\n\n\nhead\n(\nensembl_names\n)\n\n\n\n\n\n\n\nWe then use the function getBM to get the gene symbol data we want.This\ntakes about a minute.\n\n\n1\ngenemap\n-\ngetBM\n(\nattributes\n,\n filters\n=\nensembl_gene_id\n,\n values\n=\nensembl_names\n,\n mart\n=\nhuman\n)\n\n\n\n\n\n\n\nHave a look at the start of the genemap dataframe.\n\n\n1\nhead(genemap)\n\n\n\n\n\n\nWe then match the data we have retrieved to our dataset.\n\n\n1\n2\n3\n4\n5\n6\nidx \n-\nmatch\n(\nensembl_names\n,\n genemap\n$\nensembl_gene_id\n)\n\ndata\n$\nentrezgene \n-\ngenemap\n$\nentrezgene \n[\n idx \n]\n\ndata\n$\nhgnc_symbol \n-\ngenemap\n$\nhgnc_symbol \n[\n idx \n]\n\nAnn \n-\n \ncbind\n(\nrownames\n(\ndata\n),\n data\n$\nhgnc_symbol\n,\n data\n$\nentrezgene\n)\n\n\ncolnames\n(\nAnn\n)\n-\nc\n(\nEnsembl\n,\n \nSymbol\n,\n \nEntrez\n)\n\nAnn\n-\nas.data.frame\n(\nAnn\n)\n\n\n\n\n\n\n\nLet\u2019s check and see that this additional information is there.\n\n\n1\nhead(data)\n\n\n\n\n\n\nData checks\n\n\nCreate DGEList object:\n\n\n1\n2\ntreatment \n-\nfactor\n(\nc\n(\nrep\n(\nControl\n,\n4\n),\n \nrep\n(\nDHT\n,\n3\n)),\n levels\n=\nc\n(\nControl\n,\n \nDHT\n))\n\ny \n-\nDGEList\n(\ncounts\n=\ndata\n[,\n1\n:\n7\n],\n group\n=\ntreatment\n,\n genes\n=\nAnn\n)\n\n\n\n\n\n\n\nCheck the dimensions of the object:\n\n\n1\ndim(y)\n\n\n\n\n\n\nWe see we have 37435 rows (i.e. genes) and 7 columns (samples).\n\n\nNow we will filter out genes with low counts by only keeping those rows\nwhere the count per million (cpm) is at least 1 in at least three\nsamples:\n\n\n1\n2\nkeep \n-\nrowSums\n(\n cpm\n(\ny\n)\n1\n)\n \n=\n3\n\ny \n-\n y\n[\nkeep\n,\n \n]\n\n\n\n\n\n\n\nHow many rows (genes) are retained now\n\n\ndim(y) would give you 16494\n\n\nHow many genes were filtered out?\n\n\ndo 37435-16494.\n\n\nAs we have removed the lowly expressed genes the total number of counts\nper sample has not changed greatly. Let us check the total number of\nreads per sample in the original data (data) and now after filtering.\n\n\nBefore:\n\n\n1\n2\n3\ncolSums(data[,1:7])\nAfter filtering:\ncolSums(y$counts)\n\n\n\n\n\n\nWe will now perform normalization to take account of different library\nsize:\n\n\n1\ny\n-\ncalcNormFactors\n(\ny\n)\n\n\n\n\n\n\n\nWe will check the calculated normalization factors:\n\n\n1\ny$samples\n\n\n\n\n\n\nLets have a look at whether the samples cluster by condition. (You\nshould produce a plot as shown in Figure 4):\n\n\n1\nplotMDS(y, col=as.numeric(y$samples$group))\n\n\n\n\n\n\n[H] \n [fig:MDS plot]\n\n\nDoes the MDS plot indicate a difference in gene expression between the\nControls and the DHT treated samples?\n\n\nThe MDS plot shows us that the controls are separated from the DHT\ntreated cells. This indicates that there is a difference in gene\nexpression between the conditions.\n\n\nWe will now estimate the dispersion. We start by estimating the common\ndispersion. The common dispersion estimates the overall Biological\nCoefficient of Variation (BCV) of the dataset averaged over all genes.\n\n\nBy using verbose we get the Disp and BCV values printed on the screen\n\n\n1\ny \n-\n estimateCommonDisp\n(\ny\n,\n verbose\n=\nT\n)\n\n\n\n\n\n\n\nWhat value to you see for BCV?\n\n\nWe now estimate gene-specific dispersion.\n\n\n1\ny \n-\n estimateTagwiseDisp\n(\ny\n)\n\n\n\n\n\n\n\nWe will plot the tagwise dispersion and the common dispersion (You\nshould obtain a plot as shown in the Figure 5):\n\n\n1\nplotBCV(y)\n\n\n\n\n\n\n[H] \n [fig:BCV plot]\n\n\nWe see here that the common dispersion estimates the overall Biological\nCoefficient of Variation (BCV) of the dataset averaged over all genes.\nThe common dispersion is \n0.02\n and the BCV is the square root of the\ncommon dispersion (sqrt[0.02] = 0.14). A BCV of 14% is typical for cell\nline experiment.\n\n\nAs you can see from the plot the BCV of some genes (generally those with\nlow expression) can be much higher than the common dispersion. For\nexample we see genes with a reasonable level of expression with tagwise\ndispersion of 0.4 indicating 40% variation between samples.\n\n\nIf we used the common dispersion for these genes instead of the tagwise\ndispersion what effect would this have?\n\n\nIf we simply used the common dispersion for these genes we would\nunderestimate biological variability, which in turn affects whether\nthese genes would be identified as being differentially expressed\nbetween conditions.It is recommended to use the tagwise dispersion,\nwhich takes account of gene-to-gene variability.\n\n\nNow that we have normalized our data and also calculated the variability\nof gene expression between samples we are in a position to perform\ndifferential expression testing.As this is a simple comparison between\ntwo conditions, androgen treatment and placebo treatment we can use the\nexact test for the negative binomial distribution (Robinson and Smyth,\n2008).\n\n\nTesting for Differential Expression\n\n\nWe now test for differentially expressed BCV genes:\n\n\n1\net \n-\n exactTest\n(\ny\n)\n\n\n\n\n\n\n\nNow we will use the topTags function to adjust for multiple testing. We\nwill use the Benjimini Hochberg (\nBH\n) method and we will produce a\ntable of results:\n\n\n1\nres \n-\n topTags\n(\net\n,\n n\n=\nnrow\n(\ny\n$\ncounts\n),\n adjust.method\n=\nBH\n)\n$\ntable\n\n\n\n\n\n\n\nLet\u2019s have a look at the first rows of the table:\n\n\n1\nhead(res)\n\n\n\n\n\n\nTo get a summary of the number of differentially expressed genes we can\nuse the decideTestsDGE function.\n\n\n1\nsummary\n(\nde \n-\n decideTestsDGE\n(\net\n))\n\n\n\n\n\n\n\nThis tells us that 2096 genes are downregulated and 2339 genes are\nupregulated at 5% FDR.We will now make subsets of the most significant\nupregulated and downregulated genes.\n\n\n1\n2\n3\n4\nalpha\n=\n0.05\n\nlfc\n=\n1.5\n\nedgeR_res_sig\n-\nres\n[\nres\n$\nFDR\nalpha\n,]\n\nedgeR_res_sig_lfc \n-\nedgeR_res_sig\n[\nabs\n(\nedgeR_res_sig\n$\nlogFC\n)\n \n=\n lfc\n,]\nhead\n(\nedgeR_res_sig\n,\n n\n=\n20\n)\nnrow\n(\nedgeR_res_sig\n)\nnrow\n(\nedgeR_res_sig_lfc\n)\n\n\n\n\n\n\n\nWe can write out these results to our current directory.\n\n\n1\n2\nwrite.table(edgeR_res_sig , \nedgeR_res_sig.txt\n, sep=\n\\t\n, col.names=NA, quote=F)\nwrite.table(edgeR_res_sig_lfc , \nedgeR_res_sig_lfc.txt\n, sep=\n\\t\n, col.names=NA, quote=F)\n\n\n\n\n\n\nHow many differentially expressed genes are there?\n\n\n4435\n\n\nHow many upregulated genes and downregulated genes do we have?\n\n\n2339 2096\n\n\nDifferential expression using the Voom function and the limma package\n\n\nWe will now show an alternative approach to differential expression\nwhich uses the \nlimma\n package.This is based on linear models. The first\nstep is to create a design matrix. In this case we have a simple design\nwhere we have only one condition (treated vs non-treated). However, you\nmay be dealing with more complex experimental designs, for example\nlooking at treatment and other covariates, such as age, gender, batch.\n\n\n1\n2\n3\ndesign \n-\nmodel.matrix\n(\n~\ntreatment\n)\n\ncheck design\n\nprint\n(\ndesign\n)\n\n\n\n\n\n\n\nWe now use voom to transform the data into a form which is appropriate\nfor linear modelling.\n\n\n1\nv \n-\nvoom\n(\ny\n,\n design\n)\n\n\n\n\n\n\n\nNext we will fit linear model to each gene in the dataset using the\nfunction lmFit. Following this we use the function eBayes to test each\ngene to find whether foldchange between the conditions being tested is\nstatistically significant.We filter our results by using the same values\nof alpha (0.05) and log fold change (1.5) used previously.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\nfit_v \n-\nlmFit\n(\nv\n,\n design\n)\n\n\nfit_v \n-\n eBayes\n(\nfit_v\n)\n\n\nvoom_res\n-\ntopTable\n(\nfit_v\n,\n coef\n=\n2\n,\nadjust.method\n=\nBH\n,\n sort.by\n=\nP\n,\n n\n=\nnrow\n(\ny\n$\ncounts\n))\n\n\nvoom_res_sig \n-\nvoom_res\n[\nvoom_res\n$\nadj.P.Val \nalpha\n,]\n\n\nvoom_res_sig_lfc \n-\nvoom_res_sig\n[\nabs\n(\nvoom_res_sig\n$\nlogFC\n)\n \n=\n lfc\n,]\n\n\n\n\n\n\n\nHow many differentially expressed genes are identified?\n\n\n1\nnrow(voom_res_sig)nrow(voom_res_sig_lfc)\n\n\n\n\n\n\nWe will write out these results.\n\n\n1\n2\n3\nwrite.table(voom_res_sig, \nvoom_res_sig.txt\n, sep=\n\\t\n, col.names=NA, quote=F)\nwrite.table(voom_res_sig_lfc, \nvoom_res_sig_lfc.txt\n, sep=\n\\t\n, col.names=NA, quote=F)\nwrite.table(voom_res, \nvoom_res.txt\n, sep=\n\\t\n, col.names=NA, quote=F)\n\n\n\n\n\n\nData Visualisation\n\n\nNow let\u2019s visualize some of this data. First we will make a volcano plot\nusing the \nvolcanoplot\n function available in \nlimma\n. This creates a\nplot which displays fold changes versus a measure of statistical\nsignificance of the change.\n\n\n1\nvolcanoplot(fit_v, coef=2, highlight=5)\n\n\n\n\n\n\n[H] \n [fig:Volcano plot]\n\n\nNext we will create a heatmap of the top differentially expressed genes.\nWe use the heatmap.2 function available in the gplots package.\n\n\n1\n2\n3\nselect_top  \n-\n p.adjust\n(\nfit_v\n$\np.value\n[,\n \n2\n])\n \n1e-2\n\nExp_top \n-\n v\n$\nE \n[\nselect_top\n,\n \n]\n\nheatmap.2\n(\nExp_top\n,\n scale\n=\nrow\n,\n density.info\n=\nnone\n,\n trace\n=\nnone\n,\n main\n=\nTop DEGs\n,\n labRow\n=\n,\n cexRow\n=\n0.4\n,\n cexCol\n=\n0.8\n)\n\n\n\n\n\n\n\n[H] \n [fig:Heatmap]\n\n\nYou can now quit the R prompt\n\n\n1\nq()\n\n\n\n\n\n\nYou can save your workspace by typing \nY\n on prompt.\n\n\nPlease note that the output files you are creating are saved in your\npresent working directory. If you are not sure where you are in the file\nsystem try typing \npwd\n on your command prompt to find out.\n\n\nDifferential Expression using cuffdiff\n\n\nThis is optional exercise and will be run if time permits.\n\n\nOne of the stand-alone tools that perform differential expression\nanalysis is Cuffdiff. We use this tool to compare between two\nconditions; for example different conditions could be control and\ndisease, or wild-type and mutant, or various developmental stages.\n\n\nIn our case we want to identify genes that are differentially expressed\nbetween two developmental stages; a \n2cells\n embryo and \n6h\n post\nfertilization.\n\n\nThe general format of the cuffdiff command is:\n\n\n1\ncuffdiff [options]* \ntranscripts.gtf\n \nsample1_replicate1.sam[,...,sample1_replicateM]\n \nsample2_replicate1.sam[,...,sample2_replicateM.sam]\n\n\n\n\n\n\n\nWhere the input includes a \ntranscripts.gtf\n file, which is an\nannotation file of the genome of interest or the cufflinks assembled\ntranscripts, and the aligned reads (either in SAM or BAM format) for the\nconditions. Some of the Cufflinks options that we will use to run the\nprogram are:\n\n\n-o\n:   Output directory.\n\n\n-L\n:   Labels for the different conditions\n\n\n-T\n:   Tells Cuffdiff that the reads are from a time series experiment.\n\n\n-b\n:   Instructs Cufflinks to run a bias detection and correction algorithm\n    which can significantly improve accuracy of transcript abundance\n    estimates. To do this Cufflinks requires a multi-fasta file with the\n    genomic sequences against which we have aligned the reads.\n\n\n-u\n:   Tells Cufflinks to do an initial estimation procedure to more\n    accurately weight reads mapping to multiple locations in the genome\n    (multi-hits).\n\n\n\u2013library-type\n:   Before performing any type of RNA-seq analysis you need to know a\n    few things about the library preparation. Was it done using a\n    strand-specific protocol or not? If yes, which strand? In our data\n    the protocol was NOT strand specific.\n\n\n-C\n:   Biological replicates and multiple group contrast can be defined\n    here\n\n\nRun cuffdiff on the tophat generated BAM files for the 2cells vs. 6h\ndata sets:\n\n\n1\ncuffdiff -o cuffdiff/ -L ZV9_2cells,ZV9_6h -T -b genome/Danio_rerio.Zv9.66.dna.fa -u --library-type fr-unstranded annotation/Danio_rerio.Zv9.66.gtf tophat/ZV9_2cells/accepted_hits.bam tophat/ZV9_6h/accepted_hits.bam\n\n\n\n\n\n\nWe are interested in the differential expression at the gene level. The\nresults are reported by Cuffdiff in the file \ncuffdiff/gene_exp.diff\n.\nLook at the first few lines of the file using the following command:\n\n\n1\nhead -n 20 cuffdiff/gene_exp.diff\n\n\n\n\n\n\nWe would like to see which are the most significantly differentially\nexpressed genes. Therefore we will sort the above file according to the\nq value (corrected p value for multiple testing). The result will be\nstored in a different file called \ngene_exp_qval.sorted.diff\n.\n\n\n1\nsort -t$\n\\t\n -g -k 13 cuffdiff/gene_exp.diff \n cuffdiff/gene_exp_qval.sorted.diff\n\n\n\n\n\n\nLook again at the top 20 lines of the sorted file by typing:\n\n\n1\nhead -n 20 cuffdiff/gene_exp_qval.sorted.diff\n\n\n\n\n\n\nCopy an Ensembl transcript identifier from the first two columns for one\nof these genes (e.g. \nENSDARG00000045067\n). Now go back to the IGV\nbrowser and paste it in the search box.\n\n\nWhat are the various outputs generated by cuffdiff? Hint: Please refer\nto the \nCuffdiff output\n section of the cufflinks manual online.\n\n\nDo you see any difference in the read coverage between the \n2cells\n and\n\n6h\n conditions that might have given rise to this transcript being\ncalled as differentially expressed?\n\n\nThe coverage on the Ensembl browser is based on raw reads and no\nnormalisation has taken place contrary to the FPKM values.\n\n\nThe read coverage of this transcript (\nENSDARG00000045067\n) in the 6h\ndata set is much higher than in the 2cells data set.\n\n\nVisualising the CuffDiff expression analysis\n\n\nWe will use an R-Bioconductor package called \ncummeRbund\n to visualise,\nmanipulate and explore Cufflinks RNA-seq output. We will load an R\nenvironment and look at few quick tips to generate simple graphical\noutput of the cufflinks analysis we have just run.\n\n\nCummeRbund\n takes the cuffdiff output and populates a SQLite database\nwith various type of output generated by cuffdiff e.g, genes,\ntranscripts, transcription start site, isoforms and CDS regions. The\ndata from this database can be accessed and processed easily. This\npackage comes with a number of in-built plotting functions that are\ncommonly used for visualising the expression data. We strongly recommend\nreading through the bioconductor manual and user guide of CummeRbund to\nlearn about functionality of the tool. The reference is provided in the\nresource section.\n\n\nPrepare the environment. Go to the \ncuffdiff\n output folder and copy the\ntranscripts file there.\n\n\n1\n2\n3\ncd /home/trainee/rnaseq/cuffdiff\ncp /home/trainee/rnaseq/annotation/Danio_rerio.Zv9.66.gtf /home/trainee/rnaseq/cuffdiff\nls -l\n\n\n\n\n\n\nLoad the R environment\n\n\n1\nR (press enter)\n\n\n\n\n\n\nLoad the require R package.\n\n\n1\nlibrary(cummeRbund)\n\n\n\n\n\n\nRead in the cuffdiff output\n\n\n1\n2\ncuff\n-\nreadCufflinks\n(\ndir\n=\n/home/trainee/Desktop/rnaseq/cuffdiff\n,\n \\\ngtfFile\n=\nDanio_rerio.Zv9.66.gtf\n,\ngenome\n=\nZv9\n,\n rebuild\n=\nT\n)\n\n\n\n\n\n\n\nAssess the distribution of FPKM scores across samples\n\n\n1\n2\n3\n4\npdf\n(\nfile \n=\n \nSCV.pdf\n,\n height \n=\n \n6\n,\n width \n=\n \n6\n)\n\ndens\n-\ncsDensity\n(\ngenes\n(\ncuff\n))\n\ndens\ndev.off\n()\n\n\n\n\n\n\n\nBox plots of the FPKM values for each samples\n\n\n1\n2\n3\n4\npdf\n(\nfile \n=\n \nBoxP.pdf\n,\n height \n=\n \n6\n,\n width \n=\n \n6\n)\n\nb\n-\ncsBoxplot\n(\ngenes\n(\ncuff\n))\n\nb\ndev.off\n()\n\n\n\n\n\n\n\nAccessing the data\n\n\n1\n2\n3\n4\n5\n6\nsigGeneIds\n-\ngetSig\n(\ncuff\n,\nalpha\n=\n0.05\n,\nlevel\n=\ngenes\n)\n\n\nhead\n(\nsigGeneIds\n)\n\nsigGenes\n-\ngetGenes\n(\ncuff\n,\nsigGeneIds\n)\n\nsigGenes\n\nhead\n(\nfpkm\n(\nsigGenes\n))\n\n\nhead\n(\nfpkm\n(\nisoforms\n(\nsigGenes\n)))\n\n\n\n\n\n\n\nPlotting a heatmap of the differentially expressed genes\n\n\n1\n2\n3\n4\npdf\n(\nfile \n=\n \nheatmap.pdf\n,\n height \n=\n \n6\n,\n width \n=\n \n6\n)\n\nh\n-\ncsHeatmap\n(\nsigGenes\n,\ncluster\n=\nboth\n)\n\nh\ndev.off\n()\n\n\n\n\n\n\n\nWhat options would you use to draw a density or boxplot for different\nreplicates if available ? (Hint: look at the manual at Bioconductor\nwebsite)\n\n\n1\n2\ndensRep\n-\ncsDensity\n(\ngenes\n(\ncuff\n),\nreplicates\n=\nT\n)\n\nbrep\n-\ncsBoxplot\n(\ngenes\n(\ncuff\n),\nreplicates\n=\nT\n)\n\n\n\n\n\n\n\nHow many differentially expressed genes did you observe?\n\n\ntype \u2019summary(sigGenes)\u2019 on the R prompt to see.\n\n\nReferences\n\n\n\n\n\n\nTrapnell, C., Pachter, L. \n Salzberg, S. L. TopHat: discovering\n    splice junctions with RNA-Seq. Bioinformatics 25, 1105-1111 (2009).\n\n\n\n\n\n\nTrapnell, C. et al. Transcript assembly and quantification by\n    RNA-Seq reveals unannotated transcripts and isoform switching during\n    cell differentiation. Nat. Biotechnol. 28, 511-515 (2010).\n\n\n\n\n\n\nLangmead, B., Trapnell, C., Pop, M. \n Salzberg, S. L. Ultrafast and\n    memory-efficient alignment of short DNA sequences to the human\n    genome. Genome Biol. 10, R25 (2009).\n\n\n\n\n\n\nRoberts, A., Pimentel, H., Trapnell, C. \n Pachter, L. Identification\n    of novel transcripts in annotated genomes using RNA-Seq.\n    Bioinformatics 27, 2325-2329 (2011).\n\n\n\n\n\n\nRoberts, A., Trapnell, C., Donaghey, J., Rinn, J. L. \n Pachter, L.\n    Improving RNA-Seq expression estimates by correcting for fragment\n    bias. Genome Biol. 12, R22 (2011).\n\n\n\n\n\n\nRobinson MD, McCarthy DJ and Smyth GK. edgeR: a Bioconductor package\n    for differential expression analysis of digital gene expression\n    data. Bioinformatics, 26 (2010).\n\n\n\n\n\n\nRobinson MD and Smyth GK Moderated statistical tests for assessing\n    differences in tag abundance. Bioinformatics, 23, pp. -6.\n\n\n\n\n\n\nRobinson MD and Smyth GK (2008). Small-sample estimation of negative\n    binomial dispersion, with applications to SAGE data.\u201d Biostatistics,\n    9.\n\n\n\n\n\n\nMcCarthy, J. D, Chen, Yunshun, Smyth and K. G (2012). Differential\n    expression analysis of multifactor RNA-Seq experiments with respect\n    to biological variation. Nucleic Acids Research, 40(10), pp. -9.", 
            "title": "RNA-Seq"
        }, 
        {
            "location": "/modules/btp-module-rna-seq/rna-seq/#key-learning-outcomes", 
            "text": "After completing this practical the trainee should be able to:    Understand and perform a simple RNA-Seq analysis workflow.    Perform spliced alignments to an indexed reference genome using\n    TopHat.    Visualize spliced transcript alignments in a genome browser such as\n    IGV.    Be able to identify differential gene expression between two\n    experimental conditions.    Be familiar with R environment and be able to run R based RNA-seq\n    packages.    We also have bonus exercises where you can learn to:    Perform transcript assembly using Cufflinks.    Run cuffdiff, a Cufflinks utility for differential expression\n    analysis.    Visualize transcript alignments and annotation in a genome browser\n    such as IGV.", 
            "title": "Key Learning Outcomes"
        }, 
        {
            "location": "/modules/btp-module-rna-seq/rna-seq/#resources-youll-be-using", 
            "text": "", 
            "title": "Resources You\u2019ll be Using"
        }, 
        {
            "location": "/modules/btp-module-rna-seq/rna-seq/#tools-used", 
            "text": "Tophat:  https://ccb.jhu.edu/software/tophat/index.shtml  Cufflinks:  http://cole-trapnell-lab.github.io/cufflinks/  Samtools:  http://www.htslib.org/  BEDTools:  https://github.com/arq5x/bedtools2  UCSC tools:  http://hgdownload.cse.ucsc.edu/admin/exe/  IGV:  http://www.broadinstitute.org/igv/  FeatureCount:  http://subread.sourceforge.net/  edgeR package:  https://bioconductor.org/packages/release/bioc/html/edgeR.html  CummeRbund manual:  http://www.bioconductor.org/packages/release/bioc/vignettes/cummeRbund/inst/doc/cummeRbund-manual.pdf", 
            "title": "Tools Used"
        }, 
        {
            "location": "/modules/btp-module-rna-seq/rna-seq/#sources-of-data", 
            "text": "http://www.ebi.ac.uk/ena/data/view/ERR022484  http://www.ebi.ac.uk/ena/data/view/ERR022485  http://www.pnas.org/content/suppl/2008/12/16/0807121105.DCSupplemental", 
            "title": "Sources of Data"
        }, 
        {
            "location": "/modules/btp-module-rna-seq/rna-seq/#introduction", 
            "text": "The goal of this hands-on session is to perform some basic tasks in the\ndownstream analysis of RNA-seq data.\\  First we will use RNA-seq data from zebrafish. You will align one set of\nreads to the zebrafish using Tophat2. You will then view the aligned\nreads using the IGV viewer. We will also demonstrate how gene counts can\nbe derived from this data. You will go on to assembly a transcriptome\nfrom the read data using cufflinks. We will show you how this type of\ndata may be analysed for differential expression.  The second part of the tutorial will focus on RNA-seq data from a human\nexperiment (cancer cell line versus normal cells). You will use the\nBioconductor packages edgeR and voom (limma) to determine differential\ngene expression. The results from this analysis will then be used in the\nfinal session which introduces you to some of the tools used to gain\nbiological insight into the results of a differential expression\nanalysis", 
            "title": "Introduction"
        }, 
        {
            "location": "/modules/btp-module-rna-seq/rna-seq/#prepare-the-environment", 
            "text": "We will use a dataset derived from sequencing of mRNA from  Danio rerio \nembryos in two different developmental stages. Sequencing was performed\non the Illumina platform and generated 76bp paired-end sequence data\nusing polyA selected RNA. Due to the time constraints of the practical\nwe will only use a subset of the reads.  The data files are contained in the subdirectory called  data  and are\nthe following:  2cells_1.fastq  and  2cells_2.fastq \n:   \\\n    These files are based on RNA-seq data of a 2-cell zebrafish embryo  6h_1.fastq  and  6h_2.fastq \n:   \\\n    These files are based on RNA-seq data of zebrafish embryos 6h post\n    fertilization  Open the Terminal and go to the  rnaseq  working directory:  1 cd /home/trainee/rnaseq/   All commands entered into the terminal for this tutorial should be from\nwithin the  /home/trainee/rnaseq  directory.  Check that the  data  directory contains the above-mentioned files by\ntyping:  1 ls data", 
            "title": "Prepare the Environment"
        }, 
        {
            "location": "/modules/btp-module-rna-seq/rna-seq/#alignment", 
            "text": "There are numerous tools for performing short read alignment and the\nchoice of aligner should be carefully made according to the analysis\ngoals/requirements. Here we will use Tophat2, a widely used ultrafast\naligner that performs spliced alignments.  Tophat2 is based on the Bowtie2 aligner and uses an indexed genome for\nthe alignment to speed up the alignment and keep its memory footprint\nsmall. The the index for the  Danio rerio  genome has been created for\nyou.  The command to create an index is as follows. You DO NOT need to run\nthis command yourself - we have done this for you.  1 bowtie2-build genome/Danio_rerio.Zv9.66.dna.fa genome/ZV9   Tophat2 has a number of parameters in order to perform the alignment. To\nview them all type:  1 tophat2 --help   The general format of the tophat2 command is:  1 tophat2 [options]*  index_base   reads_1   reads_2    Where the last two arguments are the  .fastq  files of the paired end\nreads, and the argument before is the basename of the indexed genome.  The quality values in the FASTQ files used in this hands-on session are\nPhred+33 encoded. We explicitly tell tophat of this fact by using the\ncommand line argument  \u2013solexa-quals .  You can look at the first few reads in the file  data/2cells_1.fastq \nwith:  1 head -n 20 data/2cells_1.fastq   Some other parameters that we are going to use to run Tophat are listed\nbelow:  -g\n:   Maximum number of multihits allowed. Short reads are likely to map\n    to more than one location in the genome even though these reads can\n    have originated from only one of these regions. In RNA-seq we allow\n    for a limited number of multihits, and in this case we ask Tophat to\n    report only reads that map at most onto 2 different loci.  \u2013library-type\n:   Before performing any type of RNA-seq analysis you need to know a\n    few things about the library preparation. Was it done using a\n    strand-specific protocol or not? If yes, which strand? In our data\n    the protocol was NOT strand specific.  -j\n:   Improve spliced alignment by providing Tophat with annotated splice\n    junctions. Pre-existing genome annotation is an advantage when\n    analysing RNA-seq data. This file contains the coordinates of\n    annotated splice junctions from Ensembl. These are stored under the\n    sub-directory  annotation  in a file called  ZV9.spliceSites .  -o\n:   This specifies in which subdirectory Tophat should save the output\n    files. Given that for every run the name of the output files is the\n    same, we specify different directories for each run.  It takes some time (approx. 20 min) to perform tophat spliced\nalignments, even for this subset of reads. Therefore, we have\npre-aligned the  2cells  data for you using the following command:  You DO NOT need to run this command yourself - we have done this for\nyou.  1 tophat2 --solexa-quals -g 2 --library-type fr-unstranded -j annotation/Danio_rerio.Zv9.66.spliceSites -o tophat/ZV9_2cells genome/ZV9 data/2cells_1.fastq data/2cells_2.fastq   Align the  6h  data yourself using the following command:  1\n2 # Takes approx. 20mins\ntophat2 --solexa-quals -g 2 --library-type fr-unstranded -j annotation/Danio_rerio.Zv9.66.spliceSites -o tophat/ZV9_6h genome/ZV9 data/6h_1.fastq data/6h_2.fastq   The  6h  read alignment will take approx. 20 min to complete. Therefore,\nwe\u2019ll take a look at some of the files, generated by tophat, for the\npre-computed  2cells  data.  Tophat generates several files in the specified output directory. The\nmost important files are listed below.  accepted_hits.bam\n:   This file contains the list of read alignments in BAM format.  align_summary.txt\n:   Provides a detailed summary of the read-alignments.  unmapped.bam\n:   This file contains the unmapped reads.  The complete documentation can be found at: https://ccb.jhu.edu/software/tophat/manual.shtml", 
            "title": "Alignment"
        }, 
        {
            "location": "/modules/btp-module-rna-seq/rna-seq/#alignment-visualisation-in-igv", 
            "text": "The Integrative Genomics Viewer (IGV) is able to provide a visualisation\nof read alignments given a reference sequence and a BAM file. We\u2019ll\nvisualise the information contained in the  accepted_hits.bam  and junctions.bed  files for the pre-computed  2cells  data. The former,\ncontains the tophat spliced alignments of the reads to the reference\nwhile the latter stores the coordinates of the splice junctions present\nin the data set.  Open the  rnaseq  directory on your Desktop and double-click the tophat  subdirectory and then the  ZV9_2cells  directory.    Launch IGV by double-clicking the \u201cIGV 2.3.*\u201d icon on the Desktop\n    (ignore any warnings that you may get as it opens).  NOTE: IGV may\n    take several minutes to load for the first time, please be patient.    Choose \u201cZebrafish (Zv9)\u201d from the drop-down box in the top left of\n    the IGV window. Else you can also load the genome fasta file.    Load the  accepted_hits.sorted.bam  file by clicking the \u201cFile\u201d\n    menu, selecting \u201cLoad from File\u201d and navigating to the\n     Desktop/rnaseq/tophat/ZV9_2cells  directory.    Rename the track by right-clicking on its name and choosing \u201cRename\n    Track\u201d. Give it a meaningful name like \u201c2cells BAM\u201d.    Load the  junctions.bed  from the same directory and rename the\n    track \u201c2cells Junctions BED\u201d.    Load the Ensembl annotations file  Danio_rerio.Zv9.66.gtf  stored in\n    the  rnaseq/annotation  directory.    Navigate to a region on chromosome 12 by typing\n     chr12 : 20 , 270 , 921 - 20 , 300 , 943  into the search box at the top of the\n    IGV window.    Keep zooming to view the bam file alignments  Some useful IGV manuals can be found below  http://www.broadinstitute.org/software/igv/interpreting_insert_size  http://www.broadinstitute.org/software/igv/alignmentdata     Does the file \u2019align_summary.txt\u2019 look interesting? What information\ndoes it provide?  As the name suggests, the file provides a details summary of the\nalignment statistics.  One other important file is \u2019unmapped.bam\u2019. This file contains the\nunampped reads.  Can you identify the splice junctions from the BAM file?  Splice junctions can be identified in the alignment BAM files. These are\nthe aligned RNA-Seq reads that have skipped-bases from the reference\ngenome (most likely introns).  Are the junctions annotated for  CBY1  consistent with the annotation?  Read alignment supports an extended length in exon 5 to the gene model\n(cby1-001)  Once tophat finishes aligning the 6h data you will need to sort the\nalignments found in the BAM file and then index the sorted BAM file.  1\n2 samtools sort tophat/ZV9_6h/accepted_hits.bam tophat/ZV9_6h/accepted_hits.sorted\nsamtools index tophat/ZV9_6h/accepted_hits.sorted.bam   Load the sorted BAM file into IGV, as described previously, and rename\nthe track appropriately.", 
            "title": "Alignment Visualisation in IGV"
        }, 
        {
            "location": "/modules/btp-module-rna-seq/rna-seq/#generating-gene-counts", 
            "text": "In RNAseq experiments the digital gene expression is recorded as the\ngene counts or number of reads aligning to a known gene feature. If you\nhave a well annotated genome, you can use the gene structure file in a\nstandard gene annotation format (GTF or GFF)) along with the spliced\nalignment file to quantify the known genes. We will demonstrate a\nutility called  FeatureCounts  that comes with the  Subread  package.  1\n2 mkdir gene_counts\nfeatureCounts -a annotation/Danio_rerio.Zv9.66.gtf -t exon -g gene_id -o gene_counts/gene_counts.txt tophat/ZV9_6h/accepted_hits.sorted.bam tophat/ZV9_2cells/accepted_hits.sorted.bam", 
            "title": "Generating Gene Counts"
        }, 
        {
            "location": "/modules/btp-module-rna-seq/rna-seq/#isoform-expression-and-transcriptome-assembly", 
            "text": "For non-model organisms and genomes with draft assemblies and incomplete\nannotations, it is a common practice to take and assembly based approach\nto generate gene structures followed by the quantification step. There\nare a number of reference based transcript assemblers available that can\nbe used for this purpose such as, cufflinks, stringy. These assemblers\ncan give gene or isoform level assemblies that can be used to perform a\ngene/isoform level quantification. These assemblers require an alignment\nof reads with a reference genome or transcriptome as an input. The\nsecond optional input is a known gene structure in  GTF  or  GFF \nformat.  There are a number of tools that perform reconstruction of the\ntranscriptome and for this workshop we are going to use Cufflinks.\nCufflinks can do transcriptome assembly either  ab initio  or using a\nreference annotation. It also quantifies the isoform expression in\nFragments Per Kilobase of exon per Million fragments mapped (FPKM).  Cufflinks has a number of parameters in order to perform transcriptome\nassembly and quantification. To view them all type:  1 cufflinks --help   We aim to reconstruct the transcriptome for both samples by using the\nEnsembl annotation both strictly and as a guide. In the first case\nCufflinks will only report isoforms that are included in the annotation,\nwhile in the latter case it will report novel isoforms as well.  The Ensembl annotation for  Danio rerio  is available in annotation/Danio_rerio.Zv9.66.gtf .  The general format of the  cufflinks  command is:  1 cufflinks [options]*  aligned_reads.(sam|bam)    Where the input is the aligned reads (either in SAM or BAM format).  Some of the available parameters for Cufflinks that we are going to use\nto run Cufflinks are listed below:  -o\n:   Output directory.  -G\n:   Tells Cufflinks to use the supplied GTF annotations strictly in\n    order to estimate isoform annotation.  -b\n:   Instructs Cufflinks to run a bias detection and correction algorithm\n    which can significantly improve accuracy of transcript abundance\n    estimates. To do this Cufflinks requires a multi-fasta file with the\n    genomic sequences against which we have aligned the reads.  -u\n:   Tells Cufflinks to do an initial estimation procedure to more\n    accurately weight reads mapping to multiple locations in the genome\n    (multi-hits).  \u2013library-type\n:   Before performing any type of RNA-seq analysis you need to know a\n    few things about the library preparation. Was it done using a\n    strand-specific protocol or not? If yes, which strand? In our data\n    the protocol was NOT strand specific.  Perform transcriptome assembly, strictly using the supplied GTF\nannotations, for the  2cells  and  6h  data using cufflinks:  1\n2\n3\n4 # 2cells data (takes approx. 5mins):\ncufflinks -o cufflinks/ZV9_2cells_gtf -G annotation/Danio_rerio.Zv9.66.gtf -b genome/Danio_rerio.Zv9.66.dna.fa -u --library-type fr-unstranded tophat/ZV9_2cells/accepted_hits.bam\n# 6h data (takes approx. 5mins):\ncufflinks -o cufflinks/ZV9_6h_gtf -G annotation/Danio_rerio.Zv9.66.gtf -b genome/Danio_rerio.Zv9.66.dna.fa -u --library-type fr-unstranded tophat/ZV9_6h/accepted_hits.bam   Cufflinks generates several files in the specified output directory.\nHere\u2019s a short description of these files:  genes.fpkm_tracking\n:   Contains the estimated gene-level expression values.  isoforms.fpkm_tracking\n:   Contains the estimated isoform-level expression values.  skipped.gtf\n:   Contains loci skipped as a result of exceeding the maximum number of\n    fragments.  transcripts.gtf\n:   This GTF file contains Cufflinks\u2019 assembled isoforms.  The complete documentation can be found at:  http://cole-trapnell-lab.github.io/cufflinks/file_formats/#output-formats-used-in-the-cufflinks-suite  So far we have forced cufflinks, by using the  -G  option, to strictly\nuse the GTF annotations provided and thus novel transcripts will not be\nreported. We can get cufflinks to perform a GTF-guided transcriptome\nassembly by using the  -g  option instead. Thus, novel transcripts will\nbe reported.  GTF-guided transcriptome assembly is more computationally intensive than\nstrictly using the GTF annotations. Therefore, we have pre-computed\nthese GTF-guided assemblies for you and have placed the results under\nsubdirectories:  cufflinks/ZV9_2cells_gtf_guided  and  cufflinks/ZV9_6h_gft_guided .  You DO NOT need to run these commands. We provide them so you know how\nwe generated the the GTF-guided transcriptome assemblies:  1\n2\n3\n4 # 2cells guided transcriptome assembly (takes approx. 30mins):\ncufflinks -o cufflinks/ZV9_2cells_gtf_guided -g annotation/Danio_rerio.Zv9.66.gtf -b genome/Danio_rerio.Zv9.66.dna.fa -u --library-type fr-unstranded tophat/ZV9_2cells/accepted_hits.bam\n# 6h guided transcriptome assembly (takes approx. 30mins):\ncufflinks -o cufflinks/ZV9_6h_gtf_guided -g annotation/Danio_rerio.Zv9.66.gtf -b genome/Danio_rerio.Zv9.66.dna.fa -u --library-type fr-unstranded tophat/ZV9_6h/accepted_hits.bam     Go back to IGV and load the pre-computed, GTF-guided transcriptome\n    assembly for the  2cells  data\n    ( cufflinks/ZV9_2cells_gtf_guided/transcripts.gtf ).    Rename the track as \u201c2cells GTF-Guided Transcripts\u201d.    In the search box type  ENSDART00000082297  in order for the browser\n    to zoom in to the gene of interest.    Do you observe any difference between the Ensembl GTF annotations and\nthe GTF-guided transcripts assembled by cufflinks (the \u201c2cells\nGTF-Guided Transcripts\u201d track)?  Yes. It appears that the Ensembl annotations may have truncated the last\nexon. However, our data also doesn\u2019t contain reads that span between the\nlast two exons.", 
            "title": "Isoform Expression and Transcriptome Assembly"
        }, 
        {
            "location": "/modules/btp-module-rna-seq/rna-seq/#differential-gene-expression-analysis-using-edger", 
            "text": "", 
            "title": "Differential Gene Expression Analysis using edgeR"
        }, 
        {
            "location": "/modules/btp-module-rna-seq/rna-seq/#experiment-design", 
            "text": "The example we are working through today follows a case Study set out in\nthe edgeR Users Guide (4.3 Androgen-treated prostate cancer cells\n(RNA-Seq, two groups) which is based on an experiment conducted by Li et\nal. (2008, Proc Natl Acad Sci USA, 105, 20179-84).  The researches used a prostate cancer cell line (LNCaP cells). These\ncells are sensitive to stimulation by male hormones (androgens). Three\nreplicate RNA samples were collected from LNCaP cells treated with an\nandrogen hormone (DHT). Four replicates were collected from cells\ntreated with an inactive compound. Each of the seven samples was run on\na lane (7 lanes) of an Illumina flow cell to produce 35 bp reads. The\nexperimental design was therefore:  [H]  rrr  Lane     Treatment     Label \\\n1   Control   Con1\\\n2   Control   Con2\\\n3   Control   Con3\\\n4   Control   Con4\\\n5   DHT   DHT1\\\n6   DHT   DHT2\\\n7   DHT   DHT3\\  [tab:experimental d esign]  This workflow requires raw gene count files and these can be generated\nusing a utility called featureCounts as demonstrated above. We are using\na pre-computed gene counts data (stored in  pnas_expression.txt ) for\nthis exercise.", 
            "title": "Experiment Design"
        }, 
        {
            "location": "/modules/btp-module-rna-seq/rna-seq/#prepare-the-environment_1", 
            "text": "Prepare the environment and load R:  1\n2 cd /home/trainee/rnaseq/edgeR\nR (press enter)   Once on the R prompt. Load libraries:  1\n2\n3\n4\n5\n6 library(edgeR)\nlibrary(biomaRt)\nlibrary(gplots)\nlibrary( limma )\nlibrary( RColorBrewer )\nlibrary( org.Hs.eg.db )", 
            "title": "Prepare the Environment"
        }, 
        {
            "location": "/modules/btp-module-rna-seq/rna-seq/#read-in-data", 
            "text": "Read in count table and experimental design:  1\n2\n3\n4 data  -  read.delim ( pnas_expression.txt ,  row.names = 1 ,  header = T ) \ntargets  -  read.delim ( Targets.txt ,  header = T )  colnames ( data )   - targets $ Label head ( data ,  n = 20 )", 
            "title": "Read in Data"
        }, 
        {
            "location": "/modules/btp-module-rna-seq/rna-seq/#add-gene-annotation", 
            "text": "The data set only includes the Ensembl gene id and the counts. It is\nuseful to have other annotations such as the gene symbol and entrez id.\nNext we will add in these annotations. We will use the BiomaRt package\nto do this.  We start by using the useMart function of BiomaRt to access the human\ndata base of ensemble gene ids.  1 human - useMart ( host = www.ensembl.org ,   ENSEMBL_MART_ENSEMBL ,  dataset = hsapiens_gene_ensembl )  attributes = c ( ensembl_gene_id ,   entrezgene , hgnc_symbol )    We create a vector of our ensemble gene ids.  1\n2 ensembl_names - rownames ( data )  head ( ensembl_names )    We then use the function getBM to get the gene symbol data we want.This\ntakes about a minute.  1 genemap - getBM ( attributes ,  filters = ensembl_gene_id ,  values = ensembl_names ,  mart = human )    Have a look at the start of the genemap dataframe.  1 head(genemap)   We then match the data we have retrieved to our dataset.  1\n2\n3\n4\n5\n6 idx  - match ( ensembl_names ,  genemap $ ensembl_gene_id ) \ndata $ entrezgene  - genemap $ entrezgene  [  idx  ] \ndata $ hgnc_symbol  - genemap $ hgnc_symbol  [  idx  ] \nAnn  -   cbind ( rownames ( data ),  data $ hgnc_symbol ,  data $ entrezgene )  colnames ( Ann ) - c ( Ensembl ,   Symbol ,   Entrez ) \nAnn - as.data.frame ( Ann )    Let\u2019s check and see that this additional information is there.  1 head(data)", 
            "title": "Add Gene annotation"
        }, 
        {
            "location": "/modules/btp-module-rna-seq/rna-seq/#data-checks", 
            "text": "Create DGEList object:  1\n2 treatment  - factor ( c ( rep ( Control , 4 ),   rep ( DHT , 3 )),  levels = c ( Control ,   DHT )) \ny  - DGEList ( counts = data [, 1 : 7 ],  group = treatment ,  genes = Ann )    Check the dimensions of the object:  1 dim(y)   We see we have 37435 rows (i.e. genes) and 7 columns (samples).  Now we will filter out genes with low counts by only keeping those rows\nwhere the count per million (cpm) is at least 1 in at least three\nsamples:  1\n2 keep  - rowSums (  cpm ( y ) 1 )   = 3 \ny  -  y [ keep ,   ]    How many rows (genes) are retained now  dim(y) would give you 16494  How many genes were filtered out?  do 37435-16494.  As we have removed the lowly expressed genes the total number of counts\nper sample has not changed greatly. Let us check the total number of\nreads per sample in the original data (data) and now after filtering.  Before:  1\n2\n3 colSums(data[,1:7])\nAfter filtering:\ncolSums(y$counts)   We will now perform normalization to take account of different library\nsize:  1 y - calcNormFactors ( y )    We will check the calculated normalization factors:  1 y$samples   Lets have a look at whether the samples cluster by condition. (You\nshould produce a plot as shown in Figure 4):  1 plotMDS(y, col=as.numeric(y$samples$group))   [H]   [fig:MDS plot]  Does the MDS plot indicate a difference in gene expression between the\nControls and the DHT treated samples?  The MDS plot shows us that the controls are separated from the DHT\ntreated cells. This indicates that there is a difference in gene\nexpression between the conditions.  We will now estimate the dispersion. We start by estimating the common\ndispersion. The common dispersion estimates the overall Biological\nCoefficient of Variation (BCV) of the dataset averaged over all genes.  By using verbose we get the Disp and BCV values printed on the screen  1 y  -  estimateCommonDisp ( y ,  verbose = T )    What value to you see for BCV?  We now estimate gene-specific dispersion.  1 y  -  estimateTagwiseDisp ( y )    We will plot the tagwise dispersion and the common dispersion (You\nshould obtain a plot as shown in the Figure 5):  1 plotBCV(y)   [H]   [fig:BCV plot]  We see here that the common dispersion estimates the overall Biological\nCoefficient of Variation (BCV) of the dataset averaged over all genes.\nThe common dispersion is  0.02  and the BCV is the square root of the\ncommon dispersion (sqrt[0.02] = 0.14). A BCV of 14% is typical for cell\nline experiment.  As you can see from the plot the BCV of some genes (generally those with\nlow expression) can be much higher than the common dispersion. For\nexample we see genes with a reasonable level of expression with tagwise\ndispersion of 0.4 indicating 40% variation between samples.  If we used the common dispersion for these genes instead of the tagwise\ndispersion what effect would this have?  If we simply used the common dispersion for these genes we would\nunderestimate biological variability, which in turn affects whether\nthese genes would be identified as being differentially expressed\nbetween conditions.It is recommended to use the tagwise dispersion,\nwhich takes account of gene-to-gene variability.  Now that we have normalized our data and also calculated the variability\nof gene expression between samples we are in a position to perform\ndifferential expression testing.As this is a simple comparison between\ntwo conditions, androgen treatment and placebo treatment we can use the\nexact test for the negative binomial distribution (Robinson and Smyth,\n2008).", 
            "title": "Data checks"
        }, 
        {
            "location": "/modules/btp-module-rna-seq/rna-seq/#testing-for-differential-expression", 
            "text": "We now test for differentially expressed BCV genes:  1 et  -  exactTest ( y )    Now we will use the topTags function to adjust for multiple testing. We\nwill use the Benjimini Hochberg ( BH ) method and we will produce a\ntable of results:  1 res  -  topTags ( et ,  n = nrow ( y $ counts ),  adjust.method = BH ) $ table    Let\u2019s have a look at the first rows of the table:  1 head(res)   To get a summary of the number of differentially expressed genes we can\nuse the decideTestsDGE function.  1 summary ( de  -  decideTestsDGE ( et ))    This tells us that 2096 genes are downregulated and 2339 genes are\nupregulated at 5% FDR.We will now make subsets of the most significant\nupregulated and downregulated genes.  1\n2\n3\n4 alpha = 0.05 \nlfc = 1.5 \nedgeR_res_sig - res [ res $ FDR alpha ,] \nedgeR_res_sig_lfc  - edgeR_res_sig [ abs ( edgeR_res_sig $ logFC )   =  lfc ,] head ( edgeR_res_sig ,  n = 20 ) nrow ( edgeR_res_sig ) nrow ( edgeR_res_sig_lfc )    We can write out these results to our current directory.  1\n2 write.table(edgeR_res_sig ,  edgeR_res_sig.txt , sep= \\t , col.names=NA, quote=F)\nwrite.table(edgeR_res_sig_lfc ,  edgeR_res_sig_lfc.txt , sep= \\t , col.names=NA, quote=F)   How many differentially expressed genes are there?  4435  How many upregulated genes and downregulated genes do we have?  2339 2096", 
            "title": "Testing for Differential Expression"
        }, 
        {
            "location": "/modules/btp-module-rna-seq/rna-seq/#differential-expression-using-the-voom-function-and-the-limma-package", 
            "text": "We will now show an alternative approach to differential expression\nwhich uses the  limma  package.This is based on linear models. The first\nstep is to create a design matrix. In this case we have a simple design\nwhere we have only one condition (treated vs non-treated). However, you\nmay be dealing with more complex experimental designs, for example\nlooking at treatment and other covariates, such as age, gender, batch.  1\n2\n3 design  - model.matrix ( ~ treatment ) \ncheck design print ( design )    We now use voom to transform the data into a form which is appropriate\nfor linear modelling.  1 v  - voom ( y ,  design )    Next we will fit linear model to each gene in the dataset using the\nfunction lmFit. Following this we use the function eBayes to test each\ngene to find whether foldchange between the conditions being tested is\nstatistically significant.We filter our results by using the same values\nof alpha (0.05) and log fold change (1.5) used previously.  1\n2\n3\n4\n5\n6\n7\n8\n9 fit_v  - lmFit ( v ,  design ) \n\nfit_v  -  eBayes ( fit_v ) \n\nvoom_res - topTable ( fit_v ,  coef = 2 , adjust.method = BH ,  sort.by = P ,  n = nrow ( y $ counts )) \n\nvoom_res_sig  - voom_res [ voom_res $ adj.P.Val  alpha ,] \n\nvoom_res_sig_lfc  - voom_res_sig [ abs ( voom_res_sig $ logFC )   =  lfc ,]    How many differentially expressed genes are identified?  1 nrow(voom_res_sig)nrow(voom_res_sig_lfc)   We will write out these results.  1\n2\n3 write.table(voom_res_sig,  voom_res_sig.txt , sep= \\t , col.names=NA, quote=F)\nwrite.table(voom_res_sig_lfc,  voom_res_sig_lfc.txt , sep= \\t , col.names=NA, quote=F)\nwrite.table(voom_res,  voom_res.txt , sep= \\t , col.names=NA, quote=F)", 
            "title": "Differential expression using the Voom function and the limma package"
        }, 
        {
            "location": "/modules/btp-module-rna-seq/rna-seq/#data-visualisation", 
            "text": "Now let\u2019s visualize some of this data. First we will make a volcano plot\nusing the  volcanoplot  function available in  limma . This creates a\nplot which displays fold changes versus a measure of statistical\nsignificance of the change.  1 volcanoplot(fit_v, coef=2, highlight=5)   [H]   [fig:Volcano plot]  Next we will create a heatmap of the top differentially expressed genes.\nWe use the heatmap.2 function available in the gplots package.  1\n2\n3 select_top   -  p.adjust ( fit_v $ p.value [,   2 ])   1e-2 \nExp_top  -  v $ E  [ select_top ,   ] \nheatmap.2 ( Exp_top ,  scale = row ,  density.info = none ,  trace = none ,  main = Top DEGs ,  labRow = ,  cexRow = 0.4 ,  cexCol = 0.8 )    [H]   [fig:Heatmap]  You can now quit the R prompt  1 q()   You can save your workspace by typing  Y  on prompt.  Please note that the output files you are creating are saved in your\npresent working directory. If you are not sure where you are in the file\nsystem try typing  pwd  on your command prompt to find out.", 
            "title": "Data Visualisation"
        }, 
        {
            "location": "/modules/btp-module-rna-seq/rna-seq/#differential-expression-using-cuffdiff", 
            "text": "This is optional exercise and will be run if time permits.  One of the stand-alone tools that perform differential expression\nanalysis is Cuffdiff. We use this tool to compare between two\nconditions; for example different conditions could be control and\ndisease, or wild-type and mutant, or various developmental stages.  In our case we want to identify genes that are differentially expressed\nbetween two developmental stages; a  2cells  embryo and  6h  post\nfertilization.  The general format of the cuffdiff command is:  1 cuffdiff [options]*  transcripts.gtf   sample1_replicate1.sam[,...,sample1_replicateM]   sample2_replicate1.sam[,...,sample2_replicateM.sam]    Where the input includes a  transcripts.gtf  file, which is an\nannotation file of the genome of interest or the cufflinks assembled\ntranscripts, and the aligned reads (either in SAM or BAM format) for the\nconditions. Some of the Cufflinks options that we will use to run the\nprogram are:  -o\n:   Output directory.  -L\n:   Labels for the different conditions  -T\n:   Tells Cuffdiff that the reads are from a time series experiment.  -b\n:   Instructs Cufflinks to run a bias detection and correction algorithm\n    which can significantly improve accuracy of transcript abundance\n    estimates. To do this Cufflinks requires a multi-fasta file with the\n    genomic sequences against which we have aligned the reads.  -u\n:   Tells Cufflinks to do an initial estimation procedure to more\n    accurately weight reads mapping to multiple locations in the genome\n    (multi-hits).  \u2013library-type\n:   Before performing any type of RNA-seq analysis you need to know a\n    few things about the library preparation. Was it done using a\n    strand-specific protocol or not? If yes, which strand? In our data\n    the protocol was NOT strand specific.  -C\n:   Biological replicates and multiple group contrast can be defined\n    here  Run cuffdiff on the tophat generated BAM files for the 2cells vs. 6h\ndata sets:  1 cuffdiff -o cuffdiff/ -L ZV9_2cells,ZV9_6h -T -b genome/Danio_rerio.Zv9.66.dna.fa -u --library-type fr-unstranded annotation/Danio_rerio.Zv9.66.gtf tophat/ZV9_2cells/accepted_hits.bam tophat/ZV9_6h/accepted_hits.bam   We are interested in the differential expression at the gene level. The\nresults are reported by Cuffdiff in the file  cuffdiff/gene_exp.diff .\nLook at the first few lines of the file using the following command:  1 head -n 20 cuffdiff/gene_exp.diff   We would like to see which are the most significantly differentially\nexpressed genes. Therefore we will sort the above file according to the\nq value (corrected p value for multiple testing). The result will be\nstored in a different file called  gene_exp_qval.sorted.diff .  1 sort -t$ \\t  -g -k 13 cuffdiff/gene_exp.diff   cuffdiff/gene_exp_qval.sorted.diff   Look again at the top 20 lines of the sorted file by typing:  1 head -n 20 cuffdiff/gene_exp_qval.sorted.diff   Copy an Ensembl transcript identifier from the first two columns for one\nof these genes (e.g.  ENSDARG00000045067 ). Now go back to the IGV\nbrowser and paste it in the search box.  What are the various outputs generated by cuffdiff? Hint: Please refer\nto the  Cuffdiff output  section of the cufflinks manual online.  Do you see any difference in the read coverage between the  2cells  and 6h  conditions that might have given rise to this transcript being\ncalled as differentially expressed?  The coverage on the Ensembl browser is based on raw reads and no\nnormalisation has taken place contrary to the FPKM values.  The read coverage of this transcript ( ENSDARG00000045067 ) in the 6h\ndata set is much higher than in the 2cells data set.", 
            "title": "Differential Expression using cuffdiff"
        }, 
        {
            "location": "/modules/btp-module-rna-seq/rna-seq/#visualising-the-cuffdiff-expression-analysis", 
            "text": "We will use an R-Bioconductor package called  cummeRbund  to visualise,\nmanipulate and explore Cufflinks RNA-seq output. We will load an R\nenvironment and look at few quick tips to generate simple graphical\noutput of the cufflinks analysis we have just run.  CummeRbund  takes the cuffdiff output and populates a SQLite database\nwith various type of output generated by cuffdiff e.g, genes,\ntranscripts, transcription start site, isoforms and CDS regions. The\ndata from this database can be accessed and processed easily. This\npackage comes with a number of in-built plotting functions that are\ncommonly used for visualising the expression data. We strongly recommend\nreading through the bioconductor manual and user guide of CummeRbund to\nlearn about functionality of the tool. The reference is provided in the\nresource section.  Prepare the environment. Go to the  cuffdiff  output folder and copy the\ntranscripts file there.  1\n2\n3 cd /home/trainee/rnaseq/cuffdiff\ncp /home/trainee/rnaseq/annotation/Danio_rerio.Zv9.66.gtf /home/trainee/rnaseq/cuffdiff\nls -l   Load the R environment  1 R (press enter)   Load the require R package.  1 library(cummeRbund)   Read in the cuffdiff output  1\n2 cuff - readCufflinks ( dir = /home/trainee/Desktop/rnaseq/cuffdiff ,  \\\ngtfFile = Danio_rerio.Zv9.66.gtf , genome = Zv9 ,  rebuild = T )    Assess the distribution of FPKM scores across samples  1\n2\n3\n4 pdf ( file  =   SCV.pdf ,  height  =   6 ,  width  =   6 ) \ndens - csDensity ( genes ( cuff )) \ndens\ndev.off ()    Box plots of the FPKM values for each samples  1\n2\n3\n4 pdf ( file  =   BoxP.pdf ,  height  =   6 ,  width  =   6 ) \nb - csBoxplot ( genes ( cuff )) \nb\ndev.off ()    Accessing the data  1\n2\n3\n4\n5\n6 sigGeneIds - getSig ( cuff , alpha = 0.05 , level = genes )  head ( sigGeneIds ) \nsigGenes - getGenes ( cuff , sigGeneIds ) \nsigGenes head ( fpkm ( sigGenes ))  head ( fpkm ( isoforms ( sigGenes )))    Plotting a heatmap of the differentially expressed genes  1\n2\n3\n4 pdf ( file  =   heatmap.pdf ,  height  =   6 ,  width  =   6 ) \nh - csHeatmap ( sigGenes , cluster = both ) \nh\ndev.off ()    What options would you use to draw a density or boxplot for different\nreplicates if available ? (Hint: look at the manual at Bioconductor\nwebsite)  1\n2 densRep - csDensity ( genes ( cuff ), replicates = T ) \nbrep - csBoxplot ( genes ( cuff ), replicates = T )    How many differentially expressed genes did you observe?  type \u2019summary(sigGenes)\u2019 on the R prompt to see.", 
            "title": "Visualising the CuffDiff expression analysis"
        }, 
        {
            "location": "/modules/btp-module-rna-seq/rna-seq/#references", 
            "text": "Trapnell, C., Pachter, L.   Salzberg, S. L. TopHat: discovering\n    splice junctions with RNA-Seq. Bioinformatics 25, 1105-1111 (2009).    Trapnell, C. et al. Transcript assembly and quantification by\n    RNA-Seq reveals unannotated transcripts and isoform switching during\n    cell differentiation. Nat. Biotechnol. 28, 511-515 (2010).    Langmead, B., Trapnell, C., Pop, M.   Salzberg, S. L. Ultrafast and\n    memory-efficient alignment of short DNA sequences to the human\n    genome. Genome Biol. 10, R25 (2009).    Roberts, A., Pimentel, H., Trapnell, C.   Pachter, L. Identification\n    of novel transcripts in annotated genomes using RNA-Seq.\n    Bioinformatics 27, 2325-2329 (2011).    Roberts, A., Trapnell, C., Donaghey, J., Rinn, J. L.   Pachter, L.\n    Improving RNA-Seq expression estimates by correcting for fragment\n    bias. Genome Biol. 12, R22 (2011).    Robinson MD, McCarthy DJ and Smyth GK. edgeR: a Bioconductor package\n    for differential expression analysis of digital gene expression\n    data. Bioinformatics, 26 (2010).    Robinson MD and Smyth GK Moderated statistical tests for assessing\n    differences in tag abundance. Bioinformatics, 23, pp. -6.    Robinson MD and Smyth GK (2008). Small-sample estimation of negative\n    binomial dispersion, with applications to SAGE data.\u201d Biostatistics,\n    9.    McCarthy, J. D, Chen, Yunshun, Smyth and K. G (2012). Differential\n    expression analysis of multifactor RNA-Seq experiments with respect\n    to biological variation. Nucleic Acids Research, 40(10), pp. -9.", 
            "title": "References"
        }, 
        {
            "location": "/modules/btp-module-velvet/velvet/", 
            "text": "Key Learning Outcomes\n\n\nAfter completing this practical the trainee should be able to:\n\n\n\n\n\n\nCompile velvet with appropriate compile-time parameters set for a\n    specific analysis\n\n\n\n\n\n\nBe able to choose appropriate assembly parameters\n\n\n\n\n\n\nAssemble a set of single-ended reads\n\n\n\n\n\n\nAssemble a set of paired-end reads from a single insert-size library\n\n\n\n\n\n\nBe able to visualise an assembly in AMOS Hawkeye\n\n\n\n\n\n\nUnderstand the importance of using paired-end libraries in \nde novo\n\n    genome assembly\n\n\n\n\n\n\nResources You\u2019ll be Using\n\n\nAlthough we have provided you with an environment which contains all the\ntools and data you will be using in this module, you may like to know\nwhere we have sourced those tools and data from.\n\n\nTools Used\n\n\nVelvet:\n\n\nhttp://www.ebi.ac.uk/~zerbino/velvet/\n\n\nAMOS Hawkeye:\n\n\nhttp://apps.sourceforge.net/mediawiki/amos/index.php?title=Hawkeye\n\n\ngnx-tools:\n\n\nhttps://github.com/mh11/gnx-tools\n\n\nFastQC:\n\n\nhttp://www.bioinformatics.bbsrc.ac.uk/projects/fastqc/\n\n\nR:\n\n\nhttp://www.r-project.org/\n\n\nSources of Data\n\n\n\n\n\n\nftp://ftp.ensemblgenomes.org/pub/release-8/bacteria/fasta/Staphylococcus/s_aureus_mrsa252/dna/s_aureus_mrsa252.EB1_s_aureus_mrsa252.dna.chromosome.Chromosome.fa.gz\n\n\n\n\n\n\nhttp://www.ebi.ac.uk/ena/data/view/SRS004748\n\n\n\n\n\n\nftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR022/SRR022825/SRR022825.fastq.gz\n\n\n\n\n\n\nftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR022/SRR022823/SRR022823.fastq.gz\n\n\n\n\n\n\nhttp://www.ebi.ac.uk/ena/data/view/SRX008042\n\n\n\n\n\n\nftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR022/SRR022852/SRR022852_1.fastq.gz\n\n\n\n\n\n\nftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR022/SRR022852/SRR022852_2.fastq.gz\n\n\n\n\n\n\nftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR023/SRR023408/SRR023408_1.fastq.gz\n\n\n\n\n\n\nftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR023/SRR023408/SRR023408_2.fastq.gz\n\n\n\n\n\n\nhttp://www.ebi.ac.uk/ena/data/view/SRX000181\n\n\n\n\n\n\nftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR000/SRR000892/SRR000892.fastq.gz\n\n\n\n\n\n\nftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR000/SRR000893/SRR000893.fastq.gz\n\n\n\n\n\n\nhttp://www.ebi.ac.uk/ena/data/view/SRX007709\n\n\n\n\n\n\nftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR022/SRR022863/SRR022863_1.fastq.gz\n\n\n\n\n\n\nftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR022/SRR022863/SRR022863_2.fastq.gz\n\n\n\n\n\n\nIntroduction\n\n\nThe aim of this module is to become familiar with performing \nde novo\n\ngenome assembly using Velvet, a de Bruijn graph based assembler, on a\nvariety of sequence data.\n\n\nPrepare the Environment\n\n\nThe first exercise should get you a little more comfortable with the\ncomputer environment and the command line.\n\n\nFirst make sure that you are in the denovo working directory by typing:\n\n\n1\ncd /home/trainee/denovo\n\n\n\n\n\n\nand making absolutely sure you\u2019re there by typing:\n\n\n1\npwd\n\n\n\n\n\n\nNow create sub-directories for this and the two other velvet practicals.\nAll these directories will be made as sub-directories of a directory for\nthe whole course called NGS. For this you can use the following\ncommands:\n\n\n1\nmkdir -p NGS/velvet/{part1,part2,part3}\n\n\n\n\n\n\nThe \n-p\n tells \nmkdir\n (make directory) to make any parent directories\nif they don\u2019t already exist. You could have created the above\ndirectories one-at-a-time by doing this instead:\n\n\n1\n2\n3\n4\n5\nmkdir NGS\nmkdir NGS/velvet\nmkdir NGS/velvet/part1\nmkdir NGS/velvet/part2\nmkdir NGS/velvet/part3\n\n\n\n\n\n\nAfter creating the directories, examine the structure and move into the\ndirectory ready for the first velvet exercise by typing:\n\n\n1\n2\n3\nls -R NGS\ncd NGS/velvet/part1\npwd\n\n\n\n\n\n\nDownloading and Compiling Velvet\n\n\nFor the duration of this workshop, all the software you require has been\nset up for you already. This might not be the case when you return to\n\u201creal life\u201d. Many of the programs you will need, including velvet, are\nquite easy to set up, it might be instructive to try a couple.\n\n\nAlthough you will be using the preinstalled version of velvet, it is\nuseful to know how to compile velvet as some of the parameters you might\nlike to control can only be set at compile time. You can find the latest\nversion of velvet at:\n\n\nhttp://www.ebi.ac.uk/~zerbino/velvet/\n\n\nYou could go to this URL and download the latest velvet version, or\nequivalently, you could type the following, which will download, unpack,\ninspect, compile and execute your locally compiled version of velvet:\n\n\n1\n2\n3\n4\n5\n6\n7\ncd /home/trainee/denovo/NGS/velvet/part1\npwd\ntar xzf /home/trainee/denovo/data/velvet_1.2.10.tgz\nls -R\ncd velvet_1.2.10\nmake\n./velveth\n\n\n\n\n\n\nThe standout displayed to screen when \u2019make\u2019 runs may contain an error\nmessage but it is ignored\n\n\nTake a look at the executables you have created. They will be displayed\nas green by the command:\n\n\n1\nls --color=always\n\n\n\n\n\n\nThe switch \n\u2013color\n, instructs that files be coloured according to their\ntype. This is often the default but we are just being explicit. By\nspecifying the value \nalways\n, we ensure that colouring is always\napplied, even from a script.\n\n\nHave a look of the output the command produces and you will see that\n\nMAXKMERLENGTH=31\n and \nCATEGORIES=2\n parameters were passed into the\ncompiler.\n\n\nThis indicates that the default compilation was set for de Bruijn graph\nk-mers of maximum size 31 and to allow a maximum of just 2 read\ncategories. You can override these, and other, default configuration\nchoices using command line parameters. Assume, you want to run velvet\nwith a k-mer length of 41 using 3 categories, velvet needs to be\nrecompiled to enable this functionality by typing:\n\n\n1\n2\n3\nmake clean\nmake MAXKMERLENGTH=41 CATEGORIES=3\n./velveth\n\n\n\n\n\n\nDiscuss with the persons next to you the following questions:\\\nWhat are the consequences of the parameters you have given make for\nvelvet?\n\n\nMAXKMERLENGTH: increase the max k-mer length from 31 to 41\n\n\nCATEGORIES: paired-end data require to be put into separate categories.\nBy increasing this parameter from 2 to 3 allows you to process 3 paired\n/ mate-pair libraries and unpaired data.\n\n\nWhy does Velvet use k-mer 31 and 2 categories as default?\n\n\nPossibly a number of reason:\\\n- odd number to avoid palindromes\\\n- The first reads were very short (20-40 bp) and there were hardly any\npaired-end data\\\naround so there was no need to allow for longer k-mer lengths / more\ncategories.\\\n- For programmers: 31 bp get stored in 64 bits (using 2bit encoding)\n\n\nShould you get better results by using a longer k-mer length?\n\n\nIf you can achieve a good k-mer coverage - yes.\n\n\nWhat effect would the following compile-time parameters have on velvet:\\\n\nOPENMP=Y\n\n\nTurn on multithreading\n\n\nLONGSEQUENCES=Y\n\n\nAssembling reads / contigs longer than 32kb long\n\n\nBIGASSEMBLY=Y\n\n\nUsing more than 2.2 billion reads\n\n\nVBIGASSEMBLY=Y\n\n\nNot documented yet\n\n\nSINGLE_COV_CAT=Y\n\n\nMerge all coverage statistics into a single variable - save memory\n\n\nFor a further description of velvet compile and runtime parameters\nplease see the velvet Manual:\n\nhttps://github.com/dzerbino/velvet/wiki/Manual", 
            "title": "de novo Genome Assembly"
        }, 
        {
            "location": "/modules/btp-module-velvet/velvet/#key-learning-outcomes", 
            "text": "After completing this practical the trainee should be able to:    Compile velvet with appropriate compile-time parameters set for a\n    specific analysis    Be able to choose appropriate assembly parameters    Assemble a set of single-ended reads    Assemble a set of paired-end reads from a single insert-size library    Be able to visualise an assembly in AMOS Hawkeye    Understand the importance of using paired-end libraries in  de novo \n    genome assembly", 
            "title": "Key Learning Outcomes"
        }, 
        {
            "location": "/modules/btp-module-velvet/velvet/#resources-youll-be-using", 
            "text": "Although we have provided you with an environment which contains all the\ntools and data you will be using in this module, you may like to know\nwhere we have sourced those tools and data from.", 
            "title": "Resources You\u2019ll be Using"
        }, 
        {
            "location": "/modules/btp-module-velvet/velvet/#tools-used", 
            "text": "Velvet:  http://www.ebi.ac.uk/~zerbino/velvet/  AMOS Hawkeye:  http://apps.sourceforge.net/mediawiki/amos/index.php?title=Hawkeye  gnx-tools:  https://github.com/mh11/gnx-tools  FastQC:  http://www.bioinformatics.bbsrc.ac.uk/projects/fastqc/  R:  http://www.r-project.org/", 
            "title": "Tools Used"
        }, 
        {
            "location": "/modules/btp-module-velvet/velvet/#sources-of-data", 
            "text": "ftp://ftp.ensemblgenomes.org/pub/release-8/bacteria/fasta/Staphylococcus/s_aureus_mrsa252/dna/s_aureus_mrsa252.EB1_s_aureus_mrsa252.dna.chromosome.Chromosome.fa.gz    http://www.ebi.ac.uk/ena/data/view/SRS004748    ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR022/SRR022825/SRR022825.fastq.gz    ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR022/SRR022823/SRR022823.fastq.gz    http://www.ebi.ac.uk/ena/data/view/SRX008042    ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR022/SRR022852/SRR022852_1.fastq.gz    ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR022/SRR022852/SRR022852_2.fastq.gz    ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR023/SRR023408/SRR023408_1.fastq.gz    ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR023/SRR023408/SRR023408_2.fastq.gz    http://www.ebi.ac.uk/ena/data/view/SRX000181    ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR000/SRR000892/SRR000892.fastq.gz    ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR000/SRR000893/SRR000893.fastq.gz    http://www.ebi.ac.uk/ena/data/view/SRX007709    ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR022/SRR022863/SRR022863_1.fastq.gz    ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR022/SRR022863/SRR022863_2.fastq.gz", 
            "title": "Sources of Data"
        }, 
        {
            "location": "/modules/btp-module-velvet/velvet/#introduction", 
            "text": "The aim of this module is to become familiar with performing  de novo \ngenome assembly using Velvet, a de Bruijn graph based assembler, on a\nvariety of sequence data.", 
            "title": "Introduction"
        }, 
        {
            "location": "/modules/btp-module-velvet/velvet/#prepare-the-environment", 
            "text": "The first exercise should get you a little more comfortable with the\ncomputer environment and the command line.  First make sure that you are in the denovo working directory by typing:  1 cd /home/trainee/denovo   and making absolutely sure you\u2019re there by typing:  1 pwd   Now create sub-directories for this and the two other velvet practicals.\nAll these directories will be made as sub-directories of a directory for\nthe whole course called NGS. For this you can use the following\ncommands:  1 mkdir -p NGS/velvet/{part1,part2,part3}   The  -p  tells  mkdir  (make directory) to make any parent directories\nif they don\u2019t already exist. You could have created the above\ndirectories one-at-a-time by doing this instead:  1\n2\n3\n4\n5 mkdir NGS\nmkdir NGS/velvet\nmkdir NGS/velvet/part1\nmkdir NGS/velvet/part2\nmkdir NGS/velvet/part3   After creating the directories, examine the structure and move into the\ndirectory ready for the first velvet exercise by typing:  1\n2\n3 ls -R NGS\ncd NGS/velvet/part1\npwd", 
            "title": "Prepare the Environment"
        }, 
        {
            "location": "/modules/btp-module-velvet/velvet/#downloading-and-compiling-velvet", 
            "text": "For the duration of this workshop, all the software you require has been\nset up for you already. This might not be the case when you return to\n\u201creal life\u201d. Many of the programs you will need, including velvet, are\nquite easy to set up, it might be instructive to try a couple.  Although you will be using the preinstalled version of velvet, it is\nuseful to know how to compile velvet as some of the parameters you might\nlike to control can only be set at compile time. You can find the latest\nversion of velvet at:  http://www.ebi.ac.uk/~zerbino/velvet/  You could go to this URL and download the latest velvet version, or\nequivalently, you could type the following, which will download, unpack,\ninspect, compile and execute your locally compiled version of velvet:  1\n2\n3\n4\n5\n6\n7 cd /home/trainee/denovo/NGS/velvet/part1\npwd\ntar xzf /home/trainee/denovo/data/velvet_1.2.10.tgz\nls -R\ncd velvet_1.2.10\nmake\n./velveth   The standout displayed to screen when \u2019make\u2019 runs may contain an error\nmessage but it is ignored  Take a look at the executables you have created. They will be displayed\nas green by the command:  1 ls --color=always   The switch  \u2013color , instructs that files be coloured according to their\ntype. This is often the default but we are just being explicit. By\nspecifying the value  always , we ensure that colouring is always\napplied, even from a script.  Have a look of the output the command produces and you will see that MAXKMERLENGTH=31  and  CATEGORIES=2  parameters were passed into the\ncompiler.  This indicates that the default compilation was set for de Bruijn graph\nk-mers of maximum size 31 and to allow a maximum of just 2 read\ncategories. You can override these, and other, default configuration\nchoices using command line parameters. Assume, you want to run velvet\nwith a k-mer length of 41 using 3 categories, velvet needs to be\nrecompiled to enable this functionality by typing:  1\n2\n3 make clean\nmake MAXKMERLENGTH=41 CATEGORIES=3\n./velveth   Discuss with the persons next to you the following questions:\\\nWhat are the consequences of the parameters you have given make for\nvelvet?  MAXKMERLENGTH: increase the max k-mer length from 31 to 41  CATEGORIES: paired-end data require to be put into separate categories.\nBy increasing this parameter from 2 to 3 allows you to process 3 paired\n/ mate-pair libraries and unpaired data.  Why does Velvet use k-mer 31 and 2 categories as default?  Possibly a number of reason:\\\n- odd number to avoid palindromes\\\n- The first reads were very short (20-40 bp) and there were hardly any\npaired-end data\\\naround so there was no need to allow for longer k-mer lengths / more\ncategories.\\\n- For programmers: 31 bp get stored in 64 bits (using 2bit encoding)  Should you get better results by using a longer k-mer length?  If you can achieve a good k-mer coverage - yes.  What effect would the following compile-time parameters have on velvet:\\ OPENMP=Y  Turn on multithreading  LONGSEQUENCES=Y  Assembling reads / contigs longer than 32kb long  BIGASSEMBLY=Y  Using more than 2.2 billion reads  VBIGASSEMBLY=Y  Not documented yet  SINGLE_COV_CAT=Y  Merge all coverage statistics into a single variable - save memory  For a further description of velvet compile and runtime parameters\nplease see the velvet Manual: https://github.com/dzerbino/velvet/wiki/Manual", 
            "title": "Downloading and Compiling Velvet"
        }, 
        {
            "location": "/modules/btp-module-ngs-bio/biol/", 
            "text": "Key Learning Outcomes\n\n\nAfter completing this module the trainee should be able to:\n\n\n\n\n\n\nFind gene ontology enrichment in a list of differentially expressed\n    genes using R-based packages.\n\n\n\n\n\n\nRunning GO enrichment analysis using the web tool DAVID and the web\n    tool Gorilla\n\n\n\n\n\n\nTo run webtools such as REVIGO and STRING\n\n\n\n\n\n\nResources You\u2019ll be Using\n\n\nTools Used\n\n\nGoana from Limma:\n\n\nhttps://bioconductor.org/packages/release/bioc/html/limma.html\n\n\nDAVID:\n\n\nhttp://david.abcc.ncifcrf.gov\n\n\nGOrilla:\n\n\nhttp://cbl-gorilla.cs.technion.ac.il\n\n\nREVIGO:\n\n\nhttp://revigo.irb.hr\n\n\nSTRING:\n\n\nhttp://string-db.org\n\n\nIntroduction\n\n\nThe goal of this hands-on session is to allow you to develop some\nfamiliarity with commonly used, freely available R based packages and\nweb tools which can be used to gain biological insight from a\ndifferential expression experiment. We will use the differentially\nexpressed genes (DEGs) identified in the last session. First, we will\nlook at whether these genes are enriched for gene ontology terms which\ngives us some insight as to whether the DEGs are involved in particular\nfunctions. Then we will use a tool that constructs an interaction\nnetwork from these genes. This will allow us to identify clusters of\nDEGs that are known to interact.\n\n\nIn using any database tools it is always advisable to check whether they\nare regularly updated. We suggest that you experiment with more than one\ntool.\n\n\nGene ontology analysis with GOana\n\n\nFirst we will go back to the R environment and use the function GOana\nassociated with the limma package. To use this function we need to have\nour DEGs annotated with the entrez gene identifier for each gene.We did\nthis early on in our data processing. We use the fit object generated\nusing the voom function in limma for this analysis. To obtain more\ninformation regarding the goana function type ?goana within your R\nsession.\n\n\n1\nDE_GOana\n-\ngoana\n(\nfit_v\n,\n coef\n=\n2\n,\n geneid\n=\nfit_v\n$\ngenes\n$\nEntrez\n,\n FDR\n=\n0.05\n,\n species \n=\n \nHs\n,\n trend\n=\nF\n,\n plot\n=\nF\n \n)\n\n\n\n\n\n\n\nNow we will look at the most significant biological process (BP)ontology\nterms\n\n\n1\n2\n3\n4\nDE_GOana_top_BP_down\n-\n topGO\n(\nDE_GOana\n,\n ontology\n=\nc\n(\nBP\n),\n sort \n=\n \ndown\n,\n number\n=\n150L\n,\n truncate.term\n=\n50\n)\n\n\nhead\n(\nDE_GOana_top_BP_down\n,\n \n10\n)\n\nDE_GOana_top_BP_up\n-\n topGO\n(\nDE_GOana\n,\n ontology\n=\nc\n(\nBP\n),\n sort \n=\n \nup\n,\n number\n=\n150L\n,\n truncate.term\n=\n50\n)\n\n\nhead\n(\nDE_GOana_top_BP_up\n,\n \n10\n)\n\n\n\n\n\n\n\nRather than looking at biological process (BP) let\u2019s now look at\nmolecular function (MF) terms\n\n\n1\n2\n3\n4\nDE_GOana_top_MF_down\n-\n topGO\n(\nDE_GOana\n,\n ontology\n=\nc\n(\nMF\n),\n sort \n=\n \ndown\n,\n number\n=\n150L\n,\n truncate.term\n=\n50\n)\n\n\nhead\n(\nDE_GOana_top_MF_down\n,\n \n10\n)\n\nDE_GOana_top_MF_up\n-\n topGO\n(\nDE_GOana\n,\n ontology\n=\nc\n(\nMF\n),\n sort \n=\n \nup\n,\n number\n=\n150L\n,\n truncate.term\n=\n50\n)\n\n\nhead\n(\nDE_GOana_top_MF_up\n,\n \n10\n)\n\n\n\n\n\n\n\nQuestions and Answers\n\n\nBased on the above section, we have a little quiz for you:\n\n\nWhat is the general theme emerging when we look at biological process in\nthe down-regulated genes and the up-regulated genes?\n\n\nHint: Have a look at the top GO categories you are getting in the\nresults.\n\n\nHow many of the upregulated DEGs are annotated with the biological\nprocess term emphcell division: GO:0051301 ?\n\n\nAnswer = 76.\n\n\nHow many of the down-regulated DEGs are annotated with this term?\n\n\nAnswer = 112\n\n\nThere are a number of tools and packages available with the\nR-bioconductor repositories that you can use with your R code to run\nontologies and pathway analysis. Remember to marry up the annotation\nversions used to the annotation database versions to get the correct\nannotations.\n\n\nGene ontology analysis with DAVID\n\n\nClick on your Firefox web browser. Go to the DAVID website:\n\nhttp://david.abcc.ncifcrf.gov\n. Go to your edgeR folder and open the\nfile \nvoom_res_sig_lfc.txt\n using LibreOffice Calc. For the separator\noptions in LibreOffice Calc choose \nSeparated by tab\n. Once you have\nopened this file copy the \nEnsembl Gene Ids\n (Column A). This list can\nthen be pasted into DAVID.\\\n\\\nScreenshots of the DAVID website and the steps to move through the\nwebsite are provided in the presentation prepared for this session. Use\nthat material to work through this exercise.\\\n\\\nWe will use the Functional Annotation Clustering tool in DAVID. First we\nwill uncheck all the defaults and look only at the GO terms involving\nbiological process. After unchecking all the defaults, expand Gene\nOntology and select GOTERM_BP_FAT. Then select the button Functional\nAnnotation Clustering.\\\n\\\nThis will bring up a screen where GO terms are clustered. Statistical\ntesting is performed to assess whether the GO terms are more enriched in\nthe list of DEGs than would be expected by chance. You will see a column\nof P_Value and also adjusted P values. Have a look at the brief\ndescription of the statistical test used in DAVID\n(\nhttps://david.ncifcrf.gov/helps/functional_annotation.html\n).\n\n\nQuestions and Answers\n\n\nBased on the above section, we have a small quiz for you:\n\n\nWhat is the enrichment score of the most enriched cluster?\n\n\nAnswers may vary based on the genes entered and the options selected.\n\n\nHow many of the down-regulated DEGs are annotated with this term?\n\n\nAnswers may vary based on the genes entered and the options selected.\n\n\nDoes this seem to be sensible in an experiment that looks at the\nresponse of cancer cells to a stimulant?\n\n\nAnswers may vary based on the genes entered and the options selected.\n\n\nGene ontology analysis with GOrilla\n\n\nClick on your Firefox web browser. Go to the GOrilla website:\n\nhttp://cbl-gorilla.cs.technion.ac.il\n\\\nFor this tool we will use a background list of genes. Open the file\n\nvoom_res.txt\n using LibreOffice Calc. Copy the \nEnsembl Gene Ids\n\n(Column A) and paste this into GOrilla as the background set.\\\nAs in the previous exercise we will use the DEGs found by voom\n(\nvoom_res_sig_lfc.txt\n) as the Target set.\\\n\\\nWe will firstly look for enriched GO process terms. Screenshots of the\nwebsite showing the steps you need to follow are in the presentation for\nthis session.\\\n\\\nGOrilla will display the GO term hierarchy. This shows you which terms\nare parent and child terms and how the terms are related.Under this you\nwill find a table of the most significantly enriched GO terms. Have a\nlook at the DEGs associated with the most enriched clusters.\n\n\nQuestions and Answers\n\n\nBased on the above section, we have a little quiz for you:\n\n\nFind the GO term \nregulation of cell proliferation\n trace the parent\nterms back as far as you can go.\n\n\nregulation of cell proliferation \u2013 regulation of cellular process \u2013\nregulation of biological process \u2013 biological regulation \u2013 biological\nprocess\n\n\nWhat are the direct child terms of \nregulation of cell proliferation\n?\n\n\nregulation of stem cell proliferation, regulation of sooth muscle cell\nproliferation, positive regulation of cell proliferation\n\n\nWhat is the enrichment score for \nregulation of cell proliferation\n ?\nHow is this calculated (Hint: scroll down the page for the heading\nEnrichment).\n\n\nAnswer: 1.58 ((101/867)/(893/12099))\n\n\nREVIGO to reduce redundancy and visualise\n\n\nWe can use the results generated by the GOrilla web tool as input to\nREVIGO which will summarise the GO data and allow us to visualize the\nsimplified data.Click on the link Visualize output in REViGO. Follow the\nscreen shots in the presentation.Go to the treemap view.\n\n\nWhat are the main functional categories emerging in this analysis?\n\n\nSTRING\n\n\nUsing STRING to look at networks that may be formed by the DEGsClick on\nyour Firefox web browser. Go to the STRING website:\n\nhttp://string-db.org\n. For this exercise we will only use the top 500\nDEGs. Go to the file (\nvoom_res_sig_lfc.txt\n) copy only the top 500.\nThese will be pasted as input to the STRING website. Follow the screen\nshots in the presentation.\\\n\\\nYou will see a large interaction network being built from the 500\nDEGs.We will refine this by clicking on the Data settings tab and\nselecting \nhigh confidence\n (see the screen shot in the presentation).\nLook at the gene clusters that are generated.\n\n\nFind the gene CDK1. Look at the cluster generated around this gene. What\nother DEGs are interaction partners of CDK1.\\\nClick on CDK1 and find what functions it is involved in. Click on the\ninteraction partners of CDK1 and find their functions.\\\nDoes this help in further explaining some of the gene ontology results?\n\n\nExplore other clusters that are formed in this analysis.", 
            "title": "NGS Biological Insight"
        }, 
        {
            "location": "/modules/btp-module-ngs-bio/biol/#key-learning-outcomes", 
            "text": "After completing this module the trainee should be able to:    Find gene ontology enrichment in a list of differentially expressed\n    genes using R-based packages.    Running GO enrichment analysis using the web tool DAVID and the web\n    tool Gorilla    To run webtools such as REVIGO and STRING", 
            "title": "Key Learning Outcomes"
        }, 
        {
            "location": "/modules/btp-module-ngs-bio/biol/#resources-youll-be-using", 
            "text": "", 
            "title": "Resources You\u2019ll be Using"
        }, 
        {
            "location": "/modules/btp-module-ngs-bio/biol/#tools-used", 
            "text": "Goana from Limma:  https://bioconductor.org/packages/release/bioc/html/limma.html  DAVID:  http://david.abcc.ncifcrf.gov  GOrilla:  http://cbl-gorilla.cs.technion.ac.il  REVIGO:  http://revigo.irb.hr  STRING:  http://string-db.org", 
            "title": "Tools Used"
        }, 
        {
            "location": "/modules/btp-module-ngs-bio/biol/#introduction", 
            "text": "The goal of this hands-on session is to allow you to develop some\nfamiliarity with commonly used, freely available R based packages and\nweb tools which can be used to gain biological insight from a\ndifferential expression experiment. We will use the differentially\nexpressed genes (DEGs) identified in the last session. First, we will\nlook at whether these genes are enriched for gene ontology terms which\ngives us some insight as to whether the DEGs are involved in particular\nfunctions. Then we will use a tool that constructs an interaction\nnetwork from these genes. This will allow us to identify clusters of\nDEGs that are known to interact.  In using any database tools it is always advisable to check whether they\nare regularly updated. We suggest that you experiment with more than one\ntool.", 
            "title": "Introduction"
        }, 
        {
            "location": "/modules/btp-module-ngs-bio/biol/#gene-ontology-analysis-with-goana", 
            "text": "First we will go back to the R environment and use the function GOana\nassociated with the limma package. To use this function we need to have\nour DEGs annotated with the entrez gene identifier for each gene.We did\nthis early on in our data processing. We use the fit object generated\nusing the voom function in limma for this analysis. To obtain more\ninformation regarding the goana function type ?goana within your R\nsession.  1 DE_GOana - goana ( fit_v ,  coef = 2 ,  geneid = fit_v $ genes $ Entrez ,  FDR = 0.05 ,  species  =   Hs ,  trend = F ,  plot = F   )    Now we will look at the most significant biological process (BP)ontology\nterms  1\n2\n3\n4 DE_GOana_top_BP_down -  topGO ( DE_GOana ,  ontology = c ( BP ),  sort  =   down ,  number = 150L ,  truncate.term = 50 )  head ( DE_GOana_top_BP_down ,   10 ) \nDE_GOana_top_BP_up -  topGO ( DE_GOana ,  ontology = c ( BP ),  sort  =   up ,  number = 150L ,  truncate.term = 50 )  head ( DE_GOana_top_BP_up ,   10 )    Rather than looking at biological process (BP) let\u2019s now look at\nmolecular function (MF) terms  1\n2\n3\n4 DE_GOana_top_MF_down -  topGO ( DE_GOana ,  ontology = c ( MF ),  sort  =   down ,  number = 150L ,  truncate.term = 50 )  head ( DE_GOana_top_MF_down ,   10 ) \nDE_GOana_top_MF_up -  topGO ( DE_GOana ,  ontology = c ( MF ),  sort  =   up ,  number = 150L ,  truncate.term = 50 )  head ( DE_GOana_top_MF_up ,   10 )", 
            "title": "Gene ontology analysis with GOana"
        }, 
        {
            "location": "/modules/btp-module-ngs-bio/biol/#questions-and-answers", 
            "text": "Based on the above section, we have a little quiz for you:  What is the general theme emerging when we look at biological process in\nthe down-regulated genes and the up-regulated genes?  Hint: Have a look at the top GO categories you are getting in the\nresults.  How many of the upregulated DEGs are annotated with the biological\nprocess term emphcell division: GO:0051301 ?  Answer = 76.  How many of the down-regulated DEGs are annotated with this term?  Answer = 112  There are a number of tools and packages available with the\nR-bioconductor repositories that you can use with your R code to run\nontologies and pathway analysis. Remember to marry up the annotation\nversions used to the annotation database versions to get the correct\nannotations.", 
            "title": "Questions and Answers"
        }, 
        {
            "location": "/modules/btp-module-ngs-bio/biol/#gene-ontology-analysis-with-david", 
            "text": "Click on your Firefox web browser. Go to the DAVID website: http://david.abcc.ncifcrf.gov . Go to your edgeR folder and open the\nfile  voom_res_sig_lfc.txt  using LibreOffice Calc. For the separator\noptions in LibreOffice Calc choose  Separated by tab . Once you have\nopened this file copy the  Ensembl Gene Ids  (Column A). This list can\nthen be pasted into DAVID.\\\n\\\nScreenshots of the DAVID website and the steps to move through the\nwebsite are provided in the presentation prepared for this session. Use\nthat material to work through this exercise.\\\n\\\nWe will use the Functional Annotation Clustering tool in DAVID. First we\nwill uncheck all the defaults and look only at the GO terms involving\nbiological process. After unchecking all the defaults, expand Gene\nOntology and select GOTERM_BP_FAT. Then select the button Functional\nAnnotation Clustering.\\\n\\\nThis will bring up a screen where GO terms are clustered. Statistical\ntesting is performed to assess whether the GO terms are more enriched in\nthe list of DEGs than would be expected by chance. You will see a column\nof P_Value and also adjusted P values. Have a look at the brief\ndescription of the statistical test used in DAVID\n( https://david.ncifcrf.gov/helps/functional_annotation.html ).", 
            "title": "Gene ontology analysis with DAVID"
        }, 
        {
            "location": "/modules/btp-module-ngs-bio/biol/#questions-and-answers_1", 
            "text": "Based on the above section, we have a small quiz for you:  What is the enrichment score of the most enriched cluster?  Answers may vary based on the genes entered and the options selected.  How many of the down-regulated DEGs are annotated with this term?  Answers may vary based on the genes entered and the options selected.  Does this seem to be sensible in an experiment that looks at the\nresponse of cancer cells to a stimulant?  Answers may vary based on the genes entered and the options selected.", 
            "title": "Questions and Answers"
        }, 
        {
            "location": "/modules/btp-module-ngs-bio/biol/#gene-ontology-analysis-with-gorilla", 
            "text": "Click on your Firefox web browser. Go to the GOrilla website: http://cbl-gorilla.cs.technion.ac.il \\\nFor this tool we will use a background list of genes. Open the file voom_res.txt  using LibreOffice Calc. Copy the  Ensembl Gene Ids \n(Column A) and paste this into GOrilla as the background set.\\\nAs in the previous exercise we will use the DEGs found by voom\n( voom_res_sig_lfc.txt ) as the Target set.\\\n\\\nWe will firstly look for enriched GO process terms. Screenshots of the\nwebsite showing the steps you need to follow are in the presentation for\nthis session.\\\n\\\nGOrilla will display the GO term hierarchy. This shows you which terms\nare parent and child terms and how the terms are related.Under this you\nwill find a table of the most significantly enriched GO terms. Have a\nlook at the DEGs associated with the most enriched clusters.", 
            "title": "Gene ontology analysis with GOrilla"
        }, 
        {
            "location": "/modules/btp-module-ngs-bio/biol/#questions-and-answers_2", 
            "text": "Based on the above section, we have a little quiz for you:  Find the GO term  regulation of cell proliferation  trace the parent\nterms back as far as you can go.  regulation of cell proliferation \u2013 regulation of cellular process \u2013\nregulation of biological process \u2013 biological regulation \u2013 biological\nprocess  What are the direct child terms of  regulation of cell proliferation ?  regulation of stem cell proliferation, regulation of sooth muscle cell\nproliferation, positive regulation of cell proliferation  What is the enrichment score for  regulation of cell proliferation  ?\nHow is this calculated (Hint: scroll down the page for the heading\nEnrichment).  Answer: 1.58 ((101/867)/(893/12099))", 
            "title": "Questions and Answers"
        }, 
        {
            "location": "/modules/btp-module-ngs-bio/biol/#revigo-to-reduce-redundancy-and-visualise", 
            "text": "We can use the results generated by the GOrilla web tool as input to\nREVIGO which will summarise the GO data and allow us to visualize the\nsimplified data.Click on the link Visualize output in REViGO. Follow the\nscreen shots in the presentation.Go to the treemap view.  What are the main functional categories emerging in this analysis?", 
            "title": "REVIGO to reduce redundancy and visualise"
        }, 
        {
            "location": "/modules/btp-module-ngs-bio/biol/#string", 
            "text": "Using STRING to look at networks that may be formed by the DEGsClick on\nyour Firefox web browser. Go to the STRING website: http://string-db.org . For this exercise we will only use the top 500\nDEGs. Go to the file ( voom_res_sig_lfc.txt ) copy only the top 500.\nThese will be pasted as input to the STRING website. Follow the screen\nshots in the presentation.\\\n\\\nYou will see a large interaction network being built from the 500\nDEGs.We will refine this by clicking on the Data settings tab and\nselecting  high confidence  (see the screen shot in the presentation).\nLook at the gene clusters that are generated.  Find the gene CDK1. Look at the cluster generated around this gene. What\nother DEGs are interaction partners of CDK1.\\\nClick on CDK1 and find what functions it is involved in. Click on the\ninteraction partners of CDK1 and find their functions.\\\nDoes this help in further explaining some of the gene ontology results?  Explore other clusters that are formed in this analysis.", 
            "title": "STRING"
        }, 
        {
            "location": "/modules/cancer-module-alignment/alignment/", 
            "text": "Key Learning Outcomes\n\n\nAfter completing this practical the trainee should be able to:\n\n\n\n\n\n\nPerform the simple NGS data alignment task against reference data.\n\n\n\n\n\n\nLearn about the SAM/BAM formats for further manipulation.\n\n\n\n\n\n\nBe able to sort and index BAM format for visualisation purposes.\n\n\n\n\n\n\nResources You\u2019ll be Using\n\n\nTools Used\n\n\nBWA Burrows-Wheeler Algorithm:\n\n\nhttp://bio-bwa.sourceforge.net\n\n\nSamtools:\n\n\nhttp://picard.sourceforge.net/\n\n\nUseful Links\n\n\nSAM Specification:\n\n\nhttp://samtools.sourceforge.net/SAM1.pdf\n\n\nExplain SAM Flags:\n\n\nhttp://picard.sourceforge.net/explain-flags.html\n\n\nSources of Data\n\n\nhttp://sra.dnanexus.com/studies/ERP001071\n\n\nIntroduction\n\n\nThe goal of this hands-on session is to perform an NGS alignment on the\nsequencing data coming from a tumour and normal group of samples. We\nwill align raw sequencing data to the human genome using the BWA aligner\nand then we will discuss the sequence alignment and mapping format\n(SAM). SAM to BAM coversion, indexing and sorting will also be\ndemonstrated. These are important and essential steps for downstream\nprocessing of the aligned BAM files.\n\n\nThis data is the whole genome sequencing of a lung adenocarcinoma\npatient AK55. It was downloaded from \nERP001071\n (a link is provided in\nthe data source section). Only the HiSeq2000 data for \nBlood\n and\n\nliverMets\n were analysed.\n\n\nAccession numbers associated with read data assigned by the European\nBioinformatics Institute (EBI) start with \u2019ER\u2019. e.g. ERP is the study ID\nand ERR is the run. The original FASTQ files downloaded had the ERR\nnumber in front of each read name in the FASTQ file. The read name had\nto be edited to remove the ERR number at the start of the name. This had\ncaused problems for downstream programs such as Picard for marking\noptical duplicates.\n\n\nWe have used 4 Blood samples (8 PE \n.fastq.gz\n files) and 5 Liver\nsamples (10 PE \n.fastq.gz\n files) data from this study to perform the\nwhole genome alignment using the BWA aligner. The whole process took\n>150K CPU seconds per sample and the precomputed alignment will be used\nin different sections of this workshop.\n\n\nPrepare the Environment\n\n\nBy now you know about the raw sequence fastq format generated by the\nIllumina sequencers. Next we will see how fastq files are aligned with\nthe reference genome and what are the resulting standard alignment\nformat. In the interest of time we have selected only 1 million paired\nreads from a Blood sample to demonstrate a BWA command. The remainining\nalignments have already been performed for you and will be required in\nthe subsequent modules of the workshop.\n\n\nThe input data for this section can be found in the \nalignment\n\ndirectory on your desktop. Please follow the commands below to go to the\nright folder and view top 10 lines of the input fastq file:\n\n\nOpen the Terminal.\n\n\nFirst, go to the right folder, where the data are stored.\n\n\n1\n2\n3\ncd /home/trainee/alignment\nls\nzless input/SM_Blood_ID_ERR059356.subset_R1.fastq.gz\n\n\n\n\n\n\nAlignment\n\n\nYou already know that there are a number of competing tools for short\nread alignment, each with its own set of strengths, weaknesses, and\ncaveats. Here we will use BWA, a widely used aligner based on the\nBurrows-Wheeler Algorithm. The alignment involves two steps- first,\nindexing the genome and then running the alignment command. BWA is a\nsoftware package for mapping low-divergent sequences against a large\nreference genome, such as the human genome. It consists of three\nalgorithms: BWA-backtrack, BWA-SW and BWA-MEM. The first algorithm is\ndesigned for Illumina sequence reads up to 100bp, while the rest two for\nlonger sequences ranged from 70bp to 1Mbp. BWA-MEM and BWA-SW share\nsimilar features such as long-read support and split alignment, but\nBWA-MEM, which is the latest, is generally recommended for high-quality\nqueries as it is faster and more accurate. BWA-MEM also has better\nperformance than BWA-backtrack for 70-100bp Illumina reads (ref:bwa\nmanual).\n\n\nBWA has a number of parameters in order to perform the alignment. To\nview them all type\n\n\n1\nbwa \npress enter\n\n\n\n\n\n\n\nBWA uses an indexed genome for the alignment in order to keep its memory\nfootprint small. Because of time constraints we will not be running the\nindexing command. It is run only once for a version of genome, and the\ncomplete command to index the human genome version hg19 is given below.\n\n\nYou DO NOT need to run this command. This has already been run for you.\n\n\n1\n  bwa index -p bwaIndex/human_g1k_v37.fasta -a bwtsw human_g1k_v37.fasta\n\n\n\n\n\n\nThis command will output 6 files that constitute the index. These files\nthat have the prefix \nhuman_v37\n are stored in the \nbwa_index\n\nsubdirectory. To view if they files have been successfully created type:\n\n\nWe have used the following arguments for the indexing of the genome.\n\n\n-p\n:   = Prefix of the output database [same as db filename]\n\n\n-a\n:   = Algorithm for constructing BWT index. This method works with the\n    whole human genome.\n\n\nref genome filename\n:   = the last argument is the name of the reference genome file in the\n    fasta format\n\n\n\n\n\n1\nls -l bwaIndex\n\n\n\n\n\n\nNow that the genome is indexed we can move on to the actual alignment.\nThe first argument for \nbwa\n is the basename of the index for the genome\nto be searched; in our case this is \nhuman_g1k_v37.fasta\n.\n\n\nAlign the reads from Blood samples using the following command:\n\n\n1\nbwa mem -M -t 4 -R @RG\\tSM:Blood\\tID:ERR059354\\tLB:lb\\tPL:ILLUMINA human_g1k_v37 SM_Blood_ID_ERR059356.subset_R1.fastq.gz SM_Blood_ID_ERR059356.subset_R2.fastq.gz \n SM_Blood.sam\n\n\n\n\n\n\nThe above command outputs the alignment in SAM format and stores them in\nthe file \nSM_Blood.sam\n.\n\n\nWe have used the following arguments for the alignment of the reads.\n\n\nmem\n:   = fast mode of high quality input such the Illumina\n\n\n-M\n:   = flags extra hits as secondary. This is needed for compatibility\n    with other tools downstream.\n\n\n-t\n:   = Number of threads.\n\n\n-R\n:   = Complete read group header line.\n\n\nThe SAM (Sequence Alignment/Map) format is currently the de facto\nstandard for storing large nucleotide sequence alignments. It is a\nTAB-delimited text format consisting of a header section, which is\noptional, and an alignment section. If present, the header must be prior\nto the alignments. Header lines start with \n@\n, while alignment lines do\nnot. Each alignment line has 11 mandatory fields for essential alignment\ninformation such as mapping position.\n\n\nLook at the top 10 lines of the SAM file by typing:\n\n\n1\nhead -n 10 SM_Blood.sam\n\n\n\n\n\n\nCan you distinguish between the header of the SAM format and the actual\nalignments?\n\n\nThe header line starts with the letter \u2018@\u2019, i.e.:\n\n\n\n\n@HD   VN:1.0       SO:unsorted             \n\n  @SQ   SN:chr1      LN:195471971            \n\n  @PG   ID:Bowtie2   PN:bowtie2     VN:2.2.4   CL:\u201c/tools/bowtie2/bowtie2-default/bowtie2-align-s \u2013wrapper basic-0 -x bowtie_index/mm10 -q Oct4.fastq\u201d\n\n\n\n\nWhile, the actual alignments start with read id, i.e.:\n\n\n\n\nSRR002012.45   0    etc  \n\n  SRR002012.48   16   chr1   etc\n\n\n\n\nWhat kind of information does the header provide?\n\n\n\n\n\n\n@HD: Header line; VN: Format version; SO: the sort order of\n    alignments.\n\n\n\n\n\n\n@SQ: Reference sequence information; SN: reference sequence name;\n    LN: reference sequence length.\n\n\n\n\n\n\n@PG: Read group information; ID: Read group identifier; VN: Program\n    version; CL: the command line that produces the alignment.\n\n\n\n\n\n\nTo which chromosome are the reads mapped?\n\n\nChromosome 1.\n\n\nManipulate SAM output\n\n\nSAM files are rather big and when dealing with a high volume of NGS\ndata, storage space can become an issue. As we have already seen, we can\nconvert SAM to BAM files (their binary equivalent that are not human\nreadable) that occupy much less space.\n\n\nConvert SAM to BAM using \nsamtools view\n and store the output in the\nfile \nSM_Blood.bam\n. You have to instruct \nsamtools view\n that the input\nis in SAM format (\n-S\n), the output should be in BAM format (\n-b\n) and\nthat you want the output to be stored in the file specified by the \n-o\n\noption:\n\n\n1\nsamtools view -bSo SM_Blood.bam SM_Blood.sam\n\n\n\n\n\n\nCompute summary stats for the Flag values associated with the alignments\nusing:\n\n\n1\nsamtools flagstat SM_Blood.bam\n\n\n\n\n\n\nPost Alignment Visualisation option\n\n\nIGV is a stand-alone genome browser that can be used to visualise the\nBAM outputs.. Please check their website\n(\nhttp://www.broadinstitute.org/igv/\n) for all the formats that IGV can\ndisplay.\n\n\nWe will be using IGV later in the workshop for viewing a BAM file in the\ngenome browser. It requires the index of the BAM file to be in the same\nfolder as where the BAM file is. The index file should have the same\nname as the BAM file and the suffix \n.bai\n. Finally, to create the index\nof a BAM file you need to make sure that the file is sorted according to\nchromosomal coordinates.\n\n\nSort alignments according to chromosomal position and store the result\nin the file with the prefix \nBlood.sorted\n:\n\n\n1\nsamtools sort SM_Blood.bam SM_Blood.sorted\n\n\n\n\n\n\nIndex the sorted file.\n\n\n1\nsamtools index SM_Blood.sorted.bam\n\n\n\n\n\n\nThe indexing will create a file called \nBlood.sorted.bam.bai\n. Note that\nyou don\u2019t have to specify the name of the index file when running\n\nsamtools index\n, it simply appends a \n.bai\n suffix to the input BAM\nfile.\n\n\nHow can you quickly find out whether a BAM file is already coordinate\nsorted or not ?\n\n\nUse samtools view command to look at the SAM header.", 
            "title": "Read Alignment"
        }, 
        {
            "location": "/modules/cancer-module-alignment/alignment/#key-learning-outcomes", 
            "text": "After completing this practical the trainee should be able to:    Perform the simple NGS data alignment task against reference data.    Learn about the SAM/BAM formats for further manipulation.    Be able to sort and index BAM format for visualisation purposes.", 
            "title": "Key Learning Outcomes"
        }, 
        {
            "location": "/modules/cancer-module-alignment/alignment/#resources-youll-be-using", 
            "text": "", 
            "title": "Resources You\u2019ll be Using"
        }, 
        {
            "location": "/modules/cancer-module-alignment/alignment/#tools-used", 
            "text": "BWA Burrows-Wheeler Algorithm:  http://bio-bwa.sourceforge.net  Samtools:  http://picard.sourceforge.net/", 
            "title": "Tools Used"
        }, 
        {
            "location": "/modules/cancer-module-alignment/alignment/#useful-links", 
            "text": "SAM Specification:  http://samtools.sourceforge.net/SAM1.pdf  Explain SAM Flags:  http://picard.sourceforge.net/explain-flags.html", 
            "title": "Useful Links"
        }, 
        {
            "location": "/modules/cancer-module-alignment/alignment/#sources-of-data", 
            "text": "http://sra.dnanexus.com/studies/ERP001071", 
            "title": "Sources of Data"
        }, 
        {
            "location": "/modules/cancer-module-alignment/alignment/#introduction", 
            "text": "The goal of this hands-on session is to perform an NGS alignment on the\nsequencing data coming from a tumour and normal group of samples. We\nwill align raw sequencing data to the human genome using the BWA aligner\nand then we will discuss the sequence alignment and mapping format\n(SAM). SAM to BAM coversion, indexing and sorting will also be\ndemonstrated. These are important and essential steps for downstream\nprocessing of the aligned BAM files.  This data is the whole genome sequencing of a lung adenocarcinoma\npatient AK55. It was downloaded from  ERP001071  (a link is provided in\nthe data source section). Only the HiSeq2000 data for  Blood  and liverMets  were analysed.  Accession numbers associated with read data assigned by the European\nBioinformatics Institute (EBI) start with \u2019ER\u2019. e.g. ERP is the study ID\nand ERR is the run. The original FASTQ files downloaded had the ERR\nnumber in front of each read name in the FASTQ file. The read name had\nto be edited to remove the ERR number at the start of the name. This had\ncaused problems for downstream programs such as Picard for marking\noptical duplicates.  We have used 4 Blood samples (8 PE  .fastq.gz  files) and 5 Liver\nsamples (10 PE  .fastq.gz  files) data from this study to perform the\nwhole genome alignment using the BWA aligner. The whole process took\n>150K CPU seconds per sample and the precomputed alignment will be used\nin different sections of this workshop.", 
            "title": "Introduction"
        }, 
        {
            "location": "/modules/cancer-module-alignment/alignment/#prepare-the-environment", 
            "text": "By now you know about the raw sequence fastq format generated by the\nIllumina sequencers. Next we will see how fastq files are aligned with\nthe reference genome and what are the resulting standard alignment\nformat. In the interest of time we have selected only 1 million paired\nreads from a Blood sample to demonstrate a BWA command. The remainining\nalignments have already been performed for you and will be required in\nthe subsequent modules of the workshop.  The input data for this section can be found in the  alignment \ndirectory on your desktop. Please follow the commands below to go to the\nright folder and view top 10 lines of the input fastq file:  Open the Terminal.  First, go to the right folder, where the data are stored.  1\n2\n3 cd /home/trainee/alignment\nls\nzless input/SM_Blood_ID_ERR059356.subset_R1.fastq.gz", 
            "title": "Prepare the Environment"
        }, 
        {
            "location": "/modules/cancer-module-alignment/alignment/#alignment", 
            "text": "You already know that there are a number of competing tools for short\nread alignment, each with its own set of strengths, weaknesses, and\ncaveats. Here we will use BWA, a widely used aligner based on the\nBurrows-Wheeler Algorithm. The alignment involves two steps- first,\nindexing the genome and then running the alignment command. BWA is a\nsoftware package for mapping low-divergent sequences against a large\nreference genome, such as the human genome. It consists of three\nalgorithms: BWA-backtrack, BWA-SW and BWA-MEM. The first algorithm is\ndesigned for Illumina sequence reads up to 100bp, while the rest two for\nlonger sequences ranged from 70bp to 1Mbp. BWA-MEM and BWA-SW share\nsimilar features such as long-read support and split alignment, but\nBWA-MEM, which is the latest, is generally recommended for high-quality\nqueries as it is faster and more accurate. BWA-MEM also has better\nperformance than BWA-backtrack for 70-100bp Illumina reads (ref:bwa\nmanual).  BWA has a number of parameters in order to perform the alignment. To\nview them all type  1 bwa  press enter    BWA uses an indexed genome for the alignment in order to keep its memory\nfootprint small. Because of time constraints we will not be running the\nindexing command. It is run only once for a version of genome, and the\ncomplete command to index the human genome version hg19 is given below.  You DO NOT need to run this command. This has already been run for you.  1   bwa index -p bwaIndex/human_g1k_v37.fasta -a bwtsw human_g1k_v37.fasta   This command will output 6 files that constitute the index. These files\nthat have the prefix  human_v37  are stored in the  bwa_index \nsubdirectory. To view if they files have been successfully created type:  We have used the following arguments for the indexing of the genome.  -p\n:   = Prefix of the output database [same as db filename]  -a\n:   = Algorithm for constructing BWT index. This method works with the\n    whole human genome.  ref genome filename\n:   = the last argument is the name of the reference genome file in the\n    fasta format   1 ls -l bwaIndex   Now that the genome is indexed we can move on to the actual alignment.\nThe first argument for  bwa  is the basename of the index for the genome\nto be searched; in our case this is  human_g1k_v37.fasta .  Align the reads from Blood samples using the following command:  1 bwa mem -M -t 4 -R @RG\\tSM:Blood\\tID:ERR059354\\tLB:lb\\tPL:ILLUMINA human_g1k_v37 SM_Blood_ID_ERR059356.subset_R1.fastq.gz SM_Blood_ID_ERR059356.subset_R2.fastq.gz   SM_Blood.sam   The above command outputs the alignment in SAM format and stores them in\nthe file  SM_Blood.sam .  We have used the following arguments for the alignment of the reads.  mem\n:   = fast mode of high quality input such the Illumina  -M\n:   = flags extra hits as secondary. This is needed for compatibility\n    with other tools downstream.  -t\n:   = Number of threads.  -R\n:   = Complete read group header line.  The SAM (Sequence Alignment/Map) format is currently the de facto\nstandard for storing large nucleotide sequence alignments. It is a\nTAB-delimited text format consisting of a header section, which is\noptional, and an alignment section. If present, the header must be prior\nto the alignments. Header lines start with  @ , while alignment lines do\nnot. Each alignment line has 11 mandatory fields for essential alignment\ninformation such as mapping position.  Look at the top 10 lines of the SAM file by typing:  1 head -n 10 SM_Blood.sam   Can you distinguish between the header of the SAM format and the actual\nalignments?  The header line starts with the letter \u2018@\u2019, i.e.:   @HD   VN:1.0       SO:unsorted              \n  @SQ   SN:chr1      LN:195471971             \n  @PG   ID:Bowtie2   PN:bowtie2     VN:2.2.4   CL:\u201c/tools/bowtie2/bowtie2-default/bowtie2-align-s \u2013wrapper basic-0 -x bowtie_index/mm10 -q Oct4.fastq\u201d   While, the actual alignments start with read id, i.e.:   SRR002012.45   0    etc   \n  SRR002012.48   16   chr1   etc   What kind of information does the header provide?    @HD: Header line; VN: Format version; SO: the sort order of\n    alignments.    @SQ: Reference sequence information; SN: reference sequence name;\n    LN: reference sequence length.    @PG: Read group information; ID: Read group identifier; VN: Program\n    version; CL: the command line that produces the alignment.    To which chromosome are the reads mapped?  Chromosome 1.", 
            "title": "Alignment"
        }, 
        {
            "location": "/modules/cancer-module-alignment/alignment/#manipulate-sam-output", 
            "text": "SAM files are rather big and when dealing with a high volume of NGS\ndata, storage space can become an issue. As we have already seen, we can\nconvert SAM to BAM files (their binary equivalent that are not human\nreadable) that occupy much less space.  Convert SAM to BAM using  samtools view  and store the output in the\nfile  SM_Blood.bam . You have to instruct  samtools view  that the input\nis in SAM format ( -S ), the output should be in BAM format ( -b ) and\nthat you want the output to be stored in the file specified by the  -o \noption:  1 samtools view -bSo SM_Blood.bam SM_Blood.sam   Compute summary stats for the Flag values associated with the alignments\nusing:  1 samtools flagstat SM_Blood.bam", 
            "title": "Manipulate SAM output"
        }, 
        {
            "location": "/modules/cancer-module-alignment/alignment/#post-alignment-visualisation-option", 
            "text": "IGV is a stand-alone genome browser that can be used to visualise the\nBAM outputs.. Please check their website\n( http://www.broadinstitute.org/igv/ ) for all the formats that IGV can\ndisplay.  We will be using IGV later in the workshop for viewing a BAM file in the\ngenome browser. It requires the index of the BAM file to be in the same\nfolder as where the BAM file is. The index file should have the same\nname as the BAM file and the suffix  .bai . Finally, to create the index\nof a BAM file you need to make sure that the file is sorted according to\nchromosomal coordinates.  Sort alignments according to chromosomal position and store the result\nin the file with the prefix  Blood.sorted :  1 samtools sort SM_Blood.bam SM_Blood.sorted   Index the sorted file.  1 samtools index SM_Blood.sorted.bam   The indexing will create a file called  Blood.sorted.bam.bai . Note that\nyou don\u2019t have to specify the name of the index file when running samtools index , it simply appends a  .bai  suffix to the input BAM\nfile.  How can you quickly find out whether a BAM file is already coordinate\nsorted or not ?  Use samtools view command to look at the SAM header.", 
            "title": "Post Alignment Visualisation option"
        }, 
        {
            "location": "/modules/cancer-module-snv/snv/", 
            "text": "Key Learning Outcomes\n\n\nAfter completing this practical the trainee should be able to:\n\n\n\n\n\n\nPrepare raw BAM alignments for variant detection\n\n\n\n\n\n\nPerform QC measures on BAM files\n\n\n\n\n\n\nUnderstand and perform simple variant detection on paired NGS data\n\n\n\n\n\n\nAdd annotation information to raw variant calls\n\n\n\n\n\n\nVisualise variant calls using IGV\n\n\n\n\n\n\nResources You\u2019ll be Using\n\n\nTools Used\n\n\nSAMTools:\n\n\nhttp://sourceforge.net/projects/samtools/\n\n\nIGV:\n\n\nhttp://www.broadinstitute.org/igv/\n\n\nGenome Analysis Toolkit:\n\n\nhttp://www.broadinstitute.org/gatk/\n\n\nPicard:\n\n\nhttp://picard.sourceforge.net/\n\n\nMuTect:\n\n\nhttp://www.broadinstitute.org/cancer/cga/mutect/\n\n\nStrelka:\n\n\nhttps://sites.google.com/site/strelkasomaticvariantcaller/\n\n\nVarScan2:\n\n\nhttps://github.com/dkoboldt/varscan/\n\n\nVariant Effect Predictor:\n\n\nhttp://www.ensembl.org/info/docs/tools/vep\n\n\nGEMINI:\n\n\nhttp://gemini.readthedocs.org\n\n\nSources of Data\n\n\nhttp://sra.dnanexus.com/studies/ERP001071\n\n\nhttp://www.ncbi.nlm.nih.gov/pubmed/22194472\n\n\nIntroduction\n\n\nThis talk is based on Introduction to DNA-Seq processing for cancer data\nby Mathieu Bourgey, Ph.D\n\n\n*\nhttps://bitbucket.org/mugqic/muqpic\n_pipelines*\n\n\nThe goal of this hands-on session is to present the main steps that are\ncommonly used to process and to analyze cancer sequencing data. We will\nfocus only on whole genome data and provide command lines that allow\ndetecting Single Nucleotide Variants (SNV). This workshop will show you\nhow to launch individual steps of a complete DNA-Seq SNV pipeline using\ncancer data.\n\n\nIn the second part of the tutorial we will also be using IGV to\nvisualise and manually inspect candidate variant calls.\n\n\nPrepare the Environment\n\n\nWe will use a dataset derived from whole genome sequencing of a\n33-yr-old lung adenocarcinoma patient, who is a never-smoker and has no\nfamilial cancer history.\n\n\nThe data consists of whole genome sequencing of liver metastatic lung\ncancer (frozen), primary lung cancer (FFPE) and blood tissue of a lung\nadenocarcinoma patient (AK55).\n\n\nThe BAM alignment files are contained in the subdirectory called\n\nalignment\n and are located in the following subdirectories:\n\n\nnormal/normal.sorted.bam\n and \nnormal/normal.sorted.bam.bai\n\n:   \\\n\n\ntumor/tumor.sorted.bam\n and \ntumor/tumor.sorted.bam.bai\n\n:   \\\n\n\nThese files are based on subsetting the whole genomes derived from blood\nand liver metastases to the first 10Mb of chromosome 4. This will allow\nour analyses to run in a sufficient time during the workshop, but it\u2019s\nworth being aware that this is less \\\n 0.5% of the genome which\nhighlights the length of time and resourced required to perform cancer\ngenomics on full genomes!\n\n\nThe initial structure of your folders should look like this:\n\n\n1\n2\n3\n4\n5\n-- alignment/             # bam files\n  -- normal/              # The blood sample directory containing bam files\n  -- tumour/               # The tumour sample directory containing bam files \\\\\n-- ref/                   # Contains reference genome files      \n-- commands.sh            # cheat sheet\n\n\n\n\n\n\nNow we need to set some environment variables to save typing lengthy\nfile paths over and over. Copy and paste the following commands into\nyour terminal.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\nexport APP_ROOT=/home/trainee/snv/Applications\n\nexport IGVTOOLS_PATH=$PATH:$APP_ROOT/igvtools/\n\nexport PICARD_JAR=$APP_ROOT/picard/picard.jar\n\nexport SNPEFF_HOME=$APP_ROOT/snpeff/\n\nexport GATK_JAR=$APP_ROOT/gatk/GenomeAnalysisTK.jar\n\nexport BVATOOLS_JAR=$APP_ROOT/bvatools/bvatools-1.6-full.jar\n\nexport TRIMMOMATIC_JAR=$APP_ROOT/trimmomatic/trimmomatic-0.33.jar\n\nexport STRELKA_HOME=$APP_ROOT/strelka/\n\nexport MUTECT_JAR=$APP_ROOT/mutect/muTect-1.1.4.jar\n\nexport VARSCAN_JAR=$APP_ROOT/varscan/VarScan.v2.4.1.jar\n\nexport REF=/home/trainee/snv/ref\n\nexport SNV_BASE=/home/trainee/snv\n\nexport JAVA7=/usr/lib/jvm/java-7-openjdk-amd64/jre/bin/java\n\nexport IGV=$APP_ROOT/igv/igv.sh\n\n\n\n\n\n\nOpen the Terminal and go to the base \n/\nhome/trainee/snv working\ndirectory:\n\n\n1\ncd $SNV_BASE\n\n\n\n\n\n\nAll commands entered into the terminal for this tutorial should be from\nwithin the \n/\nhome/trainee/snv\n directory.\n\n\nCheck that the \nalignment\n directory contains the above-mentioned files\nby typing:\n\n\n1\nls alignment\n\n\n\n\n\n\nBAM Files\n\n\nLet\u2019s spend some time to explore bam files.\n\n\nStep 1: Exploring BAM files\n\n\n1\nsamtools view alignment/normal/normal.sorted.bam | head -n4\n\n\n\n\n\n\nHere you have examples of alignment results. A full description of the\nflags can be found in the SAM specification\n\nhttp://samtools.sourceforge.net/SAM1.pdf\n\n\nAnother useful bit of information in the SAM is the CIGAR string. It\u2019s\nthe 6\nth\n column in the file.\n\n\nThis column explains how the alignment was achieved.\n\n\nM == base aligns \nbut doesn\u2019t have to be a match.\n A SNP will have an\nM even if it disagrees with the reference.\\\nI == Insertion\\\nD == Deletion\\\nS == soft-clips. These are handy to find un removed adapters, viral\ninsertions, etc.\n\n\nAn in depth explanation of the CIGAR can be found\n\nhttp://genome.sph.umich.edu/wiki/SAM\n\n\nThe exact details of the cigar string can be found in the SAM spec as\nwell.\n\n\nWe won\u2019t go into too much detail at this point since we want to\nconcentrate on cancer specific issues now.\n\n\nNow, you can try using picards explain flag site to understand what is\ngoing on with your reads\n\nhttp://broadinstitute.github.io/picard/explain-flags.html\n\n\nThere are 3 unique flags, what do they mean? The flag is the second\ncolumn.\n\n\n129:\n\\\nread paired\\\nsecond in pair\\\n\\\n\n113:\n\\\nread paired\\\nread reverse strand\\\nmate reverse strand\\\nfirst in pair\\\n\\\n\n161:\n\\\nread paired\\\nmate reverse strand\\\nsecond in pair\\\n\n\nThere are lots of possible different flags, let\u2019s look at a few more\n\n\n1\nsamtools view alignment/normal/normal.sorted.bam | head -n100\n\n\n\n\n\n\nLet\u2019s take the last one, which looks properly paired and find it\u2019s mate\npair.\\\nHINT: Instead of using \u2019head\u2019 what unix command could we pipe the output\nto? HINT2: Once we\u2019ve found both reads to stop the command running we\ntype CTRL-C\n\n\n1\nsamtools view alignment/normal/normal.sorted.bam | grep HWI-ST478_0133:4:2205:14675:32513\n\n\n\n\n\n\nUsing the cigar string, what can we tell about the alignment of the mate\npair?\n\n\nThe mate pair has a less convincing alignment with two insertions and\nsoft clipping reported.\n\n\nHow might the alignment information from the original read be used by\nthe aligner?\n\n\nEven though the alignment of the mate pair is questionable the presence\nof it\u2019s properly paired mate helps the aligner in deciding where to put\nthe less-certain read.\n\n\nYou can use samtools to filter reads as well.\n\n\nHow many reads mapped and unmapped were there?\\\nHINT: Look at the samtools view help menu by typing samtools view\nwithout any arguments\n\n\n1\nsamtools view -c -f4 alignment/normal/normal.sorted.bam\n\n\n\n\n\n\n77229\n\n\n1\nsamtools view -c -F4 alignment/normal/normal.sorted.bam\n\n\n\n\n\n\n22972373\n\n\nStep 2: Pre-processing: Indel Realignment\n\n\nThe first step for this is to realign around indels and snp dense\nregions.\\\nThe Genome Analysis toolkit has a tool for this called IndelRealigner.\\\nIt basically runs in 2 steps:\\\n1. Find the targets\\\n2. Realign them\\\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n$JAVA7\n -Xmx2G  -jar \n${\nGATK_JAR\n}\n \\\n  -T RealignerTargetCreator \\\n  -R \n${\nREF\n}\n/human_g1k_v37.fasta \\\n  -o alignment/normal/realign.intervals \\\n  -I alignment/normal/normal.sorted.bam \\\n  -I alignment/tumour/tumour.sorted.bam \\\n  -L \n${\nREF\n}\n/human_g1k_v37.intervals\n\n\n$JAVA7\n -Xmx2G -jar \n${\nGATK_JAR\n}\n \\\n  -T IndelRealigner \\\n  -R \n${\nREF\n}\n/human_g1k_v37.fasta \\\n  -targetIntervals alignment/normal/realign.intervals \\\n  --nWayOut .realigned.bam \\\n  -I alignment/normal/normal.sorted.bam \\\n  -I alignment/tumour/tumour.sorted.bam \\\n  -L \n${\nREF\n}\n/human_g1k_v37.intervals\n\n  mv normal.sorted.realigned.ba* alignment/normal/\n  mv tumour.sorted.realigned.ba* alignment/tumour/\n\n\n\n\n\n\nExplanation of parameters\n\n\n-I\n:   BAM file(s)\n\n\n-T\n:   GATK algorithm to run\n\n\n-R\n:   the reference genome used for mapping (b37 from GATK here)\n\n\n-jar\n:   Path to GATK jar file\n\n\n-L\n:   Genomic intervals to operate on\n\n\nWhy did we use both normal and tumor together?\n\n\nBecause if a region needs realignment, maybe one of the sample in the\npair has less reads or was excluded from the target creation.\\\nThis makes sure the normal and tumor are all in-sync for the somatic\ncalling step.\n\n\nHow many regions did it think needed cleaning ?\n\n\n1\nwc -l alignment/normal/realign.intervals -\n 27300\n\n\n\n\n\n\nIndel Realigner also makes sure the called deletions are left aligned\nwhen there is a microsatellite or homopolymer.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\nThis\nATCGAAAA-TCG\ninto\nATCG-AAAATCG\n\nor\nATCGATATATATA--TCG\ninto\nATCG--ATATATATATCG\n\n\n\n\n\n\nWhy it is important ?\n\n\nThis makes it easier for down stream analysis tools\n\n\nFor NGS analysis, the convention is to left align indels.\n\n\nThis is only really needed when calling variants with legacy locus-based\ntools such as samtools or GATK UnifiedGenotyper. Otherwise you will have\nworse performance and accuracy.\n\n\nWith more sophisticated tools (like GATK HaplotypeCaller) that involve\nreconstructing haplotypes (eg through reassembly), the problem of\nmultiple valid representations is handled internally and does not need\nto be corrected explicitly.\n\n\nStep 3: Pre-processing: Fixmates\n\n\nSome read entries don\u2019t have their mate information written properly.\\\nWe use Picard to do this:\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n$JAVA7\n -Xmx2G -jar \n${\nPICARD_JAR\n}\n FixMateInformation \\\n  VALIDATION_STRINGENCY=SILENT \\\n  CREATE_INDEX=true \\\n  SORT_ORDER=coordinate \\\n  MAX_RECORDS_IN_RAM=500000 \\\n  INPUT=alignment/normal/normal.sorted.realigned.bam \\\n  OUTPUT=alignment/normal/normal.matefixed.bam\n\n\n$JAVA7\n -Xmx2G -jar \n${\nPICARD_JAR\n}\n FixMateInformation \\\n  VALIDATION_STRINGENCY=SILENT \\\n  CREATE_INDEX=true \\\n  SORT_ORDER=coordinate \\\n  MAX_RECORDS_IN_RAM=500000 \\\n  INPUT=alignment/tumour/tumour.sorted.realigned.bam \\\n  OUTPUT=alignment/tumour/tumour.matefixed.bam\n\n\n\n\n\n\nStep 4: Pre-processing: Mark Duplicates\n\n\nWhat are duplicate reads ?\n\n\nDifferent read pairs representing the same initial DNA fragment.\n\n\nWhat are they caused by ?\n\n\nPCR reactions (PCR duplicates)\\\nSome clusters that are thought of being separate in the flowcell but are\nthe same (optical duplicates)\n\n\nWhat are the ways to detect them ?\n\n\nPicard and samtools uses the alignment positions:\\\n- Both 5\u2019 ends of both reads need to have the same positions.\\\n- Each reads have to be on the same strand as well.\\\n\\\nAnother method is to use a kmer approach:\\\n- take a part of both ends of the fragment\\\n- build a hash table\\\n- count the similar hits\\\n\\\nBrute force, compare all the sequences.\n\n\nHere we will use picards approach:\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n$JAVA7\n -Xmx2G -jar \n${\nPICARD_JAR\n}\n MarkDuplicates \\\n  REMOVE_DUPLICATES=false \\\n  CREATE_MD5_FILE=true \\\n  VALIDATION_STRINGENCY=SILENT \\\n  CREATE_INDEX=true \\\n  INPUT=alignment/normal/normal.matefixed.bam \\\n  OUTPUT=alignment/normal/normal.sorted.dup.bam \\\n  METRICS_FILE=alignment/normal/normal.sorted.dup.metrics\n\n\n$JAVA7\n -Xmx2G -jar \n${\nPICARD_JAR\n}\n MarkDuplicates \\\n  REMOVE_DUPLICATES=false \\\n  CREATE_MD5_FILE=true \\\n  VALIDATION_STRINGENCY=SILENT \\\n  CREATE_INDEX=true \\\n  INPUT=alignment/tumour/tumour.matefixed.bam \\\n  OUTPUT=alignment/tumour/tumour.sorted.dup.bam \\\n  METRICS_FILE=alignment/tumour/tumour.sorted.dup.metrics\n\n\n\n\n\n\nWe can look in the metrics output to see what happened.\n\n\n1\nless alignment/normal/normal.sorted.dup.metrics\n\n\n\n\n\n\nWhat percent of reads are duplicates?\n\n\n0.046996%\n\n\nOften, we have multiple libraries and when this occurs separate measures\nare calculated for each library. Why is this important to do not combine\neverything ?\n\n\nEach library represents a set of different DNA fragments.\n\n\nEach library involves different PCR reactions\n\n\nSo PCR duplicates can not occur between fragment of two different\nlibraries.\n\n\nBut similar fragment could be found between libraries when the coverage\nis high.\n\n\nStep 4: Pre-processing: Base Quality Recalibration\n\n\nWhy do we need to recalibrate base quality scores ?\n\n\nThe vendors tend to inflate the values of the bases in the reads. The\nrecalibration tries to lower the scores of some biased motifs for some\ntechnologies.\n\n\nIt runs in 2 steps,\\\n1- Build covariates based on context and known snp sites\\\n2- Correct the reads based on these metrics\n\n\nGATK BaseRecalibrator:\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\nfor\n \ni\n \nin\n \nnormal\n \ntumour\n\n\ndo\n\n  \n$JAVA7\n -\nXmx2G\n -\njar\n ${\nGATK_JAR\n} \\\n    -\nT\n \nBaseRecalibrator\n \\\n    -\nnct\n \n2\n \\\n    -\nR\n ${\nREF\n}/\nhuman_g1k_v37\n.\nfasta\n \\\n    -\nknownSites\n ${\nREF\n}/\ndbSnp-138_chr4\n.\nvcf\n \\\n    -\nL\n \n4\n:\n1\n-\n10000000\n \\\n    -\no\n \nalignment\n/${\ni\n}/${\ni\n}.\nsorted\n.\ndup\n.\nrecalibration_report\n.\ngrp\n \\\n    -\nI\n \nalignment\n/${\ni\n}/${\ni\n}.\nsorted\n.\ndup\n.\nbam\n\n\n    \n$JAVA7\n -\nXmx2G\n -\njar\n ${\nGATK_JAR\n} \\\n      -\nT\n \nPrintReads\n \\\n      -\nnct\n \n2\n \\\n      -\nR\n ${\nREF\n}/\nhuman_g1k_v37\n.\nfasta\n \\\n      -\nBQSR\n \nalignment\n/${\ni\n}/${\ni\n}.\nsorted\n.\ndup\n.\nrecalibration_report\n.\ngrp\n \\\n      -\no\n \nalignment\n/${\ni\n}/${\ni\n}.\nsorted\n.\ndup\n.\nrecal\n.\nbam\n \\\n      -\nI\n \nalignment\n/${\ni\n}/${\ni\n}.\nsorted\n.\ndup\n.\nbam\n\n\ndone\n\n\n\n\n\n\n\nBAM QC\n\n\nOnce your whole bam is generated, it\u2019s always a good thing to check the\ndata again to see if everything makes sense.\n\n\nStep 1: BAM QC: Compute Coverage\n\n\nIf you have data from a capture kit, you should see how well your\ntargets worked. Both GATK and BVATools have depth of coverage tools.\\\nHere we\u2019ll use the GATK one\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\nfor\n \ni\n \nin\n \nnormal\n \ntumour\n\n\ndo\n\n  \n$JAVA7\n  -\nXmx2G\n -\njar\n ${\nGATK_JAR\n} \\\n    -\nT\n \nDepthOfCoverage\n \\\n    --\nomitDepthOutputAtEachBase\n \\\n    --\nsummaryCoverageThreshold\n \n10\n \\\n    --\nsummaryCoverageThreshold\n \n25\n \\\n    --\nsummaryCoverageThreshold\n \n50\n \\\n    --\nsummaryCoverageThreshold\n \n100\n \\\n    --\nstart\n \n1\n --\nstop\n \n500\n --\nnBins\n \n499\n -\ndt\n \nNONE\n \\\n    -\nR\n ${\nREF\n}/\nhuman_g1k_v37\n.\nfasta\n \\\n    -\no\n \nalignment\n/${\ni\n}/${\ni\n}.\nsorted\n.\ndup\n.\nrecal\n.\ncoverage\n \\\n    -\nI\n \nalignment\n/${\ni\n}/${\ni\n}.\nsorted\n.\ndup\n.\nrecal\n.\nbam\n \\\n    -\nL\n \n4\n:\n1\n-\n10000000\n\n\ndone\n\n\n\n\n\n\n\nExplanation of parameters\n\n\nomitBaseOutput\n:   Do not output depth of coverage at each base\n\n\nsummaryCoverageThreshol\n:   Coverage threshold (in percent) for summarizing statistics\n\n\ndt\n:   down sampling\n\n\nL\n:   Genomic intervals to operate on\n\n\nCoverage is expected to be \u00a025x in these project Look at the coverage:\n\n\n1\n2\nless -S alignment/normal/normal.sorted.dup.recal.coverage.sample_interval_summary\nless -S alignment/tumour/tumour.sorted.dup.recal.coverage.sample_interval_summary\n\n\n\n\n\n\nIs the coverage fit with the expectation ?\n\n\nYes the mean coverage of the region is 25x:\n\n\nsummaryCoverageThreshold is a useful function to see if your coverage is\nuniform.\n\n\nAnother way is to compare the mean to the median. If both are quite\ndifferent that means something is wrong in your coverage.\n\n\nStep 2: BAM QC: Insert Size\n\n\nIt corresponds to the size of DNA fragments sequenced.\n\n\nDifferent from the gap size (= distance between reads) !\n\n\nThese metrics are computed using Picard:\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\nfor\n \ni\n \nin\n \nnormal\n \ntumour\n\n\ndo\n\n  \n$JAVA7\n -\nXmx2G\n -\njar\n ${\nPICARD_JAR\n} \nCollectInsertSizeMetrics\n \\\n    \nVALIDATION_STRINGENCY\n=\nSILENT\n \\\n    \nREFERENCE_SEQUENCE\n=${\nREF\n}/\nhuman_g1k_v37\n.\nfasta\n \\\n    \nINPUT\n=\nalignment\n/${\ni\n}/${\ni\n}.\nsorted\n.\ndup\n.\nrecal\n.\nbam\n \\\n    \nOUTPUT\n=\nalignment\n/${\ni\n}/${\ni\n}.\nsorted\n.\ndup\n.\nrecal\n.\nmetric\n.\ninsertSize\n.\ntsv\n \\\n    \nHISTOGRAM_FILE\n=\nalignment\n/${\ni\n}/${\ni\n}.\nsorted\n.\ndup\n.\nrecal\n.\nmetric\n.\ninsertSize\n.\nhisto\n.\npdf\n \\\n    \nMETRIC_ACCUMULATION_LEVEL\n=\nLIBRARY\n\n\ndone\n\n\n\n\n\n\n\nlook at the output\n\n\n1\n2\nless -S alignment/normal/normal.sorted.dup.recal.metric.insertSize.tsv\nless -S alignment/tumour/tumour.sorted.dup.recal.metric.insertSize.tsv\n\n\n\n\n\n\nHow do the two libraries compares?\n\n\nThe tumour sample has a larger median insert size than the normal sample\n(405 vs 329)\n\n\nStep 3: BAM QC: Alignment metrics\n\n\nIt tells you if your sample and you reference fit together\n\n\nFor the alignment metrics, samtools flagstat is very fast but with\nbwa-mem since some reads get broken into pieces, the numbers are a bit\nconfusing.\n\n\nWe prefer the Picard way of computing metrics:\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\nfor\n \ni\n \nin\n \nnormal\n \ntumour\n\n\ndo\n\n  \n$JAVA7\n -\nXmx2G\n -\njar\n ${\nPICARD_JAR\n} \nCollectAlignmentSummaryMetrics\n \\\n    \nVALIDATION_STRINGENCY\n=\nSILENT\n \\\n    \nREFERENCE_SEQUENCE\n=${\nREF\n}/\nhuman_g1k_v37\n.\nfasta\n \\\n    \nINPUT\n=\nalignment\n/${\ni\n}/${\ni\n}.\nsorted\n.\ndup\n.\nrecal\n.\nbam\n \\\n    \nOUTPUT\n=\nalignment\n/${\ni\n}/${\ni\n}.\nsorted\n.\ndup\n.\nrecal\n.\nmetric\n.\nalignment\n.\ntsv\n \\\n    \nMETRIC_ACCUMULATION_LEVEL\n=\nLIBRARY\n\n\ndone\n\n\n\n\n\n\n\nexplore the results\n\n\n1\n2\nless -S alignment/normal/normal.sorted.dup.recal.metric.alignment.tsv\nless -S alignment/tumour/tumour.sorted.dup.recal.metric.alignment.tsv\n\n\n\n\n\n\nDo you think the sample and the reference genome fit together ?\n\n\nYes, 99% of the reads have been aligned\\\nUsually, we consider:\\\n- A good alignment if > 85%\\\n- Reference assembly issues if [60-85]%\\\n- Probably a mismatch between sample and ref if \\\n 60 %\n\n\nVariant Calling\n\n\nMost of SNV caller use either a Baysian, a threshold or a t-test\napproach to do the calling\n\n\nHere we will try 3 variant callers.\\\n- Varscan 2\\\n- MuTecT\\\n- Strelka\n\n\nOther candidates\\\n- Virmid\\\n- Somatic sniper\n\n\nmany, MANY others can be found here: \nhttps://www.biostars.org/p/19104/\n\n\nIn our case, let\u2019s create a new work directory to start with (from base\ndirectory):\n\n\n1\n2\ncd $SNV_BASE\nmkdir variant_calling\n\n\n\n\n\n\nvarscan 2 VarScan calls somatic variants (SNPs and indels) using a\nheuristic method and a statistical test based on the number of aligned\nreads supporting each allele.\n\n\nVarscan somatic caller expects both a normal and a tumour file in\nSAMtools pileup format from sequence alignments in binary alignment/map\n(BAM) format. To build a pileup file, you will need:\n\n\n\n\n\n\nA SAM/BAM file (\nmyData.bam\n) that has been sorted using the sort\ncommand of SAMtools. - The reference sequence (\nreference.fasta\n) to\nwhich reads were aligned, in FASTA format. - The SAMtools software\npackage.\n\n\nfor i in normal tumour\ndo\nsamtools mpileup -L 1000 -B -q 1 \\\n  -f ${REF}/human_g1k_v37.fasta \\\n  -r 4:1-10000000 \\\n  alignment/${i}/${i}.sorted.dup.recal.bam \\\n\n\n\n\nvariant_calling/${i}.mpileup\ndone\n\n\n\n\n$JAVA7 -Xmx2G -jar ${VARSCAN_JAR} \\\nsomatic variant_calling/normal.mpileup \\\nvariant_calling/tumour.mpileup \\\nvariant_calling/varscan \\\n\noutput-vcf 1 \\\n\nstrand-filter 1 \\\n\nsomatic-p-value 0.001\n\n\n\n\n\n\nNotes on samtools arguments\n\n\n-L\n:   = max per-sample depth for INDEL calling [1000] ;\n\n\n-B\n:   = disable BAQ (per-Base Alignment Quality) ;\n\n\n-q\n:   = skip alignments with mapQ smaller than 1 ;\n\n\n-g\n:   = generate genotype likelihoods in BCF format\n\n\nNotes on bcftools arguments\n\n\n-v\n:   = output potential variant sites only\n\n\n-c\n:   = SNP calling (force \u2013e : likelihood based analyses)\n\n\n-g\n:   = call genotypes at variant sites\n\n\nNow let\u2019s try a different variant caller, MuTect\\\n\n\nNote MuTecT only works with Java 6, 7 will give you an error\\\nif you get \nComparison method violates its general contract!\\\nyou used java 7\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\njava -Xmx2G -jar \n${\nMUTECT_JAR\n}\n \\\n  -T MuTect \\\n  -R \n${\nREF\n}\n/human_g1k_v37.fasta \\\n  -dt NONE -baq OFF --validation_strictness LENIENT -nt 2 \\\n  --dbsnp \n${\nREF\n}\n/dbSnp-138_chr4.vcf \\\n  --input_file:normal alignment/normal/normal.sorted.dup.recal.bam \\\n  --input_file:tumor alignment/tumour/tumour.sorted.dup.recal.bam \\\n  --out variant_calling/mutect.call_stats.txt \\\n  --coverage_file variant_calling/mutect.wig.txt \\\n  -pow variant_calling/mutect.power \\\n  -vcf variant_calling/mutect.vcf \\\n  -L 4:1-10000000\n\n\n\n\n\n\nAnd finally let\u2019s try Illumina\u2019s Strelka\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\ncp \n${\nSTRELKA_HOME\n}\n/etc/strelka_config_bwa_default.ini .\n\nsed \ns/isSkipDepthFilters =.*/isSkipDepthFilters = 1/g\n -i strelka_config_bwa_default.ini\n\n\n${\nSTRELKA_HOME\n}\n/bin/configureStrelkaWorkflow.pl \\\n  --normal=alignment/normal/normal.sorted.dup.recal.bam \\\n  --tumor=alignment/tumour/tumour.sorted.dup.recal.bam \\\n  --ref=\n${\nREF\n}\n/human_g1k_v37.fasta \\\n  --config=\n${\nSNV_BASE\n}\n/strelka_config_bwa_default.ini \\\n  --output-dir=variant_calling/strelka/\n\n  cd variant_calling/strelka/\n  make -j2\n  cd ../..\n\n  cp variant_calling/strelka/results/passed.somatic.snvs.vcf variant_calling/strelka.vcf\n\n\n\n\n\n\nNow we have variants from all three methods. Let\u2019s compress and index\nthe vcfs for future visualisation.\n\n\n1\nfor i in variant_calling/*.vcf;do bgzip -c $i \n $i.gz ; tabix -p vcf $i.gz;done\n\n\n\n\n\n\nLet\u2019s look at a compressed vcf.\n\n\n1\nzless -S variant_calling/varscan.snp.vcf.gz\n\n\n\n\n\n\nDetails on the spec can be found here:\n\nhttp://vcftools.sourceforge.net/specs.html\n\n\nFields vary from caller to caller.\n\n\nSome values are are almost always there:\\\n- The ref vs alt alleles,\\\n- variant quality (QUAL column)\\\n- The per-sample genotype (GT) values.\n\n\nNote on vcf fields\n\n\nDP\n:   = \nRaw read depth\n\n\nGT\n:   = \nGenotype\n\n\nPL\n:   = \nList of Phred-scaled genotype likelihoods\n (min is better)\n\n\nDP\n:   = \n# high-quality bases\n\n\nSP\n:   = \nPhred-scaled strand bias P-value\n\n\nGQ\n:   = \nGenotype Quality\n\n\nLooking at the three vcf files, how can we detect only somatic variants?\n\n\nsome commands to find somatic variant in the vcf file\n\n\nvarscan\n\n\n1\ngrep SOMATIC variant_calling/varscan.snp.vcf\n\n\n\n\n\n\nMuTecT\n\n\n1\ngrep -v REJECT variant_calling/mutect.vcf | grep -v \n^#\n\n\n\n\n\n\n\nStrelka\n\n\n1\ngrep -v \n^#\n variant_calling/strelka.vcf\n\n\n\n\n\n\nVariant Visualisation\n\n\nThe Integrative Genomics Viewer (IGV) is an efficient visualization tool\nfor interactive exploration of large genome datasets.\n\n\nBefore jumping into IGV, we\u2019ll generate a track IGV can use to plot\ncoverage:\n\n\n1\n2\n3\n4\n5\n6\n7\n8\nfor\n \ni\n \nin\n \nnormal\n \ntumour\n\n\ndo\n\n  \n$JAVA7\n -\njar\n ${\nIGVTOOLS_PATH\n}/\nigvtools\n.\njar\n \ncount\n \\\n    -\nf\n \nmin\n,\nmax\n,\nmean\n \\\n    \nalignment\n/${\ni\n}/${\ni\n}.\nsorted\n.\ndup\n.\nrecal\n.\nbam\n \\\n    \nalignment\n/${\ni\n}/${\ni\n}.\nsorted\n.\ndup\n.\nrecal\n.\nbam\n.\ntdf\n \\\n    \nb37\n\n\ndone\n\n\n\n\n\n\n\nOpen IGV\n\n\n1\n$IGV\n\n\n\n\n\n\nThen: 1. Chose the reference genome corresponding to those use for\nalignment (b37)\\\n2. Load bam files (tumour.sorted.dup.recal.bam and\nnormal.sorted.dup.recal.bam)\\\n3. Load vcf files (from variant_Calling directory)\n\n\nExplore/play with the data:\\\n-find germline variants\\\n-find somatic variants\\\n-Look around\n\n\nVariant Annotation\n\n\nFollowing variant calling, we end up with a VCF file of genomic\ncoordinates with the genotype(s) and quality information for each\nvariant. By itself, this information is not much use to us unless there\nis a specific genomic location we are interested in. Generally, we next\nwant to annotate these variants to determine whether they impact any\ngenes and if so what is their level of impact (e.g. are they causing a\npremature stop codon gain or are they likely less harmful missense\nmutations).\n\n\nThe sections above have dealt with calling somatic variants from the\nfirst 10Mb of chromosome 4. This is important in finding variants that\nare unique to the tumour sample(s) and may have driven both tumour\ngrowth and/or metastasis. An important secondary question is whether the\ngermline genome of the patient contains any variants that may have\ncontributed to the development of the initial tumour through\npredisposing the patient to cancer. These variants \nmay not\n be captured\nby somatic variant analysis as their allele frequency may not change in\nthe tumour genome compared with the normal.\n\n\nFor this section, we will use \nall\n variants from the first 60Mb of\nchromosome 5 that have been pre-generated using the GATK HaplotypeCaller\nvariant caller on both the normal and tumour genomes. The output of this\nwas GVCF files which were fed into GATK GenotypeGVCFs to produce a\nmerged VCF file. We will use this pre-generated file as we are primarily\ninterested in the annotation of variants rather than their generation.\nThe annotation method we will use is called \nVariant Effect Predictor\n\nor VEP for short and is available from Ensembl here:\n\nhttp://ensembl.org/info/docs/tools/vep/index.html\n.\n\n\nOur pre-generated VCF file is located in the \nvariants\n folder. Let\u2019s\nhave a quick look at the variants:\n\n\n1\nzless variants/HC.chr5.60Mb.vcf.gz\n\n\n\n\n\n\nNotice how there are two genotype blocks at the end of each line for the\nnormal (Blood) and tumour (liverMets) samples.\n\n\nLet\u2019s now run VEP on this VCF file to annotate each variant with its\nimpact(s) on the genome.\n\n\n1\nperl Applications/ensembl-tools/scripts/variant_effect_predictor/variant_effect_predictor.pl -i variants/HC.chr5.60Mb.vcf.gz --vcf -o variants/HC.chr5.60Mb.vep.vcf --stats_file variants/HC.chr5.60Mb.vep.html --format vcf --offline -fork 4 --fasta ref/human_g1k_v37.fasta --fields Consequence,Codons,Amino_acids,Gene,SYMBOL,Feature,EXON,PolyPhen,SIFT,Protein_position,BIOTYPE\n\n\n\n\n\n\nVEP will take approximately 10 minutes to run and once it is finished\nyou will have a new VCF file with all of the information in the input\nfile but with added annotations in the INFO block. VEP also produces an\nHTML report summarising the distribution and impact of variants\nidentified.\n\n\nOnce VEP is done running, let\u2019s first look at the HTML report it\nproduced with the following command:\n\n\n1\nfirefox variants/HC.chr5.60Mb.vep.html\n\n\n\n\n\n\nThis report shows information on the VEP run, the number of variants,\nthe classes of variants detected, the variant consequences and the\ndistributions of variants through the genome. Close Firefox to resume\nthe terminal prompt.\n\n\nNow let\u2019s look at the variant annotations that VEP has added to the VCF\nfile by focussing on a single variant. Let\u2019s fetch the same variant from\nthe original VCF file and the annotated VCF file to see what has been\nchanged.\n\n\n1\n2\nzcat variants/HC.chr5.60Mb.vcf.gz | grep \n5\\s174106\\s\n\ngrep \n5\\s174106\\s\n variants/HC.chr5.60Mb.vep.vcf\n\n\n\n\n\n\nThese commands give us the original variant:\n\n\n1\n5   174106  .   G   A   225.44  .   AC=2;AF=0.500;AN=4;BaseQRankSum=1.22;ClippingRankSum=0.811;DP=21;FS=0.000;GQ_MEAN=127.00;GQ_STDDEV=62.23;MLEAC=2;MLEAF=0.500;MQ=60.00;MQ0=0;MQRankSum=0.322;NCC=0;QD=10.74;ReadPosRankSum=0.377;SOR=0.446   GT:AD:DP:GQ:PL  0/1:7,6:13:99:171,0,208 0/1:5,3:8:83:83,0,145\n\n\n\n\n\n\nand the same variant annotated is:\n\n\n1\n5   174106  .   G   A   225.44  .   AC=2;AF=0.500;AN=4;BaseQRankSum=1.22;ClippingRankSum=0.811;DP=21;FS=0.000;GQ_MEAN=127.00;GQ_STDDEV=62.23;MLEAC=2;MLEAF=0.500;MQ=60.00;MQ0=0;MQRankSum=0.322;NCC=0;QD=10.74;ReadPosRankSum=0.377;SOR=0.446;CSQ=missense_variant|cGg/cAg|R/Q|ENSG00000153404|PLEKHG4B|ENST00000283426|16/18|||1076|protein_coding,non_coding_transcript_exon_variant\nnon_coding_transcript_variant|||ENSG00000153404|PLEKHG4B|ENST00000504041|5/8||||retained_intron  GT:AD:DP:GQ:PL  0/1:7,6:13:99:171,0,208 0/1:5,3:8:83:83,0,145\n\n\n\n\n\n\nYou can see that VEP has added:\n\n\n1\nCSQ=missense_variant|cGg/cAg|R/Q|ENSG00000153404|PLEKHG4B|ENST00000283426|16/18|||1076|protein_coding,non_coding_transcript_exon_variant\nnon_coding_transcript_variant|||ENSG00000153404|PLEKHG4B|ENST00000504041|5/8||||retained_intron\n\n\n\n\n\n\nThis is further composed of two annotations for this variant:\n\n\n1\nmissense_variant|cGg/cAg|R/Q|ENSG00000153404|PLEKHG4B|ENST00000283426|16/18|||1076|protein_coding\n\n\n\n\n\n\nand\n\n\n1\nnon_coding_transcript_exon_variant\nnon_coding_transcript_variant|||ENSG00000153404|PLEKHG4B|ENST00000504041|5/8||||retained_intron\n\n\n\n\n\n\nThe first of these is saying that this variant is a missense variant in\nthe gene PLEKHG4B for the transcript ENST00000283426 and the second that\nit is also a non_coding_transcript_exon_variant in the transcript\nENST00000504041.\n\n\nVariant Filtration\n\n\nWe now have a VCF file where each variant has been annotated with one or\nmore impacts for one or more genes. In a typical whole cancer genome,\nyou will have about 4-5 million variants, and therefore rows, in a VCF\nfile which takes up gigabytes of space. In our small example, we have\njust 100,000 variants which is already too large to make any kind of\nmeaningful sense out of by just opening up the VCF file in a text\neditor. We need a solution that allows us to perform intelligent queries\non our variants to search this mass of noise for the signal we are\ninterested in.\n\n\nLuckily, such a free tool exists and is called GEMINI. GEMINI takes as\nan input your annotated VCF file and creates a database file which it\ncan then query using Structured Query Language (SQL) commands. Not only\ndoes GEMINI make your variants easily searchable, it also brings in many\nexternal annotations to add more information about your variants (such\nas their frequencies in international databases).\n\n\nTo get started with GEMINI, let\u2019s make a database out of our annotated\nVCF file.\n\n\n1\ngemini load -v variants/HC.chr5.60Mb.vep.vcf --cores 4 --skip-gerp-bp --skip-cadd -t VEP variants/HC.chr5.60Mb.vep.vcf.db\n\n\n\n\n\n\nThis will take approximately 10 minutes. You will see a few errors due\nto multiallelic sites, normally these sites are decomposed and\nnormalized before creating the GEMINI database but this is outside the\nscope of this workshop.\n\n\nOnce the database has been created let\u2019s run a basic query to see what\nkind of information we get out of GEMINI.\n\n\n1\ngemini query -q \nSELECT *, (gts).(*), (gt_types).(*), (gt_depths).(*), (gt_ref_depths).(*), (gt_alt_depths).(*), (gt_quals).(*) FROM variants LIMIT 10;\n --header variants/HC.chr5.60Mb.vep.vcf.db\n\n\n\n\n\n\nThis will output a bunch of ordered information for your query to the\ncommand line, this is usually saved to a TSV file and opened in a\nspreadsheet as we will do for the next query. In the mean time, let\u2019s\ndissect this query to understand the syntax we need to use to filter our\nvariants. First, we have a SELECT statement which simply specifies that\nwe want to select data from the database. The following comma-separated\nvalues are the columns that we want to output from the database, in this\ncase we are selecting all columns with the star character and then all\nsub-columns for each sample with the other values. Then, we have a \nFROM\nvariants\n statement which is specifying the table within the database\nthat we want to fetch information from. Finally, the \nLIMIT 10\n\nstatement specifies that no more than 10 rows should be returned. In\nsummary then, we are asking for all columns for 10 rows from the table\n\nvariants\n. If you haven\u2019t used SQL before don\u2019t worry, the GEMINI\nwebsite is very helpful and provides many examples for how to query your\ndatabase.\n\n\nLet\u2019s now perform a more interesting query to find variants that have a\nmedium or high impact on a gene and are rare or not present in existing\ninternational allele frequency databases. We will save the output of\nthis query to a file and open it up in a spreadsheet.\n\n\n1\ngemini query -q \nSELECT *, (gts).(*), (gt_types).(*), (gt_depths).(*), (gt_ref_depths).(*), (gt_alt_depths).(*), (gt_quals).(*) FROM variants WHERE (impact_severity = \nHIGH\n OR impact_severity = \nMED\n) AND (aaf_1kg_all \n 0.01 OR aaf_1kg_all is null) AND (aaf_esp_all \n 0.01 OR aaf_esp_all is null) AND (aaf_exac_all \n 0.01 OR aaf_exac_all is null);\n --header variants/HC.chr5.60Mb.vep.vcf.db \n variants/gemini-result.tsv\n\n\n\n\n\n\nNotice that we have added a WHERE statement which restricts the rows\nthat are returned based on values that we specify for specific columns.\nHere, we are asking to return variants where their impact on the gene\n(impact_severity column) is medium or high and the allele frequency in\n1000Genomes, ESP and EXaC is less than 1% or the variant is not present\nin any of these databases.\n\n\nNow let\u2019s open the result in a spreadsheet to look at the annotations:\n\n\n1\n2\nlibreoffice --calc variants/gemini-result.tsv\nTick the \nTab\n under \nSeparated by\n on the dialog window that comes up.\n\n\n\n\n\n\nYou can see that the first 14 columns contain information on the variant\nincluding its location, ref, alt, dbSNP ID, quality and type. Slowly\nscroll to the right and look at the columns of data that are provided.\nMost importantly, column BD includes the gene this variant impacts, BN\nthe impact itself and BP the impact severity. Scroll towards the end of\nthe spreadsheet until you get to columns ED and EE, these contain the\ngenotype for each of the samples. Columns EH and EI contain the total\ndepth for each variant in each sample and the 4 following columns\ncontain the reference and alternate depths for each sample. Finally,\ncolumns EN and EO contain the genotype qualities (from the GQ field in\nthe VCF) for each sample. As you scroll back and forth through this\nspreadsheet, you will see that GEMINI brings in information from a\nvariety of sources including: OMIM, ClinVar, GERP, PolyPhen 2, SIFT,\nESP, 1000 Genomes, ExAC, ENCODE, CADD and more! We are only looking at a\nsmall number of variants from the start of a chromosome so not many of\nthese annotations will be present but in a full genome database they are\nincredibly useful.\n\n\nGEMINI allows you to filter your variants based on any column that you\nsee in this results file. For example, you may want all variants in a\nspecific gene, in which case you would simply add \nWHERE gene = \u2019BRCA1\u2019\n\nto your query. For complete documentation with many examples of queries,\nsee the GEMINI documentation here: \nhttp://gemini.readthedocs.org\n.\n\n\nReferences\n\n\n\n\n\n\nPaila U, Chapman BA, Kirchner R and Quinlan AR. \nGEMINI: Integrative\n    Exploration of Genetic Variation and Genome Annotations\n. PLoS\n    Comput Biol, 2013, 9(7): e1003153. doi:10.1371/journal.pcbi.1003153\n\n\n\n\n\n\nMcLaren W, Pritchard B, Rios D, Chen Y, Flicek P and Cunningham F.\n    \nDeriving the consequences of genomic variants with the Ensembl API\n    and SNP Effect Predictor\n. Bioinformatics, 2010, 26(16):2069-70,\n    doi:10.1093/bioinformatics/btq330\n\n\n\n\n\n\nAcknowledgements\n\n\nThis tutorial is an adaptation of the one created by Louis letourneau\n\nhttps://github.com/lletourn/Workshops/tree/ebiCancerWorkshop201407doc/01-SNVCalling.md\n.\nI would like to thank and acknowledge Louis for this help and for\nsharing his material. The format of the tutorial has been inspired from\nMar Gonzalez Porta. I also want to acknowledge Joel Fillon, Louis\nLetrouneau (again), Francois Lefebvre, Maxime Caron and Guillaume\nBourque for the help in building these pipelines and working with all\nthe various datasets.", 
            "title": "Single Nucleotide Variant Calling and Annotation"
        }, 
        {
            "location": "/modules/cancer-module-snv/snv/#key-learning-outcomes", 
            "text": "After completing this practical the trainee should be able to:    Prepare raw BAM alignments for variant detection    Perform QC measures on BAM files    Understand and perform simple variant detection on paired NGS data    Add annotation information to raw variant calls    Visualise variant calls using IGV", 
            "title": "Key Learning Outcomes"
        }, 
        {
            "location": "/modules/cancer-module-snv/snv/#resources-youll-be-using", 
            "text": "", 
            "title": "Resources You\u2019ll be Using"
        }, 
        {
            "location": "/modules/cancer-module-snv/snv/#tools-used", 
            "text": "SAMTools:  http://sourceforge.net/projects/samtools/  IGV:  http://www.broadinstitute.org/igv/  Genome Analysis Toolkit:  http://www.broadinstitute.org/gatk/  Picard:  http://picard.sourceforge.net/  MuTect:  http://www.broadinstitute.org/cancer/cga/mutect/  Strelka:  https://sites.google.com/site/strelkasomaticvariantcaller/  VarScan2:  https://github.com/dkoboldt/varscan/  Variant Effect Predictor:  http://www.ensembl.org/info/docs/tools/vep  GEMINI:  http://gemini.readthedocs.org", 
            "title": "Tools Used"
        }, 
        {
            "location": "/modules/cancer-module-snv/snv/#sources-of-data", 
            "text": "http://sra.dnanexus.com/studies/ERP001071  http://www.ncbi.nlm.nih.gov/pubmed/22194472", 
            "title": "Sources of Data"
        }, 
        {
            "location": "/modules/cancer-module-snv/snv/#introduction", 
            "text": "This talk is based on Introduction to DNA-Seq processing for cancer data\nby Mathieu Bourgey, Ph.D  * https://bitbucket.org/mugqic/muqpic _pipelines*  The goal of this hands-on session is to present the main steps that are\ncommonly used to process and to analyze cancer sequencing data. We will\nfocus only on whole genome data and provide command lines that allow\ndetecting Single Nucleotide Variants (SNV). This workshop will show you\nhow to launch individual steps of a complete DNA-Seq SNV pipeline using\ncancer data.  In the second part of the tutorial we will also be using IGV to\nvisualise and manually inspect candidate variant calls.", 
            "title": "Introduction"
        }, 
        {
            "location": "/modules/cancer-module-snv/snv/#prepare-the-environment", 
            "text": "We will use a dataset derived from whole genome sequencing of a\n33-yr-old lung adenocarcinoma patient, who is a never-smoker and has no\nfamilial cancer history.  The data consists of whole genome sequencing of liver metastatic lung\ncancer (frozen), primary lung cancer (FFPE) and blood tissue of a lung\nadenocarcinoma patient (AK55).  The BAM alignment files are contained in the subdirectory called alignment  and are located in the following subdirectories:  normal/normal.sorted.bam  and  normal/normal.sorted.bam.bai \n:   \\  tumor/tumor.sorted.bam  and  tumor/tumor.sorted.bam.bai \n:   \\  These files are based on subsetting the whole genomes derived from blood\nand liver metastases to the first 10Mb of chromosome 4. This will allow\nour analyses to run in a sufficient time during the workshop, but it\u2019s\nworth being aware that this is less \\  0.5% of the genome which\nhighlights the length of time and resourced required to perform cancer\ngenomics on full genomes!  The initial structure of your folders should look like this:  1\n2\n3\n4\n5 -- alignment/             # bam files\n  -- normal/              # The blood sample directory containing bam files\n  -- tumour/               # The tumour sample directory containing bam files \\\\\n-- ref/                   # Contains reference genome files      \n-- commands.sh            # cheat sheet   Now we need to set some environment variables to save typing lengthy\nfile paths over and over. Copy and paste the following commands into\nyour terminal.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27 export APP_ROOT=/home/trainee/snv/Applications\n\nexport IGVTOOLS_PATH=$PATH:$APP_ROOT/igvtools/\n\nexport PICARD_JAR=$APP_ROOT/picard/picard.jar\n\nexport SNPEFF_HOME=$APP_ROOT/snpeff/\n\nexport GATK_JAR=$APP_ROOT/gatk/GenomeAnalysisTK.jar\n\nexport BVATOOLS_JAR=$APP_ROOT/bvatools/bvatools-1.6-full.jar\n\nexport TRIMMOMATIC_JAR=$APP_ROOT/trimmomatic/trimmomatic-0.33.jar\n\nexport STRELKA_HOME=$APP_ROOT/strelka/\n\nexport MUTECT_JAR=$APP_ROOT/mutect/muTect-1.1.4.jar\n\nexport VARSCAN_JAR=$APP_ROOT/varscan/VarScan.v2.4.1.jar\n\nexport REF=/home/trainee/snv/ref\n\nexport SNV_BASE=/home/trainee/snv\n\nexport JAVA7=/usr/lib/jvm/java-7-openjdk-amd64/jre/bin/java\n\nexport IGV=$APP_ROOT/igv/igv.sh   Open the Terminal and go to the base  / home/trainee/snv working\ndirectory:  1 cd $SNV_BASE   All commands entered into the terminal for this tutorial should be from\nwithin the  / home/trainee/snv  directory.  Check that the  alignment  directory contains the above-mentioned files\nby typing:  1 ls alignment", 
            "title": "Prepare the Environment"
        }, 
        {
            "location": "/modules/cancer-module-snv/snv/#bam-files", 
            "text": "Let\u2019s spend some time to explore bam files.", 
            "title": "BAM Files"
        }, 
        {
            "location": "/modules/cancer-module-snv/snv/#step-1-exploring-bam-files", 
            "text": "1 samtools view alignment/normal/normal.sorted.bam | head -n4   Here you have examples of alignment results. A full description of the\nflags can be found in the SAM specification http://samtools.sourceforge.net/SAM1.pdf  Another useful bit of information in the SAM is the CIGAR string. It\u2019s\nthe 6 th  column in the file.  This column explains how the alignment was achieved.  M == base aligns  but doesn\u2019t have to be a match.  A SNP will have an\nM even if it disagrees with the reference.\\\nI == Insertion\\\nD == Deletion\\\nS == soft-clips. These are handy to find un removed adapters, viral\ninsertions, etc.  An in depth explanation of the CIGAR can be found http://genome.sph.umich.edu/wiki/SAM  The exact details of the cigar string can be found in the SAM spec as\nwell.  We won\u2019t go into too much detail at this point since we want to\nconcentrate on cancer specific issues now.  Now, you can try using picards explain flag site to understand what is\ngoing on with your reads http://broadinstitute.github.io/picard/explain-flags.html  There are 3 unique flags, what do they mean? The flag is the second\ncolumn.  129: \\\nread paired\\\nsecond in pair\\\n\\ 113: \\\nread paired\\\nread reverse strand\\\nmate reverse strand\\\nfirst in pair\\\n\\ 161: \\\nread paired\\\nmate reverse strand\\\nsecond in pair\\  There are lots of possible different flags, let\u2019s look at a few more  1 samtools view alignment/normal/normal.sorted.bam | head -n100   Let\u2019s take the last one, which looks properly paired and find it\u2019s mate\npair.\\\nHINT: Instead of using \u2019head\u2019 what unix command could we pipe the output\nto? HINT2: Once we\u2019ve found both reads to stop the command running we\ntype CTRL-C  1 samtools view alignment/normal/normal.sorted.bam | grep HWI-ST478_0133:4:2205:14675:32513   Using the cigar string, what can we tell about the alignment of the mate\npair?  The mate pair has a less convincing alignment with two insertions and\nsoft clipping reported.  How might the alignment information from the original read be used by\nthe aligner?  Even though the alignment of the mate pair is questionable the presence\nof it\u2019s properly paired mate helps the aligner in deciding where to put\nthe less-certain read.  You can use samtools to filter reads as well.  How many reads mapped and unmapped were there?\\\nHINT: Look at the samtools view help menu by typing samtools view\nwithout any arguments  1 samtools view -c -f4 alignment/normal/normal.sorted.bam   77229  1 samtools view -c -F4 alignment/normal/normal.sorted.bam   22972373", 
            "title": "Step 1: Exploring BAM files"
        }, 
        {
            "location": "/modules/cancer-module-snv/snv/#step-2-pre-processing-indel-realignment", 
            "text": "The first step for this is to realign around indels and snp dense\nregions.\\\nThe Genome Analysis toolkit has a tool for this called IndelRealigner.\\\nIt basically runs in 2 steps:\\\n1. Find the targets\\\n2. Realign them\\   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19 $JAVA7  -Xmx2G  -jar  ${ GATK_JAR }  \\\n  -T RealignerTargetCreator \\\n  -R  ${ REF } /human_g1k_v37.fasta \\\n  -o alignment/normal/realign.intervals \\\n  -I alignment/normal/normal.sorted.bam \\\n  -I alignment/tumour/tumour.sorted.bam \\\n  -L  ${ REF } /human_g1k_v37.intervals $JAVA7  -Xmx2G -jar  ${ GATK_JAR }  \\\n  -T IndelRealigner \\\n  -R  ${ REF } /human_g1k_v37.fasta \\\n  -targetIntervals alignment/normal/realign.intervals \\\n  --nWayOut .realigned.bam \\\n  -I alignment/normal/normal.sorted.bam \\\n  -I alignment/tumour/tumour.sorted.bam \\\n  -L  ${ REF } /human_g1k_v37.intervals\n\n  mv normal.sorted.realigned.ba* alignment/normal/\n  mv tumour.sorted.realigned.ba* alignment/tumour/   Explanation of parameters  -I\n:   BAM file(s)  -T\n:   GATK algorithm to run  -R\n:   the reference genome used for mapping (b37 from GATK here)  -jar\n:   Path to GATK jar file  -L\n:   Genomic intervals to operate on  Why did we use both normal and tumor together?  Because if a region needs realignment, maybe one of the sample in the\npair has less reads or was excluded from the target creation.\\\nThis makes sure the normal and tumor are all in-sync for the somatic\ncalling step.  How many regions did it think needed cleaning ?  1 wc -l alignment/normal/realign.intervals -  27300   Indel Realigner also makes sure the called deletions are left aligned\nwhen there is a microsatellite or homopolymer.  1\n2\n3\n4\n5\n6\n7\n8\n9 This\nATCGAAAA-TCG\ninto\nATCG-AAAATCG\n\nor\nATCGATATATATA--TCG\ninto\nATCG--ATATATATATCG   Why it is important ?  This makes it easier for down stream analysis tools  For NGS analysis, the convention is to left align indels.  This is only really needed when calling variants with legacy locus-based\ntools such as samtools or GATK UnifiedGenotyper. Otherwise you will have\nworse performance and accuracy.  With more sophisticated tools (like GATK HaplotypeCaller) that involve\nreconstructing haplotypes (eg through reassembly), the problem of\nmultiple valid representations is handled internally and does not need\nto be corrected explicitly.", 
            "title": "Step 2: Pre-processing: Indel Realignment"
        }, 
        {
            "location": "/modules/cancer-module-snv/snv/#step-3-pre-processing-fixmates", 
            "text": "Some read entries don\u2019t have their mate information written properly.\\\nWe use Picard to do this:   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15 $JAVA7  -Xmx2G -jar  ${ PICARD_JAR }  FixMateInformation \\\n  VALIDATION_STRINGENCY=SILENT \\\n  CREATE_INDEX=true \\\n  SORT_ORDER=coordinate \\\n  MAX_RECORDS_IN_RAM=500000 \\\n  INPUT=alignment/normal/normal.sorted.realigned.bam \\\n  OUTPUT=alignment/normal/normal.matefixed.bam $JAVA7  -Xmx2G -jar  ${ PICARD_JAR }  FixMateInformation \\\n  VALIDATION_STRINGENCY=SILENT \\\n  CREATE_INDEX=true \\\n  SORT_ORDER=coordinate \\\n  MAX_RECORDS_IN_RAM=500000 \\\n  INPUT=alignment/tumour/tumour.sorted.realigned.bam \\\n  OUTPUT=alignment/tumour/tumour.matefixed.bam", 
            "title": "Step 3: Pre-processing: Fixmates"
        }, 
        {
            "location": "/modules/cancer-module-snv/snv/#step-4-pre-processing-mark-duplicates", 
            "text": "What are duplicate reads ?  Different read pairs representing the same initial DNA fragment.  What are they caused by ?  PCR reactions (PCR duplicates)\\\nSome clusters that are thought of being separate in the flowcell but are\nthe same (optical duplicates)  What are the ways to detect them ?  Picard and samtools uses the alignment positions:\\\n- Both 5\u2019 ends of both reads need to have the same positions.\\\n- Each reads have to be on the same strand as well.\\\n\\\nAnother method is to use a kmer approach:\\\n- take a part of both ends of the fragment\\\n- build a hash table\\\n- count the similar hits\\\n\\\nBrute force, compare all the sequences.  Here we will use picards approach:   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17 $JAVA7  -Xmx2G -jar  ${ PICARD_JAR }  MarkDuplicates \\\n  REMOVE_DUPLICATES=false \\\n  CREATE_MD5_FILE=true \\\n  VALIDATION_STRINGENCY=SILENT \\\n  CREATE_INDEX=true \\\n  INPUT=alignment/normal/normal.matefixed.bam \\\n  OUTPUT=alignment/normal/normal.sorted.dup.bam \\\n  METRICS_FILE=alignment/normal/normal.sorted.dup.metrics $JAVA7  -Xmx2G -jar  ${ PICARD_JAR }  MarkDuplicates \\\n  REMOVE_DUPLICATES=false \\\n  CREATE_MD5_FILE=true \\\n  VALIDATION_STRINGENCY=SILENT \\\n  CREATE_INDEX=true \\\n  INPUT=alignment/tumour/tumour.matefixed.bam \\\n  OUTPUT=alignment/tumour/tumour.sorted.dup.bam \\\n  METRICS_FILE=alignment/tumour/tumour.sorted.dup.metrics   We can look in the metrics output to see what happened.  1 less alignment/normal/normal.sorted.dup.metrics   What percent of reads are duplicates?  0.046996%  Often, we have multiple libraries and when this occurs separate measures\nare calculated for each library. Why is this important to do not combine\neverything ?  Each library represents a set of different DNA fragments.  Each library involves different PCR reactions  So PCR duplicates can not occur between fragment of two different\nlibraries.  But similar fragment could be found between libraries when the coverage\nis high.", 
            "title": "Step 4: Pre-processing: Mark Duplicates"
        }, 
        {
            "location": "/modules/cancer-module-snv/snv/#step-4-pre-processing-base-quality-recalibration", 
            "text": "Why do we need to recalibrate base quality scores ?  The vendors tend to inflate the values of the bases in the reads. The\nrecalibration tries to lower the scores of some biased motifs for some\ntechnologies.  It runs in 2 steps,\\\n1- Build covariates based on context and known snp sites\\\n2- Correct the reads based on these metrics  GATK BaseRecalibrator:   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19 for   i   in   normal   tumour  do \n   $JAVA7  - Xmx2G  - jar  ${ GATK_JAR } \\\n    - T   BaseRecalibrator  \\\n    - nct   2  \\\n    - R  ${ REF }/ human_g1k_v37 . fasta  \\\n    - knownSites  ${ REF }/ dbSnp-138_chr4 . vcf  \\\n    - L   4 : 1 - 10000000  \\\n    - o   alignment /${ i }/${ i }. sorted . dup . recalibration_report . grp  \\\n    - I   alignment /${ i }/${ i }. sorted . dup . bam \n\n     $JAVA7  - Xmx2G  - jar  ${ GATK_JAR } \\\n      - T   PrintReads  \\\n      - nct   2  \\\n      - R  ${ REF }/ human_g1k_v37 . fasta  \\\n      - BQSR   alignment /${ i }/${ i }. sorted . dup . recalibration_report . grp  \\\n      - o   alignment /${ i }/${ i }. sorted . dup . recal . bam  \\\n      - I   alignment /${ i }/${ i }. sorted . dup . bam  done", 
            "title": "Step 4: Pre-processing: Base Quality Recalibration"
        }, 
        {
            "location": "/modules/cancer-module-snv/snv/#bam-qc", 
            "text": "Once your whole bam is generated, it\u2019s always a good thing to check the\ndata again to see if everything makes sense.", 
            "title": "BAM QC"
        }, 
        {
            "location": "/modules/cancer-module-snv/snv/#step-1-bam-qc-compute-coverage", 
            "text": "If you have data from a capture kit, you should see how well your\ntargets worked. Both GATK and BVATools have depth of coverage tools.\\\nHere we\u2019ll use the GATK one   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15 for   i   in   normal   tumour  do \n   $JAVA7   - Xmx2G  - jar  ${ GATK_JAR } \\\n    - T   DepthOfCoverage  \\\n    -- omitDepthOutputAtEachBase  \\\n    -- summaryCoverageThreshold   10  \\\n    -- summaryCoverageThreshold   25  \\\n    -- summaryCoverageThreshold   50  \\\n    -- summaryCoverageThreshold   100  \\\n    -- start   1  -- stop   500  -- nBins   499  - dt   NONE  \\\n    - R  ${ REF }/ human_g1k_v37 . fasta  \\\n    - o   alignment /${ i }/${ i }. sorted . dup . recal . coverage  \\\n    - I   alignment /${ i }/${ i }. sorted . dup . recal . bam  \\\n    - L   4 : 1 - 10000000  done    Explanation of parameters  omitBaseOutput\n:   Do not output depth of coverage at each base  summaryCoverageThreshol\n:   Coverage threshold (in percent) for summarizing statistics  dt\n:   down sampling  L\n:   Genomic intervals to operate on  Coverage is expected to be \u00a025x in these project Look at the coverage:  1\n2 less -S alignment/normal/normal.sorted.dup.recal.coverage.sample_interval_summary\nless -S alignment/tumour/tumour.sorted.dup.recal.coverage.sample_interval_summary   Is the coverage fit with the expectation ?  Yes the mean coverage of the region is 25x:  summaryCoverageThreshold is a useful function to see if your coverage is\nuniform.  Another way is to compare the mean to the median. If both are quite\ndifferent that means something is wrong in your coverage.", 
            "title": "Step 1: BAM QC: Compute Coverage"
        }, 
        {
            "location": "/modules/cancer-module-snv/snv/#step-2-bam-qc-insert-size", 
            "text": "It corresponds to the size of DNA fragments sequenced.  Different from the gap size (= distance between reads) !  These metrics are computed using Picard:   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 for   i   in   normal   tumour  do \n   $JAVA7  - Xmx2G  - jar  ${ PICARD_JAR }  CollectInsertSizeMetrics  \\\n     VALIDATION_STRINGENCY = SILENT  \\\n     REFERENCE_SEQUENCE =${ REF }/ human_g1k_v37 . fasta  \\\n     INPUT = alignment /${ i }/${ i }. sorted . dup . recal . bam  \\\n     OUTPUT = alignment /${ i }/${ i }. sorted . dup . recal . metric . insertSize . tsv  \\\n     HISTOGRAM_FILE = alignment /${ i }/${ i }. sorted . dup . recal . metric . insertSize . histo . pdf  \\\n     METRIC_ACCUMULATION_LEVEL = LIBRARY  done    look at the output  1\n2 less -S alignment/normal/normal.sorted.dup.recal.metric.insertSize.tsv\nless -S alignment/tumour/tumour.sorted.dup.recal.metric.insertSize.tsv   How do the two libraries compares?  The tumour sample has a larger median insert size than the normal sample\n(405 vs 329)", 
            "title": "Step 2: BAM QC: Insert Size"
        }, 
        {
            "location": "/modules/cancer-module-snv/snv/#step-3-bam-qc-alignment-metrics", 
            "text": "It tells you if your sample and you reference fit together  For the alignment metrics, samtools flagstat is very fast but with\nbwa-mem since some reads get broken into pieces, the numbers are a bit\nconfusing.  We prefer the Picard way of computing metrics:  1\n2\n3\n4\n5\n6\n7\n8\n9 for   i   in   normal   tumour  do \n   $JAVA7  - Xmx2G  - jar  ${ PICARD_JAR }  CollectAlignmentSummaryMetrics  \\\n     VALIDATION_STRINGENCY = SILENT  \\\n     REFERENCE_SEQUENCE =${ REF }/ human_g1k_v37 . fasta  \\\n     INPUT = alignment /${ i }/${ i }. sorted . dup . recal . bam  \\\n     OUTPUT = alignment /${ i }/${ i }. sorted . dup . recal . metric . alignment . tsv  \\\n     METRIC_ACCUMULATION_LEVEL = LIBRARY  done    explore the results  1\n2 less -S alignment/normal/normal.sorted.dup.recal.metric.alignment.tsv\nless -S alignment/tumour/tumour.sorted.dup.recal.metric.alignment.tsv   Do you think the sample and the reference genome fit together ?  Yes, 99% of the reads have been aligned\\\nUsually, we consider:\\\n- A good alignment if > 85%\\\n- Reference assembly issues if [60-85]%\\\n- Probably a mismatch between sample and ref if \\  60 %", 
            "title": "Step 3: BAM QC: Alignment metrics"
        }, 
        {
            "location": "/modules/cancer-module-snv/snv/#variant-calling", 
            "text": "Most of SNV caller use either a Baysian, a threshold or a t-test\napproach to do the calling  Here we will try 3 variant callers.\\\n- Varscan 2\\\n- MuTecT\\\n- Strelka  Other candidates\\\n- Virmid\\\n- Somatic sniper  many, MANY others can be found here:  https://www.biostars.org/p/19104/  In our case, let\u2019s create a new work directory to start with (from base\ndirectory):  1\n2 cd $SNV_BASE\nmkdir variant_calling   varscan 2 VarScan calls somatic variants (SNPs and indels) using a\nheuristic method and a statistical test based on the number of aligned\nreads supporting each allele.  Varscan somatic caller expects both a normal and a tumour file in\nSAMtools pileup format from sequence alignments in binary alignment/map\n(BAM) format. To build a pileup file, you will need:    A SAM/BAM file ( myData.bam ) that has been sorted using the sort\ncommand of SAMtools. - The reference sequence ( reference.fasta ) to\nwhich reads were aligned, in FASTA format. - The SAMtools software\npackage.  for i in normal tumour\ndo\nsamtools mpileup -L 1000 -B -q 1 \\\n  -f ${REF}/human_g1k_v37.fasta \\\n  -r 4:1-10000000 \\\n  alignment/${i}/${i}.sorted.dup.recal.bam \\   variant_calling/${i}.mpileup\ndone   $JAVA7 -Xmx2G -jar ${VARSCAN_JAR} \\\nsomatic variant_calling/normal.mpileup \\\nvariant_calling/tumour.mpileup \\\nvariant_calling/varscan \\ output-vcf 1 \\ strand-filter 1 \\ somatic-p-value 0.001    Notes on samtools arguments  -L\n:   = max per-sample depth for INDEL calling [1000] ;  -B\n:   = disable BAQ (per-Base Alignment Quality) ;  -q\n:   = skip alignments with mapQ smaller than 1 ;  -g\n:   = generate genotype likelihoods in BCF format  Notes on bcftools arguments  -v\n:   = output potential variant sites only  -c\n:   = SNP calling (force \u2013e : likelihood based analyses)  -g\n:   = call genotypes at variant sites  Now let\u2019s try a different variant caller, MuTect\\  Note MuTecT only works with Java 6, 7 will give you an error\\\nif you get  Comparison method violates its general contract!\\\nyou used java 7   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12 java -Xmx2G -jar  ${ MUTECT_JAR }  \\\n  -T MuTect \\\n  -R  ${ REF } /human_g1k_v37.fasta \\\n  -dt NONE -baq OFF --validation_strictness LENIENT -nt 2 \\\n  --dbsnp  ${ REF } /dbSnp-138_chr4.vcf \\\n  --input_file:normal alignment/normal/normal.sorted.dup.recal.bam \\\n  --input_file:tumor alignment/tumour/tumour.sorted.dup.recal.bam \\\n  --out variant_calling/mutect.call_stats.txt \\\n  --coverage_file variant_calling/mutect.wig.txt \\\n  -pow variant_calling/mutect.power \\\n  -vcf variant_calling/mutect.vcf \\\n  -L 4:1-10000000   And finally let\u2019s try Illumina\u2019s Strelka   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\n16 cp  ${ STRELKA_HOME } /etc/strelka_config_bwa_default.ini .\n\nsed  s/isSkipDepthFilters =.*/isSkipDepthFilters = 1/g  -i strelka_config_bwa_default.ini ${ STRELKA_HOME } /bin/configureStrelkaWorkflow.pl \\\n  --normal=alignment/normal/normal.sorted.dup.recal.bam \\\n  --tumor=alignment/tumour/tumour.sorted.dup.recal.bam \\\n  --ref= ${ REF } /human_g1k_v37.fasta \\\n  --config= ${ SNV_BASE } /strelka_config_bwa_default.ini \\\n  --output-dir=variant_calling/strelka/\n\n  cd variant_calling/strelka/\n  make -j2\n  cd ../..\n\n  cp variant_calling/strelka/results/passed.somatic.snvs.vcf variant_calling/strelka.vcf   Now we have variants from all three methods. Let\u2019s compress and index\nthe vcfs for future visualisation.  1 for i in variant_calling/*.vcf;do bgzip -c $i   $i.gz ; tabix -p vcf $i.gz;done   Let\u2019s look at a compressed vcf.  1 zless -S variant_calling/varscan.snp.vcf.gz   Details on the spec can be found here: http://vcftools.sourceforge.net/specs.html  Fields vary from caller to caller.  Some values are are almost always there:\\\n- The ref vs alt alleles,\\\n- variant quality (QUAL column)\\\n- The per-sample genotype (GT) values.  Note on vcf fields  DP\n:   =  Raw read depth  GT\n:   =  Genotype  PL\n:   =  List of Phred-scaled genotype likelihoods  (min is better)  DP\n:   =  # high-quality bases  SP\n:   =  Phred-scaled strand bias P-value  GQ\n:   =  Genotype Quality  Looking at the three vcf files, how can we detect only somatic variants?  some commands to find somatic variant in the vcf file  varscan  1 grep SOMATIC variant_calling/varscan.snp.vcf   MuTecT  1 grep -v REJECT variant_calling/mutect.vcf | grep -v  ^#    Strelka  1 grep -v  ^#  variant_calling/strelka.vcf", 
            "title": "Variant Calling"
        }, 
        {
            "location": "/modules/cancer-module-snv/snv/#variant-visualisation", 
            "text": "The Integrative Genomics Viewer (IGV) is an efficient visualization tool\nfor interactive exploration of large genome datasets.  Before jumping into IGV, we\u2019ll generate a track IGV can use to plot\ncoverage:  1\n2\n3\n4\n5\n6\n7\n8 for   i   in   normal   tumour  do \n   $JAVA7  - jar  ${ IGVTOOLS_PATH }/ igvtools . jar   count  \\\n    - f   min , max , mean  \\\n     alignment /${ i }/${ i }. sorted . dup . recal . bam  \\\n     alignment /${ i }/${ i }. sorted . dup . recal . bam . tdf  \\\n     b37  done    Open IGV  1 $IGV   Then: 1. Chose the reference genome corresponding to those use for\nalignment (b37)\\\n2. Load bam files (tumour.sorted.dup.recal.bam and\nnormal.sorted.dup.recal.bam)\\\n3. Load vcf files (from variant_Calling directory)  Explore/play with the data:\\\n-find germline variants\\\n-find somatic variants\\\n-Look around", 
            "title": "Variant Visualisation"
        }, 
        {
            "location": "/modules/cancer-module-snv/snv/#variant-annotation", 
            "text": "Following variant calling, we end up with a VCF file of genomic\ncoordinates with the genotype(s) and quality information for each\nvariant. By itself, this information is not much use to us unless there\nis a specific genomic location we are interested in. Generally, we next\nwant to annotate these variants to determine whether they impact any\ngenes and if so what is their level of impact (e.g. are they causing a\npremature stop codon gain or are they likely less harmful missense\nmutations).  The sections above have dealt with calling somatic variants from the\nfirst 10Mb of chromosome 4. This is important in finding variants that\nare unique to the tumour sample(s) and may have driven both tumour\ngrowth and/or metastasis. An important secondary question is whether the\ngermline genome of the patient contains any variants that may have\ncontributed to the development of the initial tumour through\npredisposing the patient to cancer. These variants  may not  be captured\nby somatic variant analysis as their allele frequency may not change in\nthe tumour genome compared with the normal.  For this section, we will use  all  variants from the first 60Mb of\nchromosome 5 that have been pre-generated using the GATK HaplotypeCaller\nvariant caller on both the normal and tumour genomes. The output of this\nwas GVCF files which were fed into GATK GenotypeGVCFs to produce a\nmerged VCF file. We will use this pre-generated file as we are primarily\ninterested in the annotation of variants rather than their generation.\nThe annotation method we will use is called  Variant Effect Predictor \nor VEP for short and is available from Ensembl here: http://ensembl.org/info/docs/tools/vep/index.html .  Our pre-generated VCF file is located in the  variants  folder. Let\u2019s\nhave a quick look at the variants:  1 zless variants/HC.chr5.60Mb.vcf.gz   Notice how there are two genotype blocks at the end of each line for the\nnormal (Blood) and tumour (liverMets) samples.  Let\u2019s now run VEP on this VCF file to annotate each variant with its\nimpact(s) on the genome.  1 perl Applications/ensembl-tools/scripts/variant_effect_predictor/variant_effect_predictor.pl -i variants/HC.chr5.60Mb.vcf.gz --vcf -o variants/HC.chr5.60Mb.vep.vcf --stats_file variants/HC.chr5.60Mb.vep.html --format vcf --offline -fork 4 --fasta ref/human_g1k_v37.fasta --fields Consequence,Codons,Amino_acids,Gene,SYMBOL,Feature,EXON,PolyPhen,SIFT,Protein_position,BIOTYPE   VEP will take approximately 10 minutes to run and once it is finished\nyou will have a new VCF file with all of the information in the input\nfile but with added annotations in the INFO block. VEP also produces an\nHTML report summarising the distribution and impact of variants\nidentified.  Once VEP is done running, let\u2019s first look at the HTML report it\nproduced with the following command:  1 firefox variants/HC.chr5.60Mb.vep.html   This report shows information on the VEP run, the number of variants,\nthe classes of variants detected, the variant consequences and the\ndistributions of variants through the genome. Close Firefox to resume\nthe terminal prompt.  Now let\u2019s look at the variant annotations that VEP has added to the VCF\nfile by focussing on a single variant. Let\u2019s fetch the same variant from\nthe original VCF file and the annotated VCF file to see what has been\nchanged.  1\n2 zcat variants/HC.chr5.60Mb.vcf.gz | grep  5\\s174106\\s \ngrep  5\\s174106\\s  variants/HC.chr5.60Mb.vep.vcf   These commands give us the original variant:  1 5   174106  .   G   A   225.44  .   AC=2;AF=0.500;AN=4;BaseQRankSum=1.22;ClippingRankSum=0.811;DP=21;FS=0.000;GQ_MEAN=127.00;GQ_STDDEV=62.23;MLEAC=2;MLEAF=0.500;MQ=60.00;MQ0=0;MQRankSum=0.322;NCC=0;QD=10.74;ReadPosRankSum=0.377;SOR=0.446   GT:AD:DP:GQ:PL  0/1:7,6:13:99:171,0,208 0/1:5,3:8:83:83,0,145   and the same variant annotated is:  1 5   174106  .   G   A   225.44  .   AC=2;AF=0.500;AN=4;BaseQRankSum=1.22;ClippingRankSum=0.811;DP=21;FS=0.000;GQ_MEAN=127.00;GQ_STDDEV=62.23;MLEAC=2;MLEAF=0.500;MQ=60.00;MQ0=0;MQRankSum=0.322;NCC=0;QD=10.74;ReadPosRankSum=0.377;SOR=0.446;CSQ=missense_variant|cGg/cAg|R/Q|ENSG00000153404|PLEKHG4B|ENST00000283426|16/18|||1076|protein_coding,non_coding_transcript_exon_variant non_coding_transcript_variant|||ENSG00000153404|PLEKHG4B|ENST00000504041|5/8||||retained_intron  GT:AD:DP:GQ:PL  0/1:7,6:13:99:171,0,208 0/1:5,3:8:83:83,0,145   You can see that VEP has added:  1 CSQ=missense_variant|cGg/cAg|R/Q|ENSG00000153404|PLEKHG4B|ENST00000283426|16/18|||1076|protein_coding,non_coding_transcript_exon_variant non_coding_transcript_variant|||ENSG00000153404|PLEKHG4B|ENST00000504041|5/8||||retained_intron   This is further composed of two annotations for this variant:  1 missense_variant|cGg/cAg|R/Q|ENSG00000153404|PLEKHG4B|ENST00000283426|16/18|||1076|protein_coding   and  1 non_coding_transcript_exon_variant non_coding_transcript_variant|||ENSG00000153404|PLEKHG4B|ENST00000504041|5/8||||retained_intron   The first of these is saying that this variant is a missense variant in\nthe gene PLEKHG4B for the transcript ENST00000283426 and the second that\nit is also a non_coding_transcript_exon_variant in the transcript\nENST00000504041.", 
            "title": "Variant Annotation"
        }, 
        {
            "location": "/modules/cancer-module-snv/snv/#variant-filtration", 
            "text": "We now have a VCF file where each variant has been annotated with one or\nmore impacts for one or more genes. In a typical whole cancer genome,\nyou will have about 4-5 million variants, and therefore rows, in a VCF\nfile which takes up gigabytes of space. In our small example, we have\njust 100,000 variants which is already too large to make any kind of\nmeaningful sense out of by just opening up the VCF file in a text\neditor. We need a solution that allows us to perform intelligent queries\non our variants to search this mass of noise for the signal we are\ninterested in.  Luckily, such a free tool exists and is called GEMINI. GEMINI takes as\nan input your annotated VCF file and creates a database file which it\ncan then query using Structured Query Language (SQL) commands. Not only\ndoes GEMINI make your variants easily searchable, it also brings in many\nexternal annotations to add more information about your variants (such\nas their frequencies in international databases).  To get started with GEMINI, let\u2019s make a database out of our annotated\nVCF file.  1 gemini load -v variants/HC.chr5.60Mb.vep.vcf --cores 4 --skip-gerp-bp --skip-cadd -t VEP variants/HC.chr5.60Mb.vep.vcf.db   This will take approximately 10 minutes. You will see a few errors due\nto multiallelic sites, normally these sites are decomposed and\nnormalized before creating the GEMINI database but this is outside the\nscope of this workshop.  Once the database has been created let\u2019s run a basic query to see what\nkind of information we get out of GEMINI.  1 gemini query -q  SELECT *, (gts).(*), (gt_types).(*), (gt_depths).(*), (gt_ref_depths).(*), (gt_alt_depths).(*), (gt_quals).(*) FROM variants LIMIT 10;  --header variants/HC.chr5.60Mb.vep.vcf.db   This will output a bunch of ordered information for your query to the\ncommand line, this is usually saved to a TSV file and opened in a\nspreadsheet as we will do for the next query. In the mean time, let\u2019s\ndissect this query to understand the syntax we need to use to filter our\nvariants. First, we have a SELECT statement which simply specifies that\nwe want to select data from the database. The following comma-separated\nvalues are the columns that we want to output from the database, in this\ncase we are selecting all columns with the star character and then all\nsub-columns for each sample with the other values. Then, we have a  FROM\nvariants  statement which is specifying the table within the database\nthat we want to fetch information from. Finally, the  LIMIT 10 \nstatement specifies that no more than 10 rows should be returned. In\nsummary then, we are asking for all columns for 10 rows from the table variants . If you haven\u2019t used SQL before don\u2019t worry, the GEMINI\nwebsite is very helpful and provides many examples for how to query your\ndatabase.  Let\u2019s now perform a more interesting query to find variants that have a\nmedium or high impact on a gene and are rare or not present in existing\ninternational allele frequency databases. We will save the output of\nthis query to a file and open it up in a spreadsheet.  1 gemini query -q  SELECT *, (gts).(*), (gt_types).(*), (gt_depths).(*), (gt_ref_depths).(*), (gt_alt_depths).(*), (gt_quals).(*) FROM variants WHERE (impact_severity =  HIGH  OR impact_severity =  MED ) AND (aaf_1kg_all   0.01 OR aaf_1kg_all is null) AND (aaf_esp_all   0.01 OR aaf_esp_all is null) AND (aaf_exac_all   0.01 OR aaf_exac_all is null);  --header variants/HC.chr5.60Mb.vep.vcf.db   variants/gemini-result.tsv   Notice that we have added a WHERE statement which restricts the rows\nthat are returned based on values that we specify for specific columns.\nHere, we are asking to return variants where their impact on the gene\n(impact_severity column) is medium or high and the allele frequency in\n1000Genomes, ESP and EXaC is less than 1% or the variant is not present\nin any of these databases.  Now let\u2019s open the result in a spreadsheet to look at the annotations:  1\n2 libreoffice --calc variants/gemini-result.tsv\nTick the  Tab  under  Separated by  on the dialog window that comes up.   You can see that the first 14 columns contain information on the variant\nincluding its location, ref, alt, dbSNP ID, quality and type. Slowly\nscroll to the right and look at the columns of data that are provided.\nMost importantly, column BD includes the gene this variant impacts, BN\nthe impact itself and BP the impact severity. Scroll towards the end of\nthe spreadsheet until you get to columns ED and EE, these contain the\ngenotype for each of the samples. Columns EH and EI contain the total\ndepth for each variant in each sample and the 4 following columns\ncontain the reference and alternate depths for each sample. Finally,\ncolumns EN and EO contain the genotype qualities (from the GQ field in\nthe VCF) for each sample. As you scroll back and forth through this\nspreadsheet, you will see that GEMINI brings in information from a\nvariety of sources including: OMIM, ClinVar, GERP, PolyPhen 2, SIFT,\nESP, 1000 Genomes, ExAC, ENCODE, CADD and more! We are only looking at a\nsmall number of variants from the start of a chromosome so not many of\nthese annotations will be present but in a full genome database they are\nincredibly useful.  GEMINI allows you to filter your variants based on any column that you\nsee in this results file. For example, you may want all variants in a\nspecific gene, in which case you would simply add  WHERE gene = \u2019BRCA1\u2019 \nto your query. For complete documentation with many examples of queries,\nsee the GEMINI documentation here:  http://gemini.readthedocs.org .", 
            "title": "Variant Filtration"
        }, 
        {
            "location": "/modules/cancer-module-snv/snv/#references", 
            "text": "Paila U, Chapman BA, Kirchner R and Quinlan AR.  GEMINI: Integrative\n    Exploration of Genetic Variation and Genome Annotations . PLoS\n    Comput Biol, 2013, 9(7): e1003153. doi:10.1371/journal.pcbi.1003153    McLaren W, Pritchard B, Rios D, Chen Y, Flicek P and Cunningham F.\n     Deriving the consequences of genomic variants with the Ensembl API\n    and SNP Effect Predictor . Bioinformatics, 2010, 26(16):2069-70,\n    doi:10.1093/bioinformatics/btq330", 
            "title": "References"
        }, 
        {
            "location": "/modules/cancer-module-snv/snv/#acknowledgements", 
            "text": "This tutorial is an adaptation of the one created by Louis letourneau https://github.com/lletourn/Workshops/tree/ebiCancerWorkshop201407doc/01-SNVCalling.md .\nI would like to thank and acknowledge Louis for this help and for\nsharing his material. The format of the tutorial has been inspired from\nMar Gonzalez Porta. I also want to acknowledge Joel Fillon, Louis\nLetrouneau (again), Francois Lefebvre, Maxime Caron and Guillaume\nBourque for the help in building these pipelines and working with all\nthe various datasets.", 
            "title": "Acknowledgements"
        }, 
        {
            "location": "/modules/cancer-module-cnv/cnv-tut/", 
            "text": "Key Learning Outcomes\n\n\nAfter completing this practical the trainee should be able to:\n\n\n\n\n\n\nUnderstand and perform a simple copy number variation analysis on\n    NGS data\n\n\n\n\n\n\nBecome familiar with Sequenza\n\n\n\n\n\n\nUnderstand the CNV inference process as an interplay between depth\n    of sequencing, cellularity and B-allele frequency\n\n\n\n\n\n\nVisualize CNV events by manual inspection\n\n\n\n\n\n\nResources You\u2019ll be Using\n\n\nTools Used\n\n\nSequenza:\n\n\nhttp://www.cbs.dtu.dk/biotools/sequenza/\n\n\nIGV:\n\n\nhttp://www.broadinstitute.org/igv/\n\n\nSources of Data\n\n\nhttp://sra.dnanexus.com/studies/ERP001071\n\n\nhttp://www.ncbi.nlm.nih.gov/pubmed/22194472\n\n\nIntroduction\n\n\nThe goal of this hands-on session is to perform a copy number variation\nanalysis (CNV) on a normal/tumour pair of alignment files (BAMs)\nproduced by the mapping of Illumina short read sequencing data.\n\n\nTo ensure reasonable analysis times, we will perform the analysis on a\nheavily subset pair of BAM files. These files contain just the first\n60Mb of chromosome 5 but contain several examples of inferred copy\nnumber events to enable interpretation and visualisation of the copy\nnumber variation that is present in entire cancer genomes. Sequenza is\nthe tool we will use to perform this analysis. It consists of a two\nPython pre-processing steps followed by a third step in R to infer the\ndepth ratio, cellularity, ploidy and to plot the results for\ninterpretation.\n\n\nIn the second part of the tutorial we will also be using IGV to\nvisualise and manually inspect the copy number variation we inferred in\nthe first part for validation purposes. This section will also include a\ndiscussion on the importance of good quality data by highlighting the\ninadequacies of the workshop dataset and the implications this has on\nanalysis results.\n\n\nPrepare the Environment\n\n\nWe will use a dataset derived from whole genome sequencing of a\n33-yr-old lung adenocarcinoma patient, who is a never-smoker and has no\nfamilial cancer history.\n\n\nThe data files are contained in the subdirectory called \ndata\n and are\nthe following:\n\n\nnormal.chr5.60Mb.bam\n and \nnormal.chr5.60Mb.bam.bai\n\n:   \\\n\n\ntumour.chr5.60Mb.bam\n and \ntumour.chr5.60Mb.bam\n\n:   \\\n\n\nThese files are based on subsetting the whole genomes derived from blood\nand liver metastases to the first 60Mb of chromosome 5. This will allow\nour analyses to run in a sufficient time during the workshop, but it\u2019s\nworth being aware that we are analysing just 1.9% of the genome which\nwill highlight the length of time and resources required to perform\ncancer genomics on full genomes!\n\n\nOpen the Terminal and go to the \nCNV\n working directory:\n\n\n1\ncd cnv/\n\n\n\n\n\n\nAll commands entered into the terminal for this tutorial should be from\nwithin the \ncnv\n directory.\n\n\nCheck that the \ndata\n directory contains the above-mentioned files by\ntyping:\n\n\n1\nll data\n\n\n\n\n\n\nAll commands used in this tutorial can be copy/pasted from the\ncommands.sh file in the \ncnv\n directory.\n\n\nSequenza CNV Analysis\n\n\nSequenza is run in three steps. The first pre-processing step is run on\nthe final normal and tumour mapped data (BAM files) in order to walk the\ngenome in a pileup format (automatically generated by samtools). This\nfirst step finds high quality sites in the genomes and extracts their\ndepth and genotype in the normal genome and calculates the variant\nalleles and allele frequencies in the tumour genome. The second step is\nto perform a binning on these sites to save space and analysis time in\nthe third step. Finally, the third step is run in R normalise the depth\nratio between the normal/tumour genomes, infer cellularity and ploidy\nand graphically output results for interpretation.\n\n\nStep 1: Pre-Processing \u2013 Walking the Genome\n\n\n1\npypy software/sequenza/sequenza-utils.py bam2seqz -n data/normal.chr5.60Mb.bam -t data/tumour.chr5.60Mb.bam --fasta assets/human\\_g1k\\_v37.fasta -gc assets/human\\_g1k\\_v37.gc50Base.txt.gz -C 5:1-60000000 | gzip \n stage1.seqz.gz\n\n\n\n\n\n\nHint: press tab after typing a few characters of a directory of filename\nto auto-complete the rest. This makes entering long file names very\nquick.\n\n\nExplanation of parameters\n\n\n-n\n:   the normal BAM\n\n\n-t\n:   the tumour BAM\n\n\n\u2013fasta\n:   the reference genome used for mapping (b37 here)\n\n\n-gc\n:   GC content as windows through the genome (pre-generated and\n    downloadable from the Sequenza website)\n\n\n-C\n:   specifies the genomic location to process\n\n\nThere will not be any indication that it is running once you launch the\ncommand, to make sure it is running open a new Terminal tab with Shift +\nControl + T (or from the menu with File then Open Tab) and type the\ncommand \u2019top\u2019. You should see the top line being the command \u2019pypy\u2019 with\na % CPU usage of 98/99%. Press q to quit out of this process view and go\nback to the tab running Sequenza. If everything is running correctly, it\nwill take approximately 40 minutes to run. Go have a coffee!\n\n\nOnce the command is done you will be returned to the terminal prompt.\nMake sure the output file is the correct size by typing \u2019ll -h\u2019 from the\nTerminal window that you ran Sequenza from, there should be a file\ncalled \nstage1.seqz.gz\n of the size 326M.\n\n\nYou can look at the first few lines of the output in the file\n\nstage1.seqz.gz\n with:\n\n\n1\nzcat stage1.seqz.gz | head -n 20\n\n\n\n\n\n\nThis output has one line for each position in the BAMs and includes\ninformation on the position, depths, allele frequencies, zygosity, GC in\nthe location.\n\n\nStep 2: Perform Binning\n\n\nThe binning step takes the rows of genomic positions and compresses them\ndown to 1 row for every 200 rows previously. This massively reduces the\nfile size and processing time in the third step.\n\n\n1\npypy software/sequenza/sequenza-utils.py seqz-binning -w 200 -s stage1.seqz.gz | gzip \n stage2.seqz.gz\n\n\n\n\n\n\nExplanation of parameters\n\n\n-w\n:   the window size (typically 50 for exomes, 200 for genomes)\n\n\n-s\n:   the large seqz file generated in the first step\n\n\nThis step should take approximately 4 minutes to complete.\n\n\nStep 3: Running Sequenza Algorithms and Plotting Results\n\n\nWe will now perform the CNV analysis and output the results using the R\npart of Sequenza.\n\n\nOpen the R terminal\n\n\n1\nR\n\n\n\n\n\n\nYou should now see the R prompt identified with \n>\n.\n\n\nRun the Sequenza R commands\n\n\n1\n2\n3\n4\n5\n6\nlibrary\n(\nsequenza\n)\n\n\nsetwd\n(\n/home/trainee/cnv\n)\n\ndata.file \n-\n \nstage2.seqz.gz\n\nseqzdata \n-\n sequenza.extract\n(\ndata.file\n)\n\nCP.example \n-\n sequenza.fit\n(\nseqzdata\n)\n\nsequenza.results\n(\nsequenza.extract \n=\n seqzdata\n,\n cp.table \n=\n CP.example\n,\n sample.id \n=\n \nCanGenWorkshop\n,\n out.dir\n=\nsequenza_results\n)\n\n\n\n\n\n\n\nIf every command ran successfully, you will now have a\n\nsequenza_results\n folder containing 13 files.\n\n\nQuit R\n\n\n1\n2\nq()\nThen n at the \nSave workspace image\n prompt\n\n\n\n\n\n\nSequenza Analysis Results and Visualisation\n\n\nOne of the first and most important estimates that Sequenza provides is\nthe tumour cellularity (the estimated percentage of tumour cells in the\ntumour genome). This estimate is based on the B allele frequency and\ndepth ratio through the genome and is an important metric to know for\ninterpretation of Sequenza results and for other analyses. Lets look at\nthe cellularity estimate for our analysis by opening\nCanGenWorkshop_model_fit.pdf with the command:\n\n\n1\nevince sequenza_results/CanGenWorkshop_model_fit.pdf\n\n\n\n\n\n\nThe cellularity estimate is at the top along with the average ploidy\nestimate and the standard deviation of the B allele frequency. We can\nsee that the cellularity has been estimated at 24% which is fairly low\nand we will see why this is bad in the next section on CNV\nvisualisation. The ploidy value of 2.1 indicates this piece of the\ngenome is not hugely amplified or deleted and the BAF standard deviation\nindicates there are no significant long losses of heterozygosity.\n\n\nClose the PDF window to resume the Terminal prompt.\n\n\nLet\u2019s now look at the CNV inferences through our genomic block. Open the\ngenome copy number visualisation file with:\n\n\n1\nevince sequenza_results/CanGenWorkshop_genome_view.pdf\n\n\n\n\n\n\nThis file contains three \npages\n of copy number events through the\nentire genomic block. The first page shows copy numbers of the A (red)\nand B (blue) alleles, the second page shows overall copy number changes\nand the third page shows the B allele frequency and depth ratio through\ngenomic block. Looking at the overall copy number changes, we see that\nour block is at a copy number of 2 with a small duplication to copy\nnumber 4 about \n of the way through the block and another just after\nhalfway through the block. There is also a reduction in copy number to 1\ncopy about \n of the way through the block. The gap that you see just\nbefore this reduction in copy number is the chromosomal centromere - an\narea that is notoriously difficult to sequence so always ends up in a\ngap with short read data.\n\n\nYou can see how this is a very easy to read output and lets you\nimmediately see the frequency and severity of copy number events through\nyour genome. Let\u2019s compare the small genomic block we ran with the same\noutput from the entire genome which has been pre-computed for you. This\nis located in the \npre_generated/results_whole_genome\n folder and\ncontains the same 13 output files as for the small genomic block. As\nbefore, let\u2019s look at the cellularity estimate with:\n\n\n1\nevince pre_generated/results_whole_genome/CanGenWorkshop_model_fit.pdf\n\n\n\n\n\n\nIt now looks like it\u2019s even worse at just 16%! A change is to be\nexpected as we were only analysing 1.9% of the genome. Let\u2019s now look at\nthe whole genome copy number profile with:\n\n\n1\nevince pre_generated/results_whole_genome/CanGenWorkshop_genome_view.pdf\n\n\n\n\n\n\nYou can see that there are a number of copy number events across the\ngenome and our genomic block (the first 60Mb of chromosome 5) is\ninferred as mostly copy number 4 followed by a reduction to copy number\n2, rather than 2 to 1 as we saw in the output we generated. The reason\nfor this is that Sequenza uses the genome-wide depth ratio and BAF in\norder to estimate copy number, if you ask it to analyse a small block\nmostly at copy number 4 with a small reduction to copy number 2, the\nmost likely scenario in lieu of more data is that this is a copy number\n2 block with a reduction to 1. It\u2019s important to carefully examine the\ncellularity, ploidy and BAF estimates of your sample along with the\nplots of model fit (CanGenWorkshop_model_fit.pdf) and\ncellularity/ploidy contours (CanGenWorkshop_CP_contours.pdf) in order\nto decide if you believe Sequenza\u2019s inference of the copy numbers. Have\na look at these for yourself if you want to get a better idea of how\nSequenza makes its inferences and conclusions.\n\n\nCNV Visualisation/Confirmation in IGV\n\n\nLet\u2019s see if we can visualise one of the CNV events where copy number\nincreased significantly. We\u2019ll focus on the copy number 4 event seen at\nabout \n of the way through the CanGenWorkshop_genome_view.pdf output\nwe generated. First, we need to find the coordinates that have been\npredicted for this event. Have a look at the\nCanGenWorkshop_segments.txt file in the results folder to view all\npredicted CNV events with:\n\n\n1\nless sequenza_results/CanGenWorkshop_segments.txt\n\n\n\n\n\n\nThere is only one at a copy number of 4 (CNt column) and it starts at\n21051700 to 21522065 which is 470kb and corresponds to the small block\nwe see in the genome view PDF.\n\n\nQuit out of viewing the segments file by pressing q.\n\n\nWe will now open IGV and see if we can observe the predicted increase in\ncopy number within these genomic coordinates.\n\n\n1\n/home/trainee/snv/Applications/igv/igv.sh\n\n\n\n\n\n\nIGV will take 30 seconds or so to open so just be patient.\n\n\nFor a duplication of this size, we will not be able to easily observe it\njust by looking at the raw read alignments. In order to see it we will\ngenerate two tiled data files (TDFs) within IGV which contain the\naverage read depth for a given window size through the genome. This\nmeans that we can aggregate the average read depth over relatively large\nchunks of the genome and compare these values between the normal and\ntumour genomes.\n\n\nTo begin, we will go to \nTools\n then \nRun igvtools\n in the IGV\nmenubar. Specify the normal bam file (under \ncnv\n then \ndata\n) as the\ninput file and change the window size to 100000 (one hundred thousand).\nThen press the \nRun\n button and IGV will make the TDF file. This takes\nabout 5 minutes. Repeat this for the tumour genome.\n\n\nAfter you have both TDF files, go to \nFile\n and \nLoad from file\n in\nthe menubar and select the BAM and TDF files to open. Once you have\nopened them, they will appear as tracks along with the BAM tracks we\nloaded initially. Navigate to the genomic coordinates of our event\n(5:21,051,700-21,522,065) by typing it in the coordinate box at the top.\nMouse over the two blue tracks to get the average depth values for the\n100,000bp windows. What you should see is that the liverMets sample has\n3-6X more coverage than the Blood sample for the four windows that cover\nthis region.\n\n\nThis may seem a bit underwhelming, after all, wasn\u2019t the increase of the\nregion to a copy number of 4, i.e. we expect a doubling of reads in the\ntumour? To explain why we are only seeing such a small coverage\nincrease, we need to turn to our good friend mathematics!\n\n\nImagine we have two 30X genomes for the normal and tumour samples and\nthe tumour is at 100% purity. If there is a copy number increase to 4 in\nthe tumour from 2 in the normal, the duplicated segment should indeed\nhave twice as many reads as the same segment in the normal genome. Now,\nlets imagine the tumour genome was only at a purity of 50% (i.e. it\ncontains 50% normal cells and 50% tumour cells). Now, half of the\nduplicated \ntumour genome\n segment will be at a copy number of 2 and\nhalf will be at 4. What does this mean when we sequence them as a\nmixture? The resulting average copy number of the block will be\n$(0.5*2)+(0.5*4) = 3$. Now what if we only have 16% tumour cells in our\n\ntumour genome\n? This will be $(0.84*2)+(0.16*4) = 2.32$. You can see\nhow sequencing a low cellularity tumour at a low depth makes it much\nharder to infer copy number variations!\n\n\nReturning to our genomes at hand, when we previously looked at the\ncellularity estimate of this tumour we saw it was 20% from the small\nblock we ran or 16% from the whole genome. Thus, the read depth increase\nof just 3-6X (about 10-20% more reads) in this segment is not\nsurprising. A low cellularity tumour greatly reduces our power to infer\ncopy number events as relatively small changes in depth can occur by\nchance in the genome and these can be mis-identified as copy number\nchanges. As well as this, it reduces our power for other analyses since\nwe must also remember that a tumour can itself contain multiple clones\nwhich have to share just 16% of reads.\n\n\nIt is possible to sequence through a low-cellularity sample when, for\nexample, there is no way to take another sample (as is the case of most\nbiopsies). \nSequencing through\n means to simply sequence the tumour at a\nmuch higher coverage, usually 90-120X. This will mean that there will be\na net increase in reads supplying evidence for copy number events and\nvariants and in aggregate these will still retain power to infer these\nevents when using tools that look at the whole genome like Sequenza\ndoes.\n\n\nReferences\n\n\n\n\nF. Favero, T. Joshi, A. M. Marquard, N. J. Birkbak, M. Krzystanek,\n    Q. Li, Z. Szallasi, and A. C. Eklund. \nSequenza: allele-specific\n    copy number and mutation profiles from tumor sequencing data\n.\n    Annals of Oncology, 2015, vol. 26, issue 1, 64-70.", 
            "title": "Copy Number Variation"
        }, 
        {
            "location": "/modules/cancer-module-cnv/cnv-tut/#key-learning-outcomes", 
            "text": "After completing this practical the trainee should be able to:    Understand and perform a simple copy number variation analysis on\n    NGS data    Become familiar with Sequenza    Understand the CNV inference process as an interplay between depth\n    of sequencing, cellularity and B-allele frequency    Visualize CNV events by manual inspection", 
            "title": "Key Learning Outcomes"
        }, 
        {
            "location": "/modules/cancer-module-cnv/cnv-tut/#resources-youll-be-using", 
            "text": "", 
            "title": "Resources You\u2019ll be Using"
        }, 
        {
            "location": "/modules/cancer-module-cnv/cnv-tut/#tools-used", 
            "text": "Sequenza:  http://www.cbs.dtu.dk/biotools/sequenza/  IGV:  http://www.broadinstitute.org/igv/", 
            "title": "Tools Used"
        }, 
        {
            "location": "/modules/cancer-module-cnv/cnv-tut/#sources-of-data", 
            "text": "http://sra.dnanexus.com/studies/ERP001071  http://www.ncbi.nlm.nih.gov/pubmed/22194472", 
            "title": "Sources of Data"
        }, 
        {
            "location": "/modules/cancer-module-cnv/cnv-tut/#introduction", 
            "text": "The goal of this hands-on session is to perform a copy number variation\nanalysis (CNV) on a normal/tumour pair of alignment files (BAMs)\nproduced by the mapping of Illumina short read sequencing data.  To ensure reasonable analysis times, we will perform the analysis on a\nheavily subset pair of BAM files. These files contain just the first\n60Mb of chromosome 5 but contain several examples of inferred copy\nnumber events to enable interpretation and visualisation of the copy\nnumber variation that is present in entire cancer genomes. Sequenza is\nthe tool we will use to perform this analysis. It consists of a two\nPython pre-processing steps followed by a third step in R to infer the\ndepth ratio, cellularity, ploidy and to plot the results for\ninterpretation.  In the second part of the tutorial we will also be using IGV to\nvisualise and manually inspect the copy number variation we inferred in\nthe first part for validation purposes. This section will also include a\ndiscussion on the importance of good quality data by highlighting the\ninadequacies of the workshop dataset and the implications this has on\nanalysis results.", 
            "title": "Introduction"
        }, 
        {
            "location": "/modules/cancer-module-cnv/cnv-tut/#prepare-the-environment", 
            "text": "We will use a dataset derived from whole genome sequencing of a\n33-yr-old lung adenocarcinoma patient, who is a never-smoker and has no\nfamilial cancer history.  The data files are contained in the subdirectory called  data  and are\nthe following:  normal.chr5.60Mb.bam  and  normal.chr5.60Mb.bam.bai \n:   \\  tumour.chr5.60Mb.bam  and  tumour.chr5.60Mb.bam \n:   \\  These files are based on subsetting the whole genomes derived from blood\nand liver metastases to the first 60Mb of chromosome 5. This will allow\nour analyses to run in a sufficient time during the workshop, but it\u2019s\nworth being aware that we are analysing just 1.9% of the genome which\nwill highlight the length of time and resources required to perform\ncancer genomics on full genomes!  Open the Terminal and go to the  CNV  working directory:  1 cd cnv/   All commands entered into the terminal for this tutorial should be from\nwithin the  cnv  directory.  Check that the  data  directory contains the above-mentioned files by\ntyping:  1 ll data   All commands used in this tutorial can be copy/pasted from the\ncommands.sh file in the  cnv  directory.", 
            "title": "Prepare the Environment"
        }, 
        {
            "location": "/modules/cancer-module-cnv/cnv-tut/#sequenza-cnv-analysis", 
            "text": "Sequenza is run in three steps. The first pre-processing step is run on\nthe final normal and tumour mapped data (BAM files) in order to walk the\ngenome in a pileup format (automatically generated by samtools). This\nfirst step finds high quality sites in the genomes and extracts their\ndepth and genotype in the normal genome and calculates the variant\nalleles and allele frequencies in the tumour genome. The second step is\nto perform a binning on these sites to save space and analysis time in\nthe third step. Finally, the third step is run in R normalise the depth\nratio between the normal/tumour genomes, infer cellularity and ploidy\nand graphically output results for interpretation.", 
            "title": "Sequenza CNV Analysis"
        }, 
        {
            "location": "/modules/cancer-module-cnv/cnv-tut/#step-1-pre-processing-walking-the-genome", 
            "text": "1 pypy software/sequenza/sequenza-utils.py bam2seqz -n data/normal.chr5.60Mb.bam -t data/tumour.chr5.60Mb.bam --fasta assets/human\\_g1k\\_v37.fasta -gc assets/human\\_g1k\\_v37.gc50Base.txt.gz -C 5:1-60000000 | gzip   stage1.seqz.gz   Hint: press tab after typing a few characters of a directory of filename\nto auto-complete the rest. This makes entering long file names very\nquick.  Explanation of parameters  -n\n:   the normal BAM  -t\n:   the tumour BAM  \u2013fasta\n:   the reference genome used for mapping (b37 here)  -gc\n:   GC content as windows through the genome (pre-generated and\n    downloadable from the Sequenza website)  -C\n:   specifies the genomic location to process  There will not be any indication that it is running once you launch the\ncommand, to make sure it is running open a new Terminal tab with Shift +\nControl + T (or from the menu with File then Open Tab) and type the\ncommand \u2019top\u2019. You should see the top line being the command \u2019pypy\u2019 with\na % CPU usage of 98/99%. Press q to quit out of this process view and go\nback to the tab running Sequenza. If everything is running correctly, it\nwill take approximately 40 minutes to run. Go have a coffee!  Once the command is done you will be returned to the terminal prompt.\nMake sure the output file is the correct size by typing \u2019ll -h\u2019 from the\nTerminal window that you ran Sequenza from, there should be a file\ncalled  stage1.seqz.gz  of the size 326M.  You can look at the first few lines of the output in the file stage1.seqz.gz  with:  1 zcat stage1.seqz.gz | head -n 20   This output has one line for each position in the BAMs and includes\ninformation on the position, depths, allele frequencies, zygosity, GC in\nthe location.", 
            "title": "Step 1: Pre-Processing \u2013 Walking the Genome"
        }, 
        {
            "location": "/modules/cancer-module-cnv/cnv-tut/#step-2-perform-binning", 
            "text": "The binning step takes the rows of genomic positions and compresses them\ndown to 1 row for every 200 rows previously. This massively reduces the\nfile size and processing time in the third step.  1 pypy software/sequenza/sequenza-utils.py seqz-binning -w 200 -s stage1.seqz.gz | gzip   stage2.seqz.gz   Explanation of parameters  -w\n:   the window size (typically 50 for exomes, 200 for genomes)  -s\n:   the large seqz file generated in the first step  This step should take approximately 4 minutes to complete.", 
            "title": "Step 2: Perform Binning"
        }, 
        {
            "location": "/modules/cancer-module-cnv/cnv-tut/#step-3-running-sequenza-algorithms-and-plotting-results", 
            "text": "We will now perform the CNV analysis and output the results using the R\npart of Sequenza.  Open the R terminal  1 R   You should now see the R prompt identified with  > .  Run the Sequenza R commands  1\n2\n3\n4\n5\n6 library ( sequenza )  setwd ( /home/trainee/cnv ) \ndata.file  -   stage2.seqz.gz \nseqzdata  -  sequenza.extract ( data.file ) \nCP.example  -  sequenza.fit ( seqzdata ) \nsequenza.results ( sequenza.extract  =  seqzdata ,  cp.table  =  CP.example ,  sample.id  =   CanGenWorkshop ,  out.dir = sequenza_results )    If every command ran successfully, you will now have a sequenza_results  folder containing 13 files.  Quit R  1\n2 q()\nThen n at the  Save workspace image  prompt", 
            "title": "Step 3: Running Sequenza Algorithms and Plotting Results"
        }, 
        {
            "location": "/modules/cancer-module-cnv/cnv-tut/#sequenza-analysis-results-and-visualisation", 
            "text": "One of the first and most important estimates that Sequenza provides is\nthe tumour cellularity (the estimated percentage of tumour cells in the\ntumour genome). This estimate is based on the B allele frequency and\ndepth ratio through the genome and is an important metric to know for\ninterpretation of Sequenza results and for other analyses. Lets look at\nthe cellularity estimate for our analysis by opening\nCanGenWorkshop_model_fit.pdf with the command:  1 evince sequenza_results/CanGenWorkshop_model_fit.pdf   The cellularity estimate is at the top along with the average ploidy\nestimate and the standard deviation of the B allele frequency. We can\nsee that the cellularity has been estimated at 24% which is fairly low\nand we will see why this is bad in the next section on CNV\nvisualisation. The ploidy value of 2.1 indicates this piece of the\ngenome is not hugely amplified or deleted and the BAF standard deviation\nindicates there are no significant long losses of heterozygosity.  Close the PDF window to resume the Terminal prompt.  Let\u2019s now look at the CNV inferences through our genomic block. Open the\ngenome copy number visualisation file with:  1 evince sequenza_results/CanGenWorkshop_genome_view.pdf   This file contains three  pages  of copy number events through the\nentire genomic block. The first page shows copy numbers of the A (red)\nand B (blue) alleles, the second page shows overall copy number changes\nand the third page shows the B allele frequency and depth ratio through\ngenomic block. Looking at the overall copy number changes, we see that\nour block is at a copy number of 2 with a small duplication to copy\nnumber 4 about   of the way through the block and another just after\nhalfway through the block. There is also a reduction in copy number to 1\ncopy about   of the way through the block. The gap that you see just\nbefore this reduction in copy number is the chromosomal centromere - an\narea that is notoriously difficult to sequence so always ends up in a\ngap with short read data.  You can see how this is a very easy to read output and lets you\nimmediately see the frequency and severity of copy number events through\nyour genome. Let\u2019s compare the small genomic block we ran with the same\noutput from the entire genome which has been pre-computed for you. This\nis located in the  pre_generated/results_whole_genome  folder and\ncontains the same 13 output files as for the small genomic block. As\nbefore, let\u2019s look at the cellularity estimate with:  1 evince pre_generated/results_whole_genome/CanGenWorkshop_model_fit.pdf   It now looks like it\u2019s even worse at just 16%! A change is to be\nexpected as we were only analysing 1.9% of the genome. Let\u2019s now look at\nthe whole genome copy number profile with:  1 evince pre_generated/results_whole_genome/CanGenWorkshop_genome_view.pdf   You can see that there are a number of copy number events across the\ngenome and our genomic block (the first 60Mb of chromosome 5) is\ninferred as mostly copy number 4 followed by a reduction to copy number\n2, rather than 2 to 1 as we saw in the output we generated. The reason\nfor this is that Sequenza uses the genome-wide depth ratio and BAF in\norder to estimate copy number, if you ask it to analyse a small block\nmostly at copy number 4 with a small reduction to copy number 2, the\nmost likely scenario in lieu of more data is that this is a copy number\n2 block with a reduction to 1. It\u2019s important to carefully examine the\ncellularity, ploidy and BAF estimates of your sample along with the\nplots of model fit (CanGenWorkshop_model_fit.pdf) and\ncellularity/ploidy contours (CanGenWorkshop_CP_contours.pdf) in order\nto decide if you believe Sequenza\u2019s inference of the copy numbers. Have\na look at these for yourself if you want to get a better idea of how\nSequenza makes its inferences and conclusions.", 
            "title": "Sequenza Analysis Results and Visualisation"
        }, 
        {
            "location": "/modules/cancer-module-cnv/cnv-tut/#cnv-visualisationconfirmation-in-igv", 
            "text": "Let\u2019s see if we can visualise one of the CNV events where copy number\nincreased significantly. We\u2019ll focus on the copy number 4 event seen at\nabout   of the way through the CanGenWorkshop_genome_view.pdf output\nwe generated. First, we need to find the coordinates that have been\npredicted for this event. Have a look at the\nCanGenWorkshop_segments.txt file in the results folder to view all\npredicted CNV events with:  1 less sequenza_results/CanGenWorkshop_segments.txt   There is only one at a copy number of 4 (CNt column) and it starts at\n21051700 to 21522065 which is 470kb and corresponds to the small block\nwe see in the genome view PDF.  Quit out of viewing the segments file by pressing q.  We will now open IGV and see if we can observe the predicted increase in\ncopy number within these genomic coordinates.  1 /home/trainee/snv/Applications/igv/igv.sh   IGV will take 30 seconds or so to open so just be patient.  For a duplication of this size, we will not be able to easily observe it\njust by looking at the raw read alignments. In order to see it we will\ngenerate two tiled data files (TDFs) within IGV which contain the\naverage read depth for a given window size through the genome. This\nmeans that we can aggregate the average read depth over relatively large\nchunks of the genome and compare these values between the normal and\ntumour genomes.  To begin, we will go to  Tools  then  Run igvtools  in the IGV\nmenubar. Specify the normal bam file (under  cnv  then  data ) as the\ninput file and change the window size to 100000 (one hundred thousand).\nThen press the  Run  button and IGV will make the TDF file. This takes\nabout 5 minutes. Repeat this for the tumour genome.  After you have both TDF files, go to  File  and  Load from file  in\nthe menubar and select the BAM and TDF files to open. Once you have\nopened them, they will appear as tracks along with the BAM tracks we\nloaded initially. Navigate to the genomic coordinates of our event\n(5:21,051,700-21,522,065) by typing it in the coordinate box at the top.\nMouse over the two blue tracks to get the average depth values for the\n100,000bp windows. What you should see is that the liverMets sample has\n3-6X more coverage than the Blood sample for the four windows that cover\nthis region.  This may seem a bit underwhelming, after all, wasn\u2019t the increase of the\nregion to a copy number of 4, i.e. we expect a doubling of reads in the\ntumour? To explain why we are only seeing such a small coverage\nincrease, we need to turn to our good friend mathematics!  Imagine we have two 30X genomes for the normal and tumour samples and\nthe tumour is at 100% purity. If there is a copy number increase to 4 in\nthe tumour from 2 in the normal, the duplicated segment should indeed\nhave twice as many reads as the same segment in the normal genome. Now,\nlets imagine the tumour genome was only at a purity of 50% (i.e. it\ncontains 50% normal cells and 50% tumour cells). Now, half of the\nduplicated  tumour genome  segment will be at a copy number of 2 and\nhalf will be at 4. What does this mean when we sequence them as a\nmixture? The resulting average copy number of the block will be\n$(0.5*2)+(0.5*4) = 3$. Now what if we only have 16% tumour cells in our tumour genome ? This will be $(0.84*2)+(0.16*4) = 2.32$. You can see\nhow sequencing a low cellularity tumour at a low depth makes it much\nharder to infer copy number variations!  Returning to our genomes at hand, when we previously looked at the\ncellularity estimate of this tumour we saw it was 20% from the small\nblock we ran or 16% from the whole genome. Thus, the read depth increase\nof just 3-6X (about 10-20% more reads) in this segment is not\nsurprising. A low cellularity tumour greatly reduces our power to infer\ncopy number events as relatively small changes in depth can occur by\nchance in the genome and these can be mis-identified as copy number\nchanges. As well as this, it reduces our power for other analyses since\nwe must also remember that a tumour can itself contain multiple clones\nwhich have to share just 16% of reads.  It is possible to sequence through a low-cellularity sample when, for\nexample, there is no way to take another sample (as is the case of most\nbiopsies).  Sequencing through  means to simply sequence the tumour at a\nmuch higher coverage, usually 90-120X. This will mean that there will be\na net increase in reads supplying evidence for copy number events and\nvariants and in aggregate these will still retain power to infer these\nevents when using tools that look at the whole genome like Sequenza\ndoes.", 
            "title": "CNV Visualisation/Confirmation in IGV"
        }, 
        {
            "location": "/modules/cancer-module-cnv/cnv-tut/#references", 
            "text": "F. Favero, T. Joshi, A. M. Marquard, N. J. Birkbak, M. Krzystanek,\n    Q. Li, Z. Szallasi, and A. C. Eklund.  Sequenza: allele-specific\n    copy number and mutation profiles from tumor sequencing data .\n    Annals of Oncology, 2015, vol. 26, issue 1, 64-70.", 
            "title": "References"
        }, 
        {
            "location": "/modules/cancer-module-sv/sv_tut/", 
            "text": "Key Learning Outcomes\n\n\nBy the end of the structural variant (SV) detection practical course\nparticipants will:\n\n\n\n\n\n\nHave been provided with key fundamentals on how paired-end mappings\n    and split-read/soft-clipped read patterns are used in detecting\n    deletions, tandem duplicates, inversions and translocations.\n\n\n\n\n\n\nKnow what important quality control checks need to be evaluated\n    prior to structural variant calling.\n\n\n\n\n\n\nHave run DELLY on a subset of whole genome next generation\n    sequencing data pertaining to a single human tumour with a matched\n    normal control.\n\n\n\n\n\n\nBe able to filter high confidence SV predictions.\n\n\n\n\n\n\nHave gained basic knowledge to interpret the VCF output provided by\n    DELLY.\n\n\n\n\n\n\nHave used their understanding of distinct SV paired-end mapping and\n    soft-clipped read patterns to visually verify DELLY predicted SVs\n    using IGV.\n\n\n\n\n\n\nResources You\u2019ll be Using\n\n\nTools Used\n\n\nDELLY:\n\n\nhttps://github.com/tobiasrausch/delly\n\n\nSamtools:\n\n\nhttp://sourceforge.net/projects/samtools/files/samtools/1.2\n\n\nTabix:\n\n\nhttp://sourceforge.net/projects/samtools/files/tabix/tabix-0.2.6.tar.bz2\n\n\nVcftools:\n\n\nhttps://vcftools.github.io/index.html\n\n\nPicard:\n\n\nhttps://github.com/broadinstitute/picard\n\n\nPython2.7.10:\n\n\nhttps://www.python.org/downloads/release/python-2710/\n\n\nPyVCF Banyan numpy:\n\n\nhttps://pypi.python.org/pypi\n\n\nUseful Links\n\n\nSAM Specification:\n\n\nhttp://samtools.sourceforge.net/SAM1.pdf\n\n\nExplain SAM Flags:\n\n\nhttp://picard.sourceforge.net/explain-flags.html\n\n\nAlignment Quality Control\n\n\nFor structural variant calling several alignment quality control metrics\nshould be evaluated. All paired-end mapping methods heavily rely on the\ninsert size distribution. GC-content biases are important as it can\nimpact read-depths. DELLY generates read-depth ratios between tumour and\ncontrol samples. The percentage of mapped reads, singletons, duplicates\nand properly paired reads are additional metrics you should evaluate\nprior to any structural variant calling. These statistics vary largely\nby protocol and hence, it is usually best to compare multiple different\nsequencing runs using the same against each other to highlight outliers.\n\n\nIt is recommended that Picard module commands \nCollectInsertSizeMetrics\n\nand \nCollectGcBiasMetrics\n, and \nsamtools flagstat\n command be used.\n\n\nPrepare the Environment\n\n\nAs a quick introduction we will do a structural variant analysis using a\nsingle immortal cancer cell line and its control genome (mortal parental\ncells). Total execution time to run the DELLY structural discovery\ncalling program for a matched normal tumour pair will vary depending on\nthe read coverage and the size of the genome. As a guide, it can take\napproximately 10 to 50 hours (translocation predictions taking the\nlongest), for a matched normal tumour pair (each 40-50x coverage)\nrunning on 2 cpus on a server with sufficient RAM.\n\n\nThe bam files we will be working on are a subset of the original WGS bam\nfiles, limited to specific chromosomal regions to speed up the analysis\nand to meet the time constraints for this practical.\n\n\nFirstly, we will use shell variables to help improve the readability of\ncommands and streamline scripting. Each distinct variable will store a\ndirectory path to either, the input WGS bam files, hg19 reference,\nprograms or output.\n\n\nOpen the Terminal.\n\n\nFirst, go to the right folder, where the data are stored.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\ncd /home/trainee/sv\nls\nmkdir \nYourFirstName\n\ncd \nYourFirstName\n\n\nexport DS=/home/trainee/sv/data\nexport RF=/home/trainee/sv/reference_data\nexport SF=/home/trainee/sv/variantFiltering/somaticVariants\nexport BR=/home/trainee/snv/Applications/igv\nexport CZ=/home/trainee/sv/converter\n\n\n\n\n\n\nSomatic Structural Variant Discovery using DELLY\n\n\nIn order to generate \nputative\n somatic SVs it is crucial to account for\ngermline SVs. To facilitate, DELLY requires the joint input of a match\nnormal control and the cancer aligned sequencing data (bam files).\n\n\n1\n2\n3\n4\ndelly -t DEL -x $RF/hg19.excl -o del.vcf -g $RF/hg19.fa $DS/cancer_cell_line.bam $DS/control.bam\ndelly -t DUP -x $RF/hg19.excl -o dup.vcf -g $RF/hg19.fa $DS/cancer_cell_line.bam $DS/control.bam\ndelly -t INV -x $RF/hg19.excl -o inv.vcf -g $RF/hg19.fa $DS/cancer_cell_line.bam $DS/control.bam\ndelly -t TRA -x $RF/hg19.excl -o tra.vcf -g $RF/hg19.fa $DS/cancer_cell_line.bam $DS/control.bam\n\n\n\n\n\n\nDescription of the arguments used in the command:\n\n\nDEL\n:   = conduct deletion discovery\n\n\nDUP\n:   = conduct tandem duplication discovery\n\n\nINV\n:   = conduct inversion discovery\n\n\nTRA\n:   = conduct translocation discovery\n\n\n-o\n:   = vcf output\n\n\n-g\n:   = reference genome in FASTA format\n\n\n-x\n:   = genomic regions to exclude (e.g. centro- and telomeric regions)\n\n\nDELLY VCF output\n\n\nA VCF file has multiple header lines starting with the hash # sign.\nBelow the header lines is one record for each structural variant. The\nrecord format is described in the below table:\n\n\n[H]\n\n\nlll\n\n\nColumn \n Field \n Description\\\n1 \n CHROM \n Chromosome name\\\n2 \n POS \n 1-based position. For an indel, this is the position preceding\nthe indel\\\n3 \n ID \n Variant identifier\\\n4 \n REF \n Reference sequence at POS involved in the variant\\\n5 \n ALT \n Comma delimited list of alternative sequence(s)\\\n6 \n QUAL \n Phred-scaled probability of all samples being homozygous\nreference\\\n7 \n FILTER \n Semicolon delimited list of filters that the variant fails\nto pass\\\n8 \n INFO \n Semicolon delimited list of variant information\\\n9 \n FORMAT \n Colon delimited list of the format of sample genotypes in\nsubsequent fields\\\n10+ \n \n Individual genotype information defined by FORMAT\\\n\n\n[tab:vcfoutput]\n\n\nYou can look at the header of the vcf file using grep, \n-A 1\n includes\nthe first structural variant record in the file:\n\n\n1\ngrep \n^#\n -A 1 del.vcf\n\n\n\n\n\n\nThe INFO field holds structural variant site information whereas all\ngenotype information (annotated as per the FORMAT fields) is provided in\nthe sample column. Reference supporting reads are compared to\nalternative supporting reads and mapping qualities are used to compute\ngenotype likelihoods (GL) for homozygous reference (0/0), heterozygous\nreference (0/1) and homozygous alternate (1/1) (GT). The final genotype\n(GT) is simply derived from the best GL and GQ is a phred-scaled\ngenotype quality reflecting the confidence in this genotype. If GQ\\\n15\nthe genotype is flagged as LowQual. The genotyping takes into account\nall paired-ends with a mapping quality greater than 20 by default.\n\n\nThe INFO field provides information on the quality of the SV prediction\nand breakpoints. If you browse through the vcf file you will notice that\na subset of the DELLY structural variant predictions have been refined\nusing split-reads. These precise variants are flagged in the vcf info\nfield with the tag \nPRECISE\n. To count the number of precise and\nimprecise variants you can simply use grep.\n\n\n1\n2\ngrep -c -w \nPRECISE\n *.vcf\ngrep -c -w \nIMPRECISE\n *.vcf\n\n\n\n\n\n\nDELLY clusters abnormal paired-ends and every single cluster gives rise\nto an IMPRECISE SV call. For every IMPRECISE SV call an attempt is made\nto identify supporting split-reads/soft-clipped reads DELLY then\ncomputes a consensus sequence (INFO:CONSENSUS) out of all split-read\ncandidates and then aligns this consensus sequence to the reference\nrequiring at least -m many aligned bases to the left and right (default\nis 13). INFO:PE is the number of supporting paired-ends. INFO:CT refers\nconnection types (CT), which indicates the order and orientation of\npaired-end cluster mappings (e.g. 3to3 for 3\u2019 to 3\u2019 and 5to5 for 5\u2019 to\n5\u2019). Values can be 3to5, 5to3, 3to3 or 5to5. Different names exist for\nthese connection types in the literature, head-to-head inversions,\ntail-to-tail inversions, and so on. The consensus alignment quality\n(SRQ) is a score between 0 and 1, where 1 indicates 100% identity to the\nreference. Nearby SNPs, InDels and micro-insertions at the breakpoint\ncan lower this score but only for mis-assemblies it should be very poor.\nDELLY currently drops consensus alignments with a score\\\n0.8 and then\nfalls back to an IMPRECISE prediction.\n\n\nSVs are flagged as FILTER:LowQual if PE\\\n3 OR MAPQ\\\n20 (for\ntranslocations: PE\\\n5 OR MAPQ\\\n20), otherwise, the SV results in a\nFILTER:PASS. PRECISE variants will have split-read support (SR>0).\n\n\nSomatic Structural Variant Filtering\n\n\nPlease note that this vcf file contains germline and somatic structural\nvariants but also false positives caused by repeat induced mis-mappings\nor incomplete reference sequences. As a final step we have to use the\nstructural variant site information and the cancer and normal genotype\ninformation to filter a set of confident somatic structural variants.\nDELLY ships with a somatic filtering python script. For a set of\nconfident somatic calls one could exclude all structural variants\n\\\n400bp, require a minimum variant allele frequency of 10%, no support\nin the matched normal and an overall confident structural variant site\nprediction with the VCF filter field being equal to PASS.\n\n\n1\n2\n3\n4\npython $SF/somaticFilter.py -t DEL  -T cancer_cell_line -N control -v del.vcf -o del.filt.vcf -a 0.1 -m 400 -f\npython $SF/somaticFilter.py -t DUP -T cancer_cell_line -N control -v dup.vcf -o dup.filt.vcf -a 0.1 -m 400 -f\npython $SF/somaticFilter.py -t INV  -T cancer_cell_line -N control -v inv.vcf -o inv.filt.vcf -a 0.1 -m 400 -f\npython $SF/somaticFilter.py -t TRA -T cancer_cell_line -N control -v tra.vcf -o tra.filt.vcf -a 0.1 -m 400 -f\n\n\n\n\n\n\nUsing VCFtools we can merge all somatic structural variants together in\na single vcf file.\n\n\n1\nvcf-concat del.filt.vcf dup.filt.vcf inv.filt.vcf tra.filt.vcf | vcf-sort \n somatic.sv.vcf\n\n\n\n\n\n\nFor large VCF files you should also zip and index them using bgzip and\ntabix. Please run the below commands to meet the requirements for\nvisualising somatic structural variants using IGV.\n\n\n1\n2\nbgzip somatic.sv.vcf\ntabix somatic.sv.vcf.gz\n\n\n\n\n\n\nVisualisation of Somatic Structural Variants\n\n\nThe final step will be to browse some of these somatic structural\nvariants in IGV and to visually verify the reliability of the calls. VCF\nfile structure was designed by the 1000Genomes Project consortium with\nconsiderable focus being placed on single-nucleotide variants and\nInDels. It is arguable whether the file format is easy to interpret for\nbasic SVs and much less for complex SV formatting. To make it easier to\nsee the breakpoints we will create a bed file.\n\n\n1\n2\n$CZ/sv.vcf2bed.sh somatic.sv.vcf.gz \n somatic.sv.bed\nhead somatic.sv.bed\n\n\n\n\n\n\nLoad the IGV browser\n\n\n1\n$BR/igv.sh\n\n\n\n\n\n\nOnce IGV has started use \nFile\n and \nLoad from File\n to load the\ncancer_cell_line.bam. To dedicate more of the screen to read pile ups,\nright click on the left hand box that contains the\n\ncancer_cell_line.bam Junctions\n then select \nRemove Track\n. Go to\n\nView\n menu and select \nPreferences\n, then click on the Alignments tab\nand in the \nVisibility range threshold (kb)\n text box, enter 600. This\nwill allow you to increase your visibility of pile ups as you zoom out.\nNow look for check box for \nFilter secondary alignments\n click on to\nensure we do not see secondary alignments (alternate mapped position of\na read) . Also ensure that \nShow soft-clipped bases\n has been checked\nthen click \nOK\n.\n\n\nThen import \nsomatic.sv.bed\n from your working directory using \nRegions\n\nand \nImport Regions\n.\n\n\nVerify Deletion\n\n\nThe somatic structural variants can then be browsed easily using the\n\nRegion Navigator\n. Select the deletion (chrX:76853017-77014863) from\nthe \nRegion Navigator\n and click \nView\n. This will centre the IGV\nalignment view on the selected structural variant. Close the regions of\ninterest pop up window. The red bar below the ruler marks the region of\nthe deletion.\n\n\nIt\u2019s usually best to zoom out once by clicking on the \u2019-\u2019 sign in the\ntoolbar at the top, to give a wide view of the supporting abnormal\npaired-end read mappings.\n\n\nTo highlight the abnormal paired-ends right click on the main track\ndisplay panel and select \nColor alignments by\n and then switch to\n\ninsert size and pair orientation\n.\n\n\nRead pairs that have a larger than expected insert size will be\nhighlighted in red. Click \nView as pairs\n. Right click and \nSort\nalignments by\n then select \nstart location\n.\n\n\nHow many abnormal \npaired-end read pairs\n (red coloured F/R oriented\nread pairs) can you see that spans the deletion region? Does this number\ncoincide with the INFO:PE?\n\n\n19, YES\n\n\nZoom into left breakpoint and tally the number of soft-clipped reads at\nboth junctions. How many abnormal split-reads (soft-clipped reads) did\nyou observe? Clue INFO:SR.\n\n\n11\n\n\nGo to the RefSeq genes track at the bottom of IGV and right click to\n\nExpanded\n. Is the predicted deletion likely to have a deleterious\nimpact on a gene? If so, what gene and exons are deleted?\n\n\nATRX, exons 2 to 25.\n\n\nDoes this region appear to be completely removed from this cancer\ngenome? How can you tell?\n\n\nYes, there is no read coverage within this deletion region relative to\nthe control genome.\n\n\nVerify translocation\n\n\nSelect the duplication (chrX:45649874-45689322) from the \nRegion\nNavigator\n.\n\n\nHighlight the abnormal paired-ends by clicking and selecting \nColor\nalignments by\n and then switch to \ninsert size and pair orientation\n.\nAlso, invoke \nSort alignments by\n then select \nstart location\n.\n\n\nZoom out until you can see all the purple reads at the junction.\n\n\nWhat is the direction of the purple cluster of reads (indicates that\nmate reads are mapped to Chr15)? Is it pointing to the tail or head of\nChr18?\n\n\nForward, towards the tail, or 3\u2019 (+ive)\n\n\nRight click on to one of the purple coloured reads and select \nView mate\nregion in split-screen\n. This will split the screen and display Chr15 on\nthe left and place a red highlighted outline on both reads, to indicate\nthe pairs. Right click and \nSort alignments by\n then select \nstart\nlocation\n. What is the direction of the yellow cluster of reads\n(indicates that mate reads are mapped to Chr18)? Is it pointing to the\ntail or head of Chr15? If you wish to zoom in or out, first click inside\nof the chromosome ideogram panel, then ctrl- to zoom out and shift+ to\nzoom in.\n\n\n? Reverse, towards the head, or 5\u2019 (+ive)\n\n\nHow is the Chr15 and Chr18 fused (which one of the four translocation\nconnection types)? If you are uncertain then run a BLAT search using the\nINFO:CONSENSUS sequence (gedit somatic.sv.vcf).\n\nhttps://genome.ucsc.edu/cgi-bin/hgBlat?command=start\n\n\nRF, head to tail, or 5 to 3. Therefore, Chr18 left side is fused to\nChr15 right side.\n\n\nDid DELLY predict a reciprocal translocation? How can you tell?\n\n\nNo, as we would expect to observe a Chr18 right side fused to Chr15 left\nside, near the same breakpoints.\n\n\nWhat gene structures is this translocation predicted to impact?\n\n\nPARD6G on Chr15 and ADAMTSL3 on Chr18.\n\n\nWhat is one possible reason why there is no observable read coverage\nafter Chr18 breakpoint?\n\n\nChromosome loss.\n\n\nVerify tandem duplication\n\n\nSelect the tandem duplication (chrX:45649874-45689322) from the \nRegion\nNavigator\n.\n\n\nHighlight the abnormal paired-ends by clicking and selecting \nColor\nalignments by\n and then switch to \ninsert size and pair orientation\n.\nAlso, invoke \nSort alignments by\n then select \nstart location\n.\n\n\nZoom out until you can see all the red paired-end reads spanning the two\njunctions. After that zoom in on the cluster of abnormal reads on the\nleft junction and then right junction.\n\n\nWhich is the order and orientation of these paired-end reads (FR, RF, FF\nor RR)?\n\n\nRF\n\n\nWhat is the estimated read-depth ratio of the cancer_cell_line versus\nnormal control (INFO:RDRATIO) over the duplicate region?\n\n\n3.3 (\u00a03 x increased read depth)\n\n\nVerify Inversion\n\n\nType into the search box near at tool bar, Chr20:54834492\n\n\nRight click on main display and select \nGroup alignments by\n then switch\non \npaired-orientation\n. Also, right click and \nSort alignments by\n then\nselect \nstart location\n.\n\n\nZoom out until you can see all the red coloured cluster of reads near\nthe breakpoint.\n\n\nRight click on to one of the red coloured reads and select \nView mate\nregion in split-screen\n. This will split the screen and display the read\nmate on the right side. This will take you to the mate-reads near the\nsecond breakpoint.\n\n\nWhich direction are the paired-end reads spanning (left or right\nspanning)?\n\n\nRight\n\n\nWhat is the estimated size of the inverted interval?\n\n\n55,408,660 \u2013 54,834,492 = 574,168 bp\n\n\nAcknowledgements\n\n\nWe would like to thank and acknowledge Tobias Rausch (EMBL Heidelberg)\nfor his help and for allowing us to borrow and adapt his replies to\nquestions and original course material.", 
            "title": "Structural Variant Analysis"
        }, 
        {
            "location": "/modules/cancer-module-sv/sv_tut/#key-learning-outcomes", 
            "text": "By the end of the structural variant (SV) detection practical course\nparticipants will:    Have been provided with key fundamentals on how paired-end mappings\n    and split-read/soft-clipped read patterns are used in detecting\n    deletions, tandem duplicates, inversions and translocations.    Know what important quality control checks need to be evaluated\n    prior to structural variant calling.    Have run DELLY on a subset of whole genome next generation\n    sequencing data pertaining to a single human tumour with a matched\n    normal control.    Be able to filter high confidence SV predictions.    Have gained basic knowledge to interpret the VCF output provided by\n    DELLY.    Have used their understanding of distinct SV paired-end mapping and\n    soft-clipped read patterns to visually verify DELLY predicted SVs\n    using IGV.", 
            "title": "Key Learning Outcomes"
        }, 
        {
            "location": "/modules/cancer-module-sv/sv_tut/#resources-youll-be-using", 
            "text": "", 
            "title": "Resources You\u2019ll be Using"
        }, 
        {
            "location": "/modules/cancer-module-sv/sv_tut/#tools-used", 
            "text": "DELLY:  https://github.com/tobiasrausch/delly  Samtools:  http://sourceforge.net/projects/samtools/files/samtools/1.2  Tabix:  http://sourceforge.net/projects/samtools/files/tabix/tabix-0.2.6.tar.bz2  Vcftools:  https://vcftools.github.io/index.html  Picard:  https://github.com/broadinstitute/picard  Python2.7.10:  https://www.python.org/downloads/release/python-2710/  PyVCF Banyan numpy:  https://pypi.python.org/pypi", 
            "title": "Tools Used"
        }, 
        {
            "location": "/modules/cancer-module-sv/sv_tut/#useful-links", 
            "text": "SAM Specification:  http://samtools.sourceforge.net/SAM1.pdf  Explain SAM Flags:  http://picard.sourceforge.net/explain-flags.html", 
            "title": "Useful Links"
        }, 
        {
            "location": "/modules/cancer-module-sv/sv_tut/#alignment-quality-control", 
            "text": "For structural variant calling several alignment quality control metrics\nshould be evaluated. All paired-end mapping methods heavily rely on the\ninsert size distribution. GC-content biases are important as it can\nimpact read-depths. DELLY generates read-depth ratios between tumour and\ncontrol samples. The percentage of mapped reads, singletons, duplicates\nand properly paired reads are additional metrics you should evaluate\nprior to any structural variant calling. These statistics vary largely\nby protocol and hence, it is usually best to compare multiple different\nsequencing runs using the same against each other to highlight outliers.  It is recommended that Picard module commands  CollectInsertSizeMetrics \nand  CollectGcBiasMetrics , and  samtools flagstat  command be used.", 
            "title": "Alignment Quality Control"
        }, 
        {
            "location": "/modules/cancer-module-sv/sv_tut/#prepare-the-environment", 
            "text": "As a quick introduction we will do a structural variant analysis using a\nsingle immortal cancer cell line and its control genome (mortal parental\ncells). Total execution time to run the DELLY structural discovery\ncalling program for a matched normal tumour pair will vary depending on\nthe read coverage and the size of the genome. As a guide, it can take\napproximately 10 to 50 hours (translocation predictions taking the\nlongest), for a matched normal tumour pair (each 40-50x coverage)\nrunning on 2 cpus on a server with sufficient RAM.  The bam files we will be working on are a subset of the original WGS bam\nfiles, limited to specific chromosomal regions to speed up the analysis\nand to meet the time constraints for this practical.  Firstly, we will use shell variables to help improve the readability of\ncommands and streamline scripting. Each distinct variable will store a\ndirectory path to either, the input WGS bam files, hg19 reference,\nprograms or output.  Open the Terminal.  First, go to the right folder, where the data are stored.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 cd /home/trainee/sv\nls\nmkdir  YourFirstName \ncd  YourFirstName \n\nexport DS=/home/trainee/sv/data\nexport RF=/home/trainee/sv/reference_data\nexport SF=/home/trainee/sv/variantFiltering/somaticVariants\nexport BR=/home/trainee/snv/Applications/igv\nexport CZ=/home/trainee/sv/converter", 
            "title": "Prepare the Environment"
        }, 
        {
            "location": "/modules/cancer-module-sv/sv_tut/#somatic-structural-variant-discovery-using-delly", 
            "text": "In order to generate  putative  somatic SVs it is crucial to account for\ngermline SVs. To facilitate, DELLY requires the joint input of a match\nnormal control and the cancer aligned sequencing data (bam files).  1\n2\n3\n4 delly -t DEL -x $RF/hg19.excl -o del.vcf -g $RF/hg19.fa $DS/cancer_cell_line.bam $DS/control.bam\ndelly -t DUP -x $RF/hg19.excl -o dup.vcf -g $RF/hg19.fa $DS/cancer_cell_line.bam $DS/control.bam\ndelly -t INV -x $RF/hg19.excl -o inv.vcf -g $RF/hg19.fa $DS/cancer_cell_line.bam $DS/control.bam\ndelly -t TRA -x $RF/hg19.excl -o tra.vcf -g $RF/hg19.fa $DS/cancer_cell_line.bam $DS/control.bam   Description of the arguments used in the command:  DEL\n:   = conduct deletion discovery  DUP\n:   = conduct tandem duplication discovery  INV\n:   = conduct inversion discovery  TRA\n:   = conduct translocation discovery  -o\n:   = vcf output  -g\n:   = reference genome in FASTA format  -x\n:   = genomic regions to exclude (e.g. centro- and telomeric regions)", 
            "title": "Somatic Structural Variant Discovery using DELLY"
        }, 
        {
            "location": "/modules/cancer-module-sv/sv_tut/#delly-vcf-output", 
            "text": "A VCF file has multiple header lines starting with the hash # sign.\nBelow the header lines is one record for each structural variant. The\nrecord format is described in the below table:  [H]  lll  Column   Field   Description\\\n1   CHROM   Chromosome name\\\n2   POS   1-based position. For an indel, this is the position preceding\nthe indel\\\n3   ID   Variant identifier\\\n4   REF   Reference sequence at POS involved in the variant\\\n5   ALT   Comma delimited list of alternative sequence(s)\\\n6   QUAL   Phred-scaled probability of all samples being homozygous\nreference\\\n7   FILTER   Semicolon delimited list of filters that the variant fails\nto pass\\\n8   INFO   Semicolon delimited list of variant information\\\n9   FORMAT   Colon delimited list of the format of sample genotypes in\nsubsequent fields\\\n10+     Individual genotype information defined by FORMAT\\  [tab:vcfoutput]  You can look at the header of the vcf file using grep,  -A 1  includes\nthe first structural variant record in the file:  1 grep  ^#  -A 1 del.vcf   The INFO field holds structural variant site information whereas all\ngenotype information (annotated as per the FORMAT fields) is provided in\nthe sample column. Reference supporting reads are compared to\nalternative supporting reads and mapping qualities are used to compute\ngenotype likelihoods (GL) for homozygous reference (0/0), heterozygous\nreference (0/1) and homozygous alternate (1/1) (GT). The final genotype\n(GT) is simply derived from the best GL and GQ is a phred-scaled\ngenotype quality reflecting the confidence in this genotype. If GQ\\ 15\nthe genotype is flagged as LowQual. The genotyping takes into account\nall paired-ends with a mapping quality greater than 20 by default.  The INFO field provides information on the quality of the SV prediction\nand breakpoints. If you browse through the vcf file you will notice that\na subset of the DELLY structural variant predictions have been refined\nusing split-reads. These precise variants are flagged in the vcf info\nfield with the tag  PRECISE . To count the number of precise and\nimprecise variants you can simply use grep.  1\n2 grep -c -w  PRECISE  *.vcf\ngrep -c -w  IMPRECISE  *.vcf   DELLY clusters abnormal paired-ends and every single cluster gives rise\nto an IMPRECISE SV call. For every IMPRECISE SV call an attempt is made\nto identify supporting split-reads/soft-clipped reads DELLY then\ncomputes a consensus sequence (INFO:CONSENSUS) out of all split-read\ncandidates and then aligns this consensus sequence to the reference\nrequiring at least -m many aligned bases to the left and right (default\nis 13). INFO:PE is the number of supporting paired-ends. INFO:CT refers\nconnection types (CT), which indicates the order and orientation of\npaired-end cluster mappings (e.g. 3to3 for 3\u2019 to 3\u2019 and 5to5 for 5\u2019 to\n5\u2019). Values can be 3to5, 5to3, 3to3 or 5to5. Different names exist for\nthese connection types in the literature, head-to-head inversions,\ntail-to-tail inversions, and so on. The consensus alignment quality\n(SRQ) is a score between 0 and 1, where 1 indicates 100% identity to the\nreference. Nearby SNPs, InDels and micro-insertions at the breakpoint\ncan lower this score but only for mis-assemblies it should be very poor.\nDELLY currently drops consensus alignments with a score\\ 0.8 and then\nfalls back to an IMPRECISE prediction.  SVs are flagged as FILTER:LowQual if PE\\ 3 OR MAPQ\\ 20 (for\ntranslocations: PE\\ 5 OR MAPQ\\ 20), otherwise, the SV results in a\nFILTER:PASS. PRECISE variants will have split-read support (SR>0).", 
            "title": "DELLY VCF output"
        }, 
        {
            "location": "/modules/cancer-module-sv/sv_tut/#somatic-structural-variant-filtering", 
            "text": "Please note that this vcf file contains germline and somatic structural\nvariants but also false positives caused by repeat induced mis-mappings\nor incomplete reference sequences. As a final step we have to use the\nstructural variant site information and the cancer and normal genotype\ninformation to filter a set of confident somatic structural variants.\nDELLY ships with a somatic filtering python script. For a set of\nconfident somatic calls one could exclude all structural variants\n\\ 400bp, require a minimum variant allele frequency of 10%, no support\nin the matched normal and an overall confident structural variant site\nprediction with the VCF filter field being equal to PASS.  1\n2\n3\n4 python $SF/somaticFilter.py -t DEL  -T cancer_cell_line -N control -v del.vcf -o del.filt.vcf -a 0.1 -m 400 -f\npython $SF/somaticFilter.py -t DUP -T cancer_cell_line -N control -v dup.vcf -o dup.filt.vcf -a 0.1 -m 400 -f\npython $SF/somaticFilter.py -t INV  -T cancer_cell_line -N control -v inv.vcf -o inv.filt.vcf -a 0.1 -m 400 -f\npython $SF/somaticFilter.py -t TRA -T cancer_cell_line -N control -v tra.vcf -o tra.filt.vcf -a 0.1 -m 400 -f   Using VCFtools we can merge all somatic structural variants together in\na single vcf file.  1 vcf-concat del.filt.vcf dup.filt.vcf inv.filt.vcf tra.filt.vcf | vcf-sort   somatic.sv.vcf   For large VCF files you should also zip and index them using bgzip and\ntabix. Please run the below commands to meet the requirements for\nvisualising somatic structural variants using IGV.  1\n2 bgzip somatic.sv.vcf\ntabix somatic.sv.vcf.gz", 
            "title": "Somatic Structural Variant Filtering"
        }, 
        {
            "location": "/modules/cancer-module-sv/sv_tut/#visualisation-of-somatic-structural-variants", 
            "text": "The final step will be to browse some of these somatic structural\nvariants in IGV and to visually verify the reliability of the calls. VCF\nfile structure was designed by the 1000Genomes Project consortium with\nconsiderable focus being placed on single-nucleotide variants and\nInDels. It is arguable whether the file format is easy to interpret for\nbasic SVs and much less for complex SV formatting. To make it easier to\nsee the breakpoints we will create a bed file.  1\n2 $CZ/sv.vcf2bed.sh somatic.sv.vcf.gz   somatic.sv.bed\nhead somatic.sv.bed   Load the IGV browser  1 $BR/igv.sh   Once IGV has started use  File  and  Load from File  to load the\ncancer_cell_line.bam. To dedicate more of the screen to read pile ups,\nright click on the left hand box that contains the cancer_cell_line.bam Junctions  then select  Remove Track . Go to View  menu and select  Preferences , then click on the Alignments tab\nand in the  Visibility range threshold (kb)  text box, enter 600. This\nwill allow you to increase your visibility of pile ups as you zoom out.\nNow look for check box for  Filter secondary alignments  click on to\nensure we do not see secondary alignments (alternate mapped position of\na read) . Also ensure that  Show soft-clipped bases  has been checked\nthen click  OK .  Then import  somatic.sv.bed  from your working directory using  Regions \nand  Import Regions .", 
            "title": "Visualisation of Somatic Structural Variants"
        }, 
        {
            "location": "/modules/cancer-module-sv/sv_tut/#verify-deletion", 
            "text": "The somatic structural variants can then be browsed easily using the Region Navigator . Select the deletion (chrX:76853017-77014863) from\nthe  Region Navigator  and click  View . This will centre the IGV\nalignment view on the selected structural variant. Close the regions of\ninterest pop up window. The red bar below the ruler marks the region of\nthe deletion.  It\u2019s usually best to zoom out once by clicking on the \u2019-\u2019 sign in the\ntoolbar at the top, to give a wide view of the supporting abnormal\npaired-end read mappings.  To highlight the abnormal paired-ends right click on the main track\ndisplay panel and select  Color alignments by  and then switch to insert size and pair orientation .  Read pairs that have a larger than expected insert size will be\nhighlighted in red. Click  View as pairs . Right click and  Sort\nalignments by  then select  start location .  How many abnormal  paired-end read pairs  (red coloured F/R oriented\nread pairs) can you see that spans the deletion region? Does this number\ncoincide with the INFO:PE?  19, YES  Zoom into left breakpoint and tally the number of soft-clipped reads at\nboth junctions. How many abnormal split-reads (soft-clipped reads) did\nyou observe? Clue INFO:SR.  11  Go to the RefSeq genes track at the bottom of IGV and right click to Expanded . Is the predicted deletion likely to have a deleterious\nimpact on a gene? If so, what gene and exons are deleted?  ATRX, exons 2 to 25.  Does this region appear to be completely removed from this cancer\ngenome? How can you tell?  Yes, there is no read coverage within this deletion region relative to\nthe control genome.", 
            "title": "Verify Deletion"
        }, 
        {
            "location": "/modules/cancer-module-sv/sv_tut/#verify-translocation", 
            "text": "Select the duplication (chrX:45649874-45689322) from the  Region\nNavigator .  Highlight the abnormal paired-ends by clicking and selecting  Color\nalignments by  and then switch to  insert size and pair orientation .\nAlso, invoke  Sort alignments by  then select  start location .  Zoom out until you can see all the purple reads at the junction.  What is the direction of the purple cluster of reads (indicates that\nmate reads are mapped to Chr15)? Is it pointing to the tail or head of\nChr18?  Forward, towards the tail, or 3\u2019 (+ive)  Right click on to one of the purple coloured reads and select  View mate\nregion in split-screen . This will split the screen and display Chr15 on\nthe left and place a red highlighted outline on both reads, to indicate\nthe pairs. Right click and  Sort alignments by  then select  start\nlocation . What is the direction of the yellow cluster of reads\n(indicates that mate reads are mapped to Chr18)? Is it pointing to the\ntail or head of Chr15? If you wish to zoom in or out, first click inside\nof the chromosome ideogram panel, then ctrl- to zoom out and shift+ to\nzoom in.  ? Reverse, towards the head, or 5\u2019 (+ive)  How is the Chr15 and Chr18 fused (which one of the four translocation\nconnection types)? If you are uncertain then run a BLAT search using the\nINFO:CONSENSUS sequence (gedit somatic.sv.vcf). https://genome.ucsc.edu/cgi-bin/hgBlat?command=start  RF, head to tail, or 5 to 3. Therefore, Chr18 left side is fused to\nChr15 right side.  Did DELLY predict a reciprocal translocation? How can you tell?  No, as we would expect to observe a Chr18 right side fused to Chr15 left\nside, near the same breakpoints.  What gene structures is this translocation predicted to impact?  PARD6G on Chr15 and ADAMTSL3 on Chr18.  What is one possible reason why there is no observable read coverage\nafter Chr18 breakpoint?  Chromosome loss.", 
            "title": "Verify translocation"
        }, 
        {
            "location": "/modules/cancer-module-sv/sv_tut/#verify-tandem-duplication", 
            "text": "Select the tandem duplication (chrX:45649874-45689322) from the  Region\nNavigator .  Highlight the abnormal paired-ends by clicking and selecting  Color\nalignments by  and then switch to  insert size and pair orientation .\nAlso, invoke  Sort alignments by  then select  start location .  Zoom out until you can see all the red paired-end reads spanning the two\njunctions. After that zoom in on the cluster of abnormal reads on the\nleft junction and then right junction.  Which is the order and orientation of these paired-end reads (FR, RF, FF\nor RR)?  RF  What is the estimated read-depth ratio of the cancer_cell_line versus\nnormal control (INFO:RDRATIO) over the duplicate region?  3.3 (\u00a03 x increased read depth)", 
            "title": "Verify tandem duplication"
        }, 
        {
            "location": "/modules/cancer-module-sv/sv_tut/#verify-inversion", 
            "text": "Type into the search box near at tool bar, Chr20:54834492  Right click on main display and select  Group alignments by  then switch\non  paired-orientation . Also, right click and  Sort alignments by  then\nselect  start location .  Zoom out until you can see all the red coloured cluster of reads near\nthe breakpoint.  Right click on to one of the red coloured reads and select  View mate\nregion in split-screen . This will split the screen and display the read\nmate on the right side. This will take you to the mate-reads near the\nsecond breakpoint.  Which direction are the paired-end reads spanning (left or right\nspanning)?  Right  What is the estimated size of the inverted interval?  55,408,660 \u2013 54,834,492 = 574,168 bp", 
            "title": "Verify Inversion"
        }, 
        {
            "location": "/modules/cancer-module-sv/sv_tut/#acknowledgements", 
            "text": "We would like to thank and acknowledge Tobias Rausch (EMBL Heidelberg)\nfor his help and for allowing us to borrow and adapt his replies to\nquestions and original course material.", 
            "title": "Acknowledgements"
        }, 
        {
            "location": "/modules/cancer-module-viz/visu/", 
            "text": "The on-line version is available at here:\n\nhttps://github.com/mbourgey/EBI_cancer_workshop_visualization/tree/australia_2015\n\n\nKey Learning Outcomes\n\n\nAfter completing this practical the trainee should be able to:\n\n\n\n\nGenerate circos like graphics using R\n\n\n\n\nResources You\u2019ll be Using\n\n\nTools Used\n\n\nR:\n\n\nhttps://cran.r-project.org/\n\n\nR package circlize:\n\n\nhttps://cran.r-project.org/web/packages/circlize/index.html\n\n\nIntroduction\n\n\nThis short workshop will show you how to visualize your data.\n\n\nWe will be working on 3 types of somatic calls:\n\n\n\n\n\n\nSNV calls from MuTect (vcf)\n\n\n\n\n\n\nSV calls from DELLY (vcf)\n\n\n\n\n\n\nCNV calls from SCoNEs (tsv)\n\n\n\n\n\n\nThis work is licensed under a Creative Commons Attribution-ShareAlike\n3.0 Unported License. This means that you are able to copy, share and\nmodify the work, as long as the result is distributed under the same\nlicense.\n\n\nPrepare the Environment\n\n\nWe will use a dataset derived from the analysis of whole genome\nsequencing paired normal/tumour samples\n\n\nThe call files are contained in the folder visualization:\n\n\nmutect.somatic.vcf\n\\\n: \n\n\ndelly.somatic.vcf\n\\\n: \n\n\nscones.somatic.tsv\n\\\n:   \n\n\nMany tools are available to do this the most common know is circos. But\ncircos is a really not user friendly. In this tutoriel we show you an\neasy alternative to build circular representation of genomic data.\n\n\nFirst we need to go in the folder to do the analysis\n\n\n1\ncd /home/trainee/visualization/\n\n\n\n\n\n\nLet see what is in this folder\n\n\n1\nls\n\n\n\n\n\n\ncircos.R delly.somatic.vcf mutect.somatic.vcf scones.somatic.tsv\n\n\nTake a look of the data files.\n\n\nThese are data of the are not restricted to a short piece of the\nchromosome.\n\n\nSNVs have already been filtered\n\n\n1\n2\n3\nless mutect.somatic.tsv\nless delly.somatic.vcf\nless data/scones.somatic.30k.tsv\n\n\n\n\n\n\nWhat can you see from this data ?\n\n\nI saw:\n\n\n\n\n\n\nthe filtered output of 3 differents software: mutect (SNVs), delly\n    (SVs), SCoNEs (CNVs)\n\n\n\n\n\n\nThe 3 files show 2 different formats (vcf, tsv)\n\n\n\n\n\n\nalmost all type of variant are represented here: mutations,\n    deletion, inversion, translocation, large amplification and deletion\n    (CNVs)\n\n\n\n\n\n\nWhy don\u2019t we use the vcf format for all type of call?\n\n\nThe 1000 Genomes project try to use/include SVs call in the vcf format.\nSome tools like Delly use this format for SV. This a good idea to try to\ninclude everything altogether but, to my point of view, this not the\nbest way to handle SV and CNV.\n\n\nWhy ?\n\n\nDue to the nature of these calls, you can not easily integrate the\npostional information of the two breakpoints (that could be located\nfaraway or in an other chormosome) using a single position format.\n\n\nThe analysis will be done using the R program\n\n\n1\nR\n\n\n\n\n\n\nWe will use the circlize package from the cran R project. This package\nis dedicated to generate circular plot and had the advantage to provide\npre-build function for genomics data. One of the main advantage of this\ntools is the use of bed format as input data.\n\n\n1\nlibrary(circlize)\n\n\n\n\n\n\nLet\u2019s import the variants\n\n\n1\n2\n3\nsnp=read.table(\nmutect.somatic.vcf\n)\nsv=read.table(\nsomatic.sv.vcf\n)\ncnv=read.table(\ndata/scones.somatic.tsv\n,header=T)\n\n\n\n\n\n\nWe need to set-up the generic graphical parameters\n\n\n1\n2\n3\n4\npar(mar = c(1, 1, 1, 1))\ncircos.par(\nstart.degree\n = 90)\ncircos.par(\ntrack.height\n = 0.05)\ncircos.par(\ncanvas.xlim\n = c(-1.3, 1.3), \ncanvas.ylim\n = c(-1.3, 1.3))\n\n\n\n\n\n\nLet\u2019s draw hg19 reference ideograms\n\n\n1\ncircos.initializeWithIdeogram(species = \nhg19\n)\n\n\n\n\n\n\nUnfortunately circlize does not support hg38 yet. So we will need to\nreformat our data to fit the hg19 standards As we work only on autosomes\nwe won\u2019t need to lift-over and we could simply add \nchr\n at the begin\nof the chromosome names\n\n\nWe can now draw 1 track for somatic mutations\n\n\n1\n2\n3\n4\n5\nsnv_tmp=read.table(\ndata/mutec.somatic.vcf\n,comment.char=\n#\n)\nsnv=cbind(paste(\nchr\n,as.character(snp[,1]),sep=\n),snp[2],snp[,2]+1)\ncircos.genomicTrackPlotRegion(snv,stack=TRUE, panel.fun = function(region, value, ...) {\n    circos.genomicPoints(region, value, cex = 0.05, pch = 9,col=\norange\n , ...)\n})\n\n\n\n\n\n\nLet\u2019s draw the 2 tracks for cnvs. One track for duplication in red and\none blue track for deletion.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\ndup=cnv[cnv[,5]\n2,]\ndup[,1]=paste(\nchr\n,as.character(dup[,1]),sep=\n)\ndel=cnv[cnv[,5]\n2,]\ndel[,1]=paste(\nchr\n,as.character(del[,1]),sep=\n)\ncircos.genomicTrackPlotRegion(dup, stack = TRUE,panel.fun = function(region, value, ...) {\n        circos.genomicRect(region, value, col = \nred\n,bg.border = NA, cex=1 , ...)\n})\ncircos.genomicTrackPlotRegion(del, stack = TRUE,panel.fun = function(region, value, ...) {\n        circos.genomicRect(region, value, col = \nblue\n,bg.border = NA, cex=1 , ...)\n})\n\n\n\n\n\n\nWe can cleary see a massive deletion in the chromosome 3.\n\n\nTo finish we just need to draw 3 tracks + positional links to represent\nSVs\n\n\nUnfortunately the vcf format has not been designed for SVs. SVs are\ndefined by 2 breakpoints and the vcf format store the second one in the\ninfo field. So we will need to extract this information to draw these\ncalls.\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\nchrEnd=NULL\nposEnd=NULL\nfor (i in 1:dim(sv)[1]) {\n    addInfo=strsplit(as.character(sv[i,8]),split=\n;\n)\n    chrInf=strsplit(addInfo[[1]][3],split=\n=\n)\n    chrEnd=c(chrEnd,chrInf[[1]][2])\n    posInf=strsplit(addInfo[[1]][4],split=\n=\n)\n    posEnd=c(posEnd,posInf[[1]][2])\n}\nsvTable=data.frame(paste(\nchr\n,sv[,1],sep=\n),as.numeric(sv[,2]),as.numeric(posEnd),paste(\nchr\n,chrEnd,sep=\n),as.character(sv[,5]))\n\n\n\n\n\n\nNow that we reformat the SV calls, let\u2019s draw them\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\ntypeE=c(\nDEL\n,\nINS\n,\nINV\n)\ncolE=c(\nblue\n,\nblack\n,\ngreen\n)\nfor (i in 1:3) {\n        bed_list=svTable[svTable[,5]==typeE[i],]\n        circos.genomicTrackPlotRegion(bed_list,stack=TRUE, panel.fun = function(region, value, ...) {\n                circos.genomicPoints(region, value, cex = 0.5, pch = 16, col = colE[i], ...)\n        })\n}\n\nbed1=cbind(svTable[svTable[,5]==\nTRA\n,1:2],svTable[svTable[,5]==\nTRA\n,2]+5)\nbed2=cbind(svTable[svTable[,5]==\nTRA\n,c(4,3)],svTable[svTable[,5]==\nTRA\n,3]+5)\n\nfor (i in 1:dim(bed1)[1]) {\n    circos.link(bed1[i,1],bed1[i,2],bed2[i,1],bed2[i,2])\n}\n\n\n\n\n\n\nA good graph needs title and legends\n\n\n1\n2\n3\ntitle(\nSomatic calls (SNV - SV - CNV)\n)\nlegend(0.7,1.4,legend=c(\nSNV\n, \nCNV-DUPLICATION\n,\nCNV-DELETION\n,\nSV-DELETION\n,\nSV-INSERTION\n,\nSV-INVERSION\n),col=c(\norange\n,\nred\n,\nblue\n,\nblue\n,\nblack\n,\ngreen\n,\nred\n),pch=c(16,15,15,16,16,16,16,16),cex=0.75,title=\nTracks:\n,bty=\nn\n)\nlegend(0.6,0.95,legend=\nSV-TRANSLOCATION\n,col=\nblack\n,lty=1,cex=0.75,lwd=1.2,bty=\nn\n)\n\n\n\n\n\n\nyou should obtain a plot like this one:\n\n\n[h] \n\n\ncould you generate the graph and save it into a pdf file ?\n\n\n1\n2\n3\n4\ncircos.clear()\npdf(\ncircos.pdf\n)\n...\ndev.off()\n\n\n\n\n\n\nFinally exit R\n\n\n1\nq(\nyes\n)\n\n\n\n\n\n\nAcknowledgements\n\n\nI would like to thank and acknowledge Louis Letourneau for this help and\nfor sharing his material. The format of the tutorial has been inspired\nfrom Mar Gonzalez Porta. I also want to acknowledge Joel Fillon, Louis\nLetrouneau (again), Francois Lefebvre, Maxime Caron and Guillaume\nBourque for the help in building these pipelines and working with all\nthe various datasets.", 
            "title": "Variant Visualisation"
        }, 
        {
            "location": "/modules/cancer-module-viz/visu/#key-learning-outcomes", 
            "text": "After completing this practical the trainee should be able to:   Generate circos like graphics using R", 
            "title": "Key Learning Outcomes"
        }, 
        {
            "location": "/modules/cancer-module-viz/visu/#resources-youll-be-using", 
            "text": "", 
            "title": "Resources You\u2019ll be Using"
        }, 
        {
            "location": "/modules/cancer-module-viz/visu/#tools-used", 
            "text": "R:  https://cran.r-project.org/  R package circlize:  https://cran.r-project.org/web/packages/circlize/index.html", 
            "title": "Tools Used"
        }, 
        {
            "location": "/modules/cancer-module-viz/visu/#introduction", 
            "text": "This short workshop will show you how to visualize your data.  We will be working on 3 types of somatic calls:    SNV calls from MuTect (vcf)    SV calls from DELLY (vcf)    CNV calls from SCoNEs (tsv)    This work is licensed under a Creative Commons Attribution-ShareAlike\n3.0 Unported License. This means that you are able to copy, share and\nmodify the work, as long as the result is distributed under the same\nlicense.", 
            "title": "Introduction"
        }, 
        {
            "location": "/modules/cancer-module-viz/visu/#prepare-the-environment", 
            "text": "We will use a dataset derived from the analysis of whole genome\nsequencing paired normal/tumour samples  The call files are contained in the folder visualization:  mutect.somatic.vcf \\\n:   delly.somatic.vcf \\\n:   scones.somatic.tsv \\\n:     Many tools are available to do this the most common know is circos. But\ncircos is a really not user friendly. In this tutoriel we show you an\neasy alternative to build circular representation of genomic data.  First we need to go in the folder to do the analysis  1 cd /home/trainee/visualization/   Let see what is in this folder  1 ls   circos.R delly.somatic.vcf mutect.somatic.vcf scones.somatic.tsv  Take a look of the data files.  These are data of the are not restricted to a short piece of the\nchromosome.  SNVs have already been filtered  1\n2\n3 less mutect.somatic.tsv\nless delly.somatic.vcf\nless data/scones.somatic.30k.tsv   What can you see from this data ?  I saw:    the filtered output of 3 differents software: mutect (SNVs), delly\n    (SVs), SCoNEs (CNVs)    The 3 files show 2 different formats (vcf, tsv)    almost all type of variant are represented here: mutations,\n    deletion, inversion, translocation, large amplification and deletion\n    (CNVs)    Why don\u2019t we use the vcf format for all type of call?  The 1000 Genomes project try to use/include SVs call in the vcf format.\nSome tools like Delly use this format for SV. This a good idea to try to\ninclude everything altogether but, to my point of view, this not the\nbest way to handle SV and CNV.  Why ?  Due to the nature of these calls, you can not easily integrate the\npostional information of the two breakpoints (that could be located\nfaraway or in an other chormosome) using a single position format.  The analysis will be done using the R program  1 R   We will use the circlize package from the cran R project. This package\nis dedicated to generate circular plot and had the advantage to provide\npre-build function for genomics data. One of the main advantage of this\ntools is the use of bed format as input data.  1 library(circlize)   Let\u2019s import the variants  1\n2\n3 snp=read.table( mutect.somatic.vcf )\nsv=read.table( somatic.sv.vcf )\ncnv=read.table( data/scones.somatic.tsv ,header=T)   We need to set-up the generic graphical parameters  1\n2\n3\n4 par(mar = c(1, 1, 1, 1))\ncircos.par( start.degree  = 90)\ncircos.par( track.height  = 0.05)\ncircos.par( canvas.xlim  = c(-1.3, 1.3),  canvas.ylim  = c(-1.3, 1.3))   Let\u2019s draw hg19 reference ideograms  1 circos.initializeWithIdeogram(species =  hg19 )   Unfortunately circlize does not support hg38 yet. So we will need to\nreformat our data to fit the hg19 standards As we work only on autosomes\nwe won\u2019t need to lift-over and we could simply add  chr  at the begin\nof the chromosome names  We can now draw 1 track for somatic mutations  1\n2\n3\n4\n5 snv_tmp=read.table( data/mutec.somatic.vcf ,comment.char= # )\nsnv=cbind(paste( chr ,as.character(snp[,1]),sep= ),snp[2],snp[,2]+1)\ncircos.genomicTrackPlotRegion(snv,stack=TRUE, panel.fun = function(region, value, ...) {\n    circos.genomicPoints(region, value, cex = 0.05, pch = 9,col= orange  , ...)\n})   Let\u2019s draw the 2 tracks for cnvs. One track for duplication in red and\none blue track for deletion.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 dup=cnv[cnv[,5] 2,]\ndup[,1]=paste( chr ,as.character(dup[,1]),sep= )\ndel=cnv[cnv[,5] 2,]\ndel[,1]=paste( chr ,as.character(del[,1]),sep= )\ncircos.genomicTrackPlotRegion(dup, stack = TRUE,panel.fun = function(region, value, ...) {\n        circos.genomicRect(region, value, col =  red ,bg.border = NA, cex=1 , ...)\n})\ncircos.genomicTrackPlotRegion(del, stack = TRUE,panel.fun = function(region, value, ...) {\n        circos.genomicRect(region, value, col =  blue ,bg.border = NA, cex=1 , ...)\n})   We can cleary see a massive deletion in the chromosome 3.  To finish we just need to draw 3 tracks + positional links to represent\nSVs  Unfortunately the vcf format has not been designed for SVs. SVs are\ndefined by 2 breakpoints and the vcf format store the second one in the\ninfo field. So we will need to extract this information to draw these\ncalls.   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10 chrEnd=NULL\nposEnd=NULL\nfor (i in 1:dim(sv)[1]) {\n    addInfo=strsplit(as.character(sv[i,8]),split= ; )\n    chrInf=strsplit(addInfo[[1]][3],split= = )\n    chrEnd=c(chrEnd,chrInf[[1]][2])\n    posInf=strsplit(addInfo[[1]][4],split= = )\n    posEnd=c(posEnd,posInf[[1]][2])\n}\nsvTable=data.frame(paste( chr ,sv[,1],sep= ),as.numeric(sv[,2]),as.numeric(posEnd),paste( chr ,chrEnd,sep= ),as.character(sv[,5]))   Now that we reformat the SV calls, let\u2019s draw them   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15 typeE=c( DEL , INS , INV )\ncolE=c( blue , black , green )\nfor (i in 1:3) {\n        bed_list=svTable[svTable[,5]==typeE[i],]\n        circos.genomicTrackPlotRegion(bed_list,stack=TRUE, panel.fun = function(region, value, ...) {\n                circos.genomicPoints(region, value, cex = 0.5, pch = 16, col = colE[i], ...)\n        })\n}\n\nbed1=cbind(svTable[svTable[,5]== TRA ,1:2],svTable[svTable[,5]== TRA ,2]+5)\nbed2=cbind(svTable[svTable[,5]== TRA ,c(4,3)],svTable[svTable[,5]== TRA ,3]+5)\n\nfor (i in 1:dim(bed1)[1]) {\n    circos.link(bed1[i,1],bed1[i,2],bed2[i,1],bed2[i,2])\n}   A good graph needs title and legends  1\n2\n3 title( Somatic calls (SNV - SV - CNV) )\nlegend(0.7,1.4,legend=c( SNV ,  CNV-DUPLICATION , CNV-DELETION , SV-DELETION , SV-INSERTION , SV-INVERSION ),col=c( orange , red , blue , blue , black , green , red ),pch=c(16,15,15,16,16,16,16,16),cex=0.75,title= Tracks: ,bty= n )\nlegend(0.6,0.95,legend= SV-TRANSLOCATION ,col= black ,lty=1,cex=0.75,lwd=1.2,bty= n )   you should obtain a plot like this one:  [h]   could you generate the graph and save it into a pdf file ?  1\n2\n3\n4 circos.clear()\npdf( circos.pdf )\n...\ndev.off()   Finally exit R  1 q( yes )", 
            "title": "Prepare the Environment"
        }, 
        {
            "location": "/modules/cancer-module-viz/visu/#acknowledgements", 
            "text": "I would like to thank and acknowledge Louis Letourneau for this help and\nfor sharing his material. The format of the tutorial has been inspired\nfrom Mar Gonzalez Porta. I also want to acknowledge Joel Fillon, Louis\nLetrouneau (again), Francois Lefebvre, Maxime Caron and Guillaume\nBourque for the help in building these pipelines and working with all\nthe various datasets.", 
            "title": "Acknowledgements"
        }, 
        {
            "location": "/modules/cancer-module-somatic/01_signatures/", 
            "text": "Key Learning Outcomes\n\n\nAfter completing this practical the trainee should be able to:\n\n\n\n\n\n\nVisualise mutational signatures present in a cohort using somatic\n    single nucleotide mutation data in Variant Call Format (vcf) files.\n\n\n\n\n\n\nCompare analysis output with published results to identify common\n    mutational signatures.\n\n\n\n\n\n\nHave gained overview knowledge of how somatic signatures can help\n    with cohort cancer analysis.\n\n\n\n\n\n\nResources You\u2019ll be Using\n\n\nTools Used\n\n\nR-3.2.2 statistical environment:\n\n\nhttps://www.r-project.org/\n\n\nSomaticSignatures R package:\n\n\nhttp://bioconductor.org/packages/release/bioc/html/SomaticSignatures.html\n\n\nBSgenome.Hsapiens.UCSC.hg19:\n\n\nhttp://bioconductor.org/packages/release/data/annotation/html/BSgenome.Hsapiens.UCSC.hg19.html\n\n\nVariantAnnotation:\n\n\nhttps://bioconductor.org/packages/release/bioc/html/VariantAnnotation.html\n\n\nGenomicRanges:\n\n\nhttps://bioconductor.org/packages/release/bioc/html/GenomicRanges.html\n\n\nCairo:\n\n\nhttps://cran.rstudio.com/web/packages/Cairo/index.html\n\n\nSources of Data\n\n\nTCGA melanoma SNV data:\n\n\nhttps://tcga-data.nci.nih.gov/tcga/\n\n\nICGC ovarian SNV data: \n\n\nhttps://dcc.icgc.org/\n\n\nUseful Links\n\n\nVariant Call Format (VCF) specification: \n\n\nhttp://samtools.github.io/hts-specs/VCFv4.2.pdf\n\n\nIntroduction\n\n\nThe most common genetic model for cancer development is the accumulation\nof DNA mutations over time, eventually leading to the disruption or\ndysregulation of enough key genes that lead cells to uncontrolled\ngrowth. Cells in our bodies accumulate DNA mutations over time due to\nnormal aging processes and through exposure to carcinogens.\n\n\nRecently researchers found a method to take all the single nucleotide\nmutations identified in tumour cells (somatic SNVs) and group them\ntogether by the type of the mutation and also what the neighbouring\nbases are. This is commonly referred to as somatic mutational\nsignatures.\n\n\nCommon mutational processes that are regularly identified in cancer\nsequencing are:\n\n\n\n\n\n\nAge: the aging process. These are high in C/T transitions due to\n    deamination of methyl-cytidine.\n\n\n\n\n\n\nSmoking: marks exposure to inhaled carcinogens and has high numbers\n    of C/A transversions.\n\n\n\n\n\n\nUV: UV exposure. These are also high in C/T transitions at\n    di-pyrimidine sites.\n\n\n\n\n\n\nBRCA: Indicates that the homologous recombination repair pathway is\n    defective.\n\n\n\n\n\n\nAPOBEC: Thought to be marking dysregulated APOBEC enzyme activity on\n    single stranded DNA produced during the repair processing of other\n    lesions such as double stand breaks.\n\n\n\n\n\n\nMMR: Mismatch repair pathway not working properly. These are high in\n    C/T mutations too.\n\n\n\n\n\n\nIn cohort cancer analysis it is common to try to generate subtypes to\ngroup your data based on a particular molecular phenotype. A reason for\ndoing may include finding sets of patients that have a similar form of\nthe disease and therefore all might benefit from a particular treatment.\nWe can use the somatic mutational signatures analysis to group the data\nfrom a cohort of patients to inform which genomes are most similar based\non the pattern of exposures or processes that have contributed to their\ngenome changes. The patients don\u2019t have to have the same type of cancer\nso pan-cancer studies are using this analysis to find similarities\nacross cancer types.\n\n\nPreparing the R environment\n\n\nThe mathematical framework developed by Alexandrov et al was implemented\nin MATLAB. We are going to use a version implemented in R by Gehring et\nal, called \nSomaticSignatures package\n, that is very quick and flexible\nbut currently only accepts point mutations not insertions or deletions\n(indels). In tests on our data we have found that the Somatic Signatures\npackage in R returns very similar results to the full implementation of\nAlexandrov\u2019s framework.\n\n\nThe data files you will need are contained in the subdirectory called\n\nsomatic/somatic_signatures\n:\n\n\nOpen the Terminal and go to the \nsomatic_signatures\n working directory:\n\n\n1\n2\ncd ~/somatic/somatic_signatures\npwd\n\n\n\n\n\n\nIn this folder you should find 12 files that end with the extension\n.vcf. Use the list command to make sure you can see them.\n\n\n1\nls\n\n\n\n\n\n\nThese files contain data extracted from the TCGA melanoma paper and\nAustralian ICGC ovarian paper both mentioned in the introductory slides.\nThey have been edited in order to allow this practical to run quickly\nand are not good examples of VCF files.\n\n\nStart R and set the working directory. Just start by typing R onto the\ncommand line.\n\n\n1\nR\n\n\n\n\n\n\nLoad all the package libraries needed for this analysis by running the\ncommands.\n\n\n1\n2\n3\n4\nlibrary(SomaticSignatures)\nlibrary(BSgenome.Hsapiens.UCSC.hg19)\nlibrary(ggplot2)\nlibrary(Cairo)\n\n\n\n\n\n\nSet the directory where any output files will be generated\n\n\n1\nsetwd(\n~/somatic/somatic_signatures\n)\n\n\n\n\n\n\nLoading and preparing the SNV mutation data\n\n\nThe mutations used in this analysis need to be high quality somatic\nmutations\n\n\n\n\n\n\nRemember the goal is to find the key mutational processes that these\n    tumours have been exposed to, so you need to exclude germline\n    mutations (mutations that the person was born with that can be seen\n    in the sequencing of matched normal samples).\n\n\n\n\n\n\nSequencing errors can also occur at particular DNA sequence contexts\n    and can also be picked up using this method. To avoid this use only\n    high quality mutation calls.\n\n\n\n\n\n\nRead in the mutations from the 12 vcf files\n\n\n1\nfiles \n-\n \nlist.files\n(\n~/somatic/somatic_signatures\n,\n pattern\n=\nvcf$\n,\n full.names\n=\nTRUE\n)\n\n\n\n\n\n\n\nTo make sure all the files are listed run the command.\n\n\n1\nfiles\n\n\n\n\n\n\nYou should see a list of 12 sample files.\n\n\nNext read in all the genomic positions of variants in the VCF files\nusing the vranges class.\n\n\n1\nvranges \n-\n \nlapply\n(\nfiles\n,\n \nfunction\n(\nv\n)\n readVcfAsVRanges\n(\nv\n,\nhg19\n))\n\n\n\n\n\n\n\nJoin all the lists of variant positions into one big data set so that it\ncan be processed together and look at what is contained in the\nconcatenated vranges data\n\n\n1\n2\nvranges.cat \n-\n \ndo.call\n(\nc\n,\nvranges\n)\n\nvranges.cat\n\n\n\n\n\n\nThe first line of output of the \nvranges.cat\n shows us that in total we\nhave put over 100,000 mutations recording the chromosome positions and\nmutation base changes along with what sample they were seen in.\n\n\nNote there are a lot of NA values in this data set because we have left\nout non-essential information in order to cut down on the processing\ntime.\n\n\nNext we need to ensure all the positions in the vranges object have been\nrecorded in UCSC notation form so that they will match up with the\nreference we are using.\n\n\n1\nvranges.cat \n-\n ucsc\n(\nvranges.cat\n)\n\n\n\n\n\n\n\nIt is always important to select the correct reference for your data.\n\n\nWe can print out how many mutations we have read in for each of the\ncancer samples we are using by using the command.\n\n\n1\nprint(table(sampleNames(vranges.cat)))\n\n\n\n\n\n\nWe have now added all the positional and base change information now we\ncan use the reference and the position of the mutation to look up the\nbases on either side of the mutation i.e. the mutation context.\n\n\nRun the mutationContext function of SomaticSignatures.\n\n\n1\nmc \n-\n mutationContext\n(\nvranges.cat\n,\n BSgenome.Hsapiens.UCSC.hg19\n)\n\n\n\n\n\n\n\nWe can inspect what information we had added to the vranges.cat object\nby typing \u2019mc\u2019 on the command line. Notice that the mutation and its\ncontext have been added to the last two columns.\n\n\n1\nmc\n\n\n\n\n\n\nSNV mutation context\n\n\nThere are a total of 96 possible single base mutations and context\ncombinations. We can calculate this by listing out the six possible\ntypes of single nucleotide mutations:\n\n\n\n\n\n\nA/C\n\n\nthe reverse compliment (T/G) is also in this group\n\n\n\n\n\n\nA/G\n\n\nincludes (T/C)\n\n\n\n\n\n\nA/T\n\n\nincludes (T/A)\n\n\n\n\n\n\nC/A\n\n\nincludes (G/T)\n\n\n\n\n\n\nC/G\n\n\nincludes (G/C)\n\n\n\n\n\n\nC/T\n\n\nincludes (G/C)\n\n\n\n\n\n\nThe neighbouring bases, on either side of a mutation, are referred to as\nthe mutation context. There are 16 possible combinations of mutation\ncontexts. Here [.] stands for one of the mutations listed above.\n\n\n\n\n\n\nA[.]A\n\n\nA[.]C\n\n\nA[.]G\n\n\nA[.]T\n\n\n\n\n\n\nC[.]A\n\n\nC[.]C\n\n\nC[.]G\n\n\nC[.]T\n\n\n\n\n\n\nG[.]A\n\n\nG[.]C\n\n\nG[.]G\n\n\nG[.]T\n\n\n\n\n\n\nT[.]A\n\n\nT[.]C\n\n\nT[.]G\n\n\nT[.]T\n\n\n\n\n\n\nNow if we substitute the [.]\u2019s with each of the 6 different mutations\nyou will find there are 96 possible types of combined mutations and\ncontexts (6 x 16).\n\n\nStart by substituting [.] for the A/C mutation type\n\n\n\n\n\n\nA[A/C]A\n\n\n\n\n\n\nA[A/C]C\n\n\n\n\n\n\nA[A/C]G\n\n\n\n\n\n\nA[A/C]T\n\n\n\n\n\n\nC[A/C]A\n\n\n\n\n\n\nC[A/C]C\n\n\n\n\n\n\nC[A/C]G\n\n\n\n\n\n\nand so on\n\n\nWe assign all the somatic mutations identified in a single tumour to one\nof these categories and total up the number in each.\n\n\nWhat about a mutation that looks like G[T/G]A, where should this go?\nHint remember to reverse compliment all the nucleotides.\n\n\nIn the T[A/C]C context count.\n\n\nNow we have all the information that is needed for each sample we can\nmake a matrix that contains counts of mutations in each of the 96\npossible combinations of mutations and contexts counting up the totals\nseparately for each sample\n\n\n1\n2\nmm \n-\n motifMatrix\n(\nmc\n,\n group \n=\n \nsampleNames\n,\n normalize\n=\nTRUE\n)\n\n\ndim\n(\nmm\n)\n\n\n\n\n\n\n\nThe output of the \ndim(mm)\n command show us that there are 96 rows\n(these are the context values) and 12 columns which are the 12 samples.\n\n\nRunning the NMF analysis\n\n\nUsing the matrix we have made we can now run the non-negative matrix\nfactorisation (NMF) process that attempts to find the most stable,\ngrouping solutions for all of the combinations of mutations and\ncontexts. It does this by trying to find similar patterns, or profiles,\namongst the samples to sort the data into firstly just 2 groups. This is\nrepeated to get replicate values for each attempt and then separating\nthe data by 3 groups, and then 4 and so on.\n\n\nThese parameter choices have been made to keep running time short for\nthis practical. If you have more samples from potentially diverse\nsources you may need to run with a larger range of signatures and with\nmore replicates.\n\n\nto find out how many signatures we have in the data run the command.\n\n\n1\ngof_nmf \n-\n assessNumberSignatures\n(\nmm\n,\n \n2\n:\n10\n,\n nReplicates \n=\n \n5\n)\n\n\n\n\n\n\n\nVisualise the results from the NMF processing by making a pdf of the\nplot\n\n\n1\n2\n3\nCairo(file=\nplotNumberOfSignatures.pdf\n, type=\npdf\n, units=\nin\n, width=9, height=8, dpi=72)\nplotNumberSignatures(gof_nmf)\ndev.off()\n\n\n\n\n\n\nOpen up the PDF and examine the curve. The plotNumberOfSignatures PDF\nthat will have been made in the working directory that you set up at the\nbeginning\n\n\n[H] \n [Figure 1:Number of\nsignatures plot]\n\n\nLook at the y-axis scale on the bottom panel of Figure 1. The explained\nvariance is already very high and so close to finding the correct\nsolution for the number of signatures even with just 2. The error bars\naround each point are fairly small considering we have a very small\nsample set. Deciding how many signatures are present can be tricky but\nhere let\u2019s go for 3. This is where the gradient of both curves have\nstarted to flatten out.\n\n\nNow run the NMF again but this time stipulating that you want to group\nthe data into 3 different mutational signatures.\n\n\n1\nsigs_nmf = identifySignatures(mm, 3, nmfDecomposition)\n\n\n\n\n\n\nVisualise the shape of the profiles for these 3 signatures\n\n\n1\n2\n3\nCairo(file=\nplot3Signatures.pdf\n, type=\npdf\n, units=\nin\n, width=10, height=8, dpi=72)\nplotSignatures(sigs_nmf,normalize=TRUE, percent=FALSE) + ggtitle(\nSomatic Signatures: NMF - Barchart\n) + scale_fill_brewer(palette = \nSet2\n)\ndev.off()\n\n\n\n\n\n\nopen up the plot3Signatures pdf that will have been made in the working\ndirectory.\n\n\nYou should have generated a plot with three signature profiles obtained\nfrom the NMF processing of the test dataset.\n\n\nThe 96 possible mutation/context combinations are plotted along the x\naxis arranged in blocks of 6 lots of 16 (see information above). The\nheight of the bars indicates the frequency of those particular mutation\nand context combinations in each signature.\n\n\nAlthough the section colours are different to the plot you have\ngenerated the mutations are still in the same order across the plot.\n\n\nInterpreting the signature results\n\n\nIn their paper Alexandrov et al used this analysis to generate profiles\nfrom the data for more than 7000 tumour samples sequenced through both\nexome and whole genome approaches. They were able to group the data to\nreveal which genomes have been exposed to similar mutational processes\ncontributing to the genome mutations.\n\n\n[H] \n [Figure 2. 21 signature\npatterns identified from the analysis of more than 7000 different\ntumours from Alexandrov et al. Nature 2013.]\n\n\nCan you match up, by eye, the profile shapes against a selection of\nknown mutational signatures supplied (figure 2)?\n\n\nTry to match up the patterns made by the positions of the highest peaks\nfor each signature.\n\n\n\n\n\n\nAlexandrov signature 7 matches with our signature 1\n\n\n\n\n\n\nAlexandrov signature 13 matches with our signature 2\n\n\n\n\n\n\nAlexandrov signature 3 matches with our signature 3\n\n\n\n\n\n\nNow use the table from Alexandrov et al to identify which mutational\nprocesses our three generated signatures have been associated with.\n\n\n[H] \n [Figure 3 Table\nindicating the probable association of the identified signatures with\nmutational processes and the origin site of the cancer samples from\nAlexandrov et al. Nature 2013.]\n\n\nWhat mutational mechanisms have been associated with the signatures that\nyou have generated?\n\n\n\n\n\n\nOur signature 1 (AS7) is associated with Ultraviolet radiation\n    damage to DNA. This has previously been identified in Head and Neck\n    and Melanoma cancer samples.\n\n\n\n\n\n\nOur signature 2 (AS 13) is associated with the activity of\n    anti-viral APOBEC enzymes. This has previously been seen in Breast\n    and Bladder cancer samples.\n\n\n\n\n\n\nOur signature 3 (AS3) is associated with BRCA1 and BRCA2 mutations,\n    i.e. the homologous recombination repair pathway not working\n    properly. This has been seen in Breast, Ovarian and Pancreas cancer\n    samples.\n\n\n\n\n\n\nNow we can plot out the results for the individual samples in our\ndataset to show what proportion of their mutations have been assigned to\neach of the signatures.\n\n\n1\n2\n3\nCairo(file=\nPlotSampleContribution3Signatures.pdf\n, type=\npdf\n, units=\nin\n, width=9, height=6, dpi=72)\nplotSamples(sigs_nmf, normalize=TRUE) + scale_y_continuous(breaks=seq(0, 1, 0.2), expand = c(0,0))+ theme(axis.text.x = element_text(size=6))\ndev.off()\n\n\n\n\n\n\nIf you don\u2019t have time to carry out the advanced questions you can exit\nR and return to the normal terminal command line.\n\n\n1\n2\nquit()\nn\n\n\n\n\n\n\nOpen the resulting PlotSampleContribution3Signatures.pdf.\n\n\nThis shows the results for the mutation grouping for each sample. The\nsamples are listed on the x-axis and the proportion of all mutations for\nthat sample is shown on the y-axis. The colours of the bars indicate\nwhat proportion of the mutations for that sample were grouped into each\nof the signatures. The colour that makes up most of the bar for each\nsample is called its \nmajor signature\n.\n\n\nThe data you have been using contains samples from High Grade Serous\nOvarian Carcinomas and Cutaneous Melanoma.\n\n\nUsing the major signature found for each sample can you guess which are\novarian and which are melanoma samples?\n\n\n\n\n\n\nSamples 9-12 have the majority signature of our signature 1. This is\n    the UV signature and so these are likely to be Melanoma samples.\n\n\n\n\n\n\nSamples 4-8 have the majority signature of our signature 3. This is\n    the BRCA signature and these are most likely to be ovarian samples.\n\n\n\n\n\n\nSamples 1-3 have the majority signature of our signature 2. This is\n    the APOBEC signature indicating activity of the anti-viral APOBEC\n    enzymes. These are less likely to be from cutaneous melanoma because\n    they have very few UV associated mutations although it could\n    possibly be from a different subtype. However it is much more likely\n    that these will be ovarian tumours as this APOBEC signature has been\n    seen in breast tumours which can be similar to ovarian cancers in\n    terms of the mutated genes.\n\n\n\n\n\n\nThis is an open question for discussion at the end of the practical.\n\n\nHow can this analysis be useful for cancer genomics studies?\n\n\nNow rerun the process this time using 4 signatures as the solution.\n\n\nHint: you don\u2019t have to start back at the beginning but you can jump to\nthe step where you run the NMF but this time for 4 instead of 3\nsignatures. Then continue through making the plots.\n\n\nYou will need to change the name of each plot you remake with 4\nsignatures because Cairo won\u2019t let you overwrite and existing file.\n\n\nCan you find a good match in the set of known signatures for all 4\npatterns?\n\n\nCan you find a verified process for all of the profiles you are seeing?\n\n\nReferences\n\n\nAlexandrov et al. Nature 2013: \n\n\nhttp://www.nature.com/nature/journal/v500/n7463/pdf/nature12477.pdf\n\n\nGehring et al. Bioinformatics 2015: \n\n\nhttp://bioinformatics.oxfordjournals.org/content/early/2015/07/31/bioinformatics.btv408.full", 
            "title": "Assessing somatic mutational signatures"
        }, 
        {
            "location": "/modules/cancer-module-somatic/01_signatures/#key-learning-outcomes", 
            "text": "After completing this practical the trainee should be able to:    Visualise mutational signatures present in a cohort using somatic\n    single nucleotide mutation data in Variant Call Format (vcf) files.    Compare analysis output with published results to identify common\n    mutational signatures.    Have gained overview knowledge of how somatic signatures can help\n    with cohort cancer analysis.", 
            "title": "Key Learning Outcomes"
        }, 
        {
            "location": "/modules/cancer-module-somatic/01_signatures/#resources-youll-be-using", 
            "text": "", 
            "title": "Resources You\u2019ll be Using"
        }, 
        {
            "location": "/modules/cancer-module-somatic/01_signatures/#tools-used", 
            "text": "R-3.2.2 statistical environment:  https://www.r-project.org/  SomaticSignatures R package:  http://bioconductor.org/packages/release/bioc/html/SomaticSignatures.html  BSgenome.Hsapiens.UCSC.hg19:  http://bioconductor.org/packages/release/data/annotation/html/BSgenome.Hsapiens.UCSC.hg19.html  VariantAnnotation:  https://bioconductor.org/packages/release/bioc/html/VariantAnnotation.html  GenomicRanges:  https://bioconductor.org/packages/release/bioc/html/GenomicRanges.html  Cairo:  https://cran.rstudio.com/web/packages/Cairo/index.html", 
            "title": "Tools Used"
        }, 
        {
            "location": "/modules/cancer-module-somatic/01_signatures/#sources-of-data", 
            "text": "TCGA melanoma SNV data:  https://tcga-data.nci.nih.gov/tcga/  ICGC ovarian SNV data:   https://dcc.icgc.org/", 
            "title": "Sources of Data"
        }, 
        {
            "location": "/modules/cancer-module-somatic/01_signatures/#useful-links", 
            "text": "Variant Call Format (VCF) specification:   http://samtools.github.io/hts-specs/VCFv4.2.pdf", 
            "title": "Useful Links"
        }, 
        {
            "location": "/modules/cancer-module-somatic/01_signatures/#introduction", 
            "text": "The most common genetic model for cancer development is the accumulation\nof DNA mutations over time, eventually leading to the disruption or\ndysregulation of enough key genes that lead cells to uncontrolled\ngrowth. Cells in our bodies accumulate DNA mutations over time due to\nnormal aging processes and through exposure to carcinogens.  Recently researchers found a method to take all the single nucleotide\nmutations identified in tumour cells (somatic SNVs) and group them\ntogether by the type of the mutation and also what the neighbouring\nbases are. This is commonly referred to as somatic mutational\nsignatures.  Common mutational processes that are regularly identified in cancer\nsequencing are:    Age: the aging process. These are high in C/T transitions due to\n    deamination of methyl-cytidine.    Smoking: marks exposure to inhaled carcinogens and has high numbers\n    of C/A transversions.    UV: UV exposure. These are also high in C/T transitions at\n    di-pyrimidine sites.    BRCA: Indicates that the homologous recombination repair pathway is\n    defective.    APOBEC: Thought to be marking dysregulated APOBEC enzyme activity on\n    single stranded DNA produced during the repair processing of other\n    lesions such as double stand breaks.    MMR: Mismatch repair pathway not working properly. These are high in\n    C/T mutations too.    In cohort cancer analysis it is common to try to generate subtypes to\ngroup your data based on a particular molecular phenotype. A reason for\ndoing may include finding sets of patients that have a similar form of\nthe disease and therefore all might benefit from a particular treatment.\nWe can use the somatic mutational signatures analysis to group the data\nfrom a cohort of patients to inform which genomes are most similar based\non the pattern of exposures or processes that have contributed to their\ngenome changes. The patients don\u2019t have to have the same type of cancer\nso pan-cancer studies are using this analysis to find similarities\nacross cancer types.", 
            "title": "Introduction"
        }, 
        {
            "location": "/modules/cancer-module-somatic/01_signatures/#preparing-the-r-environment", 
            "text": "The mathematical framework developed by Alexandrov et al was implemented\nin MATLAB. We are going to use a version implemented in R by Gehring et\nal, called  SomaticSignatures package , that is very quick and flexible\nbut currently only accepts point mutations not insertions or deletions\n(indels). In tests on our data we have found that the Somatic Signatures\npackage in R returns very similar results to the full implementation of\nAlexandrov\u2019s framework.  The data files you will need are contained in the subdirectory called somatic/somatic_signatures :  Open the Terminal and go to the  somatic_signatures  working directory:  1\n2 cd ~/somatic/somatic_signatures\npwd   In this folder you should find 12 files that end with the extension\n.vcf. Use the list command to make sure you can see them.  1 ls   These files contain data extracted from the TCGA melanoma paper and\nAustralian ICGC ovarian paper both mentioned in the introductory slides.\nThey have been edited in order to allow this practical to run quickly\nand are not good examples of VCF files.  Start R and set the working directory. Just start by typing R onto the\ncommand line.  1 R   Load all the package libraries needed for this analysis by running the\ncommands.  1\n2\n3\n4 library(SomaticSignatures)\nlibrary(BSgenome.Hsapiens.UCSC.hg19)\nlibrary(ggplot2)\nlibrary(Cairo)   Set the directory where any output files will be generated  1 setwd( ~/somatic/somatic_signatures )", 
            "title": "Preparing the R environment"
        }, 
        {
            "location": "/modules/cancer-module-somatic/01_signatures/#loading-and-preparing-the-snv-mutation-data", 
            "text": "The mutations used in this analysis need to be high quality somatic\nmutations    Remember the goal is to find the key mutational processes that these\n    tumours have been exposed to, so you need to exclude germline\n    mutations (mutations that the person was born with that can be seen\n    in the sequencing of matched normal samples).    Sequencing errors can also occur at particular DNA sequence contexts\n    and can also be picked up using this method. To avoid this use only\n    high quality mutation calls.    Read in the mutations from the 12 vcf files  1 files  -   list.files ( ~/somatic/somatic_signatures ,  pattern = vcf$ ,  full.names = TRUE )    To make sure all the files are listed run the command.  1 files   You should see a list of 12 sample files.  Next read in all the genomic positions of variants in the VCF files\nusing the vranges class.  1 vranges  -   lapply ( files ,   function ( v )  readVcfAsVRanges ( v , hg19 ))    Join all the lists of variant positions into one big data set so that it\ncan be processed together and look at what is contained in the\nconcatenated vranges data  1\n2 vranges.cat  -   do.call ( c , vranges ) \nvranges.cat   The first line of output of the  vranges.cat  shows us that in total we\nhave put over 100,000 mutations recording the chromosome positions and\nmutation base changes along with what sample they were seen in.  Note there are a lot of NA values in this data set because we have left\nout non-essential information in order to cut down on the processing\ntime.  Next we need to ensure all the positions in the vranges object have been\nrecorded in UCSC notation form so that they will match up with the\nreference we are using.  1 vranges.cat  -  ucsc ( vranges.cat )    It is always important to select the correct reference for your data.  We can print out how many mutations we have read in for each of the\ncancer samples we are using by using the command.  1 print(table(sampleNames(vranges.cat)))   We have now added all the positional and base change information now we\ncan use the reference and the position of the mutation to look up the\nbases on either side of the mutation i.e. the mutation context.  Run the mutationContext function of SomaticSignatures.  1 mc  -  mutationContext ( vranges.cat ,  BSgenome.Hsapiens.UCSC.hg19 )    We can inspect what information we had added to the vranges.cat object\nby typing \u2019mc\u2019 on the command line. Notice that the mutation and its\ncontext have been added to the last two columns.  1 mc", 
            "title": "Loading and preparing the SNV mutation data"
        }, 
        {
            "location": "/modules/cancer-module-somatic/01_signatures/#snv-mutation-context", 
            "text": "There are a total of 96 possible single base mutations and context\ncombinations. We can calculate this by listing out the six possible\ntypes of single nucleotide mutations:    A/C  the reverse compliment (T/G) is also in this group    A/G  includes (T/C)    A/T  includes (T/A)    C/A  includes (G/T)    C/G  includes (G/C)    C/T  includes (G/C)    The neighbouring bases, on either side of a mutation, are referred to as\nthe mutation context. There are 16 possible combinations of mutation\ncontexts. Here [.] stands for one of the mutations listed above.    A[.]A  A[.]C  A[.]G  A[.]T    C[.]A  C[.]C  C[.]G  C[.]T    G[.]A  G[.]C  G[.]G  G[.]T    T[.]A  T[.]C  T[.]G  T[.]T    Now if we substitute the [.]\u2019s with each of the 6 different mutations\nyou will find there are 96 possible types of combined mutations and\ncontexts (6 x 16).  Start by substituting [.] for the A/C mutation type    A[A/C]A    A[A/C]C    A[A/C]G    A[A/C]T    C[A/C]A    C[A/C]C    C[A/C]G    and so on  We assign all the somatic mutations identified in a single tumour to one\nof these categories and total up the number in each.  What about a mutation that looks like G[T/G]A, where should this go?\nHint remember to reverse compliment all the nucleotides.  In the T[A/C]C context count.  Now we have all the information that is needed for each sample we can\nmake a matrix that contains counts of mutations in each of the 96\npossible combinations of mutations and contexts counting up the totals\nseparately for each sample  1\n2 mm  -  motifMatrix ( mc ,  group  =   sampleNames ,  normalize = TRUE )  dim ( mm )    The output of the  dim(mm)  command show us that there are 96 rows\n(these are the context values) and 12 columns which are the 12 samples.", 
            "title": "SNV mutation context"
        }, 
        {
            "location": "/modules/cancer-module-somatic/01_signatures/#running-the-nmf-analysis", 
            "text": "Using the matrix we have made we can now run the non-negative matrix\nfactorisation (NMF) process that attempts to find the most stable,\ngrouping solutions for all of the combinations of mutations and\ncontexts. It does this by trying to find similar patterns, or profiles,\namongst the samples to sort the data into firstly just 2 groups. This is\nrepeated to get replicate values for each attempt and then separating\nthe data by 3 groups, and then 4 and so on.  These parameter choices have been made to keep running time short for\nthis practical. If you have more samples from potentially diverse\nsources you may need to run with a larger range of signatures and with\nmore replicates.  to find out how many signatures we have in the data run the command.  1 gof_nmf  -  assessNumberSignatures ( mm ,   2 : 10 ,  nReplicates  =   5 )    Visualise the results from the NMF processing by making a pdf of the\nplot  1\n2\n3 Cairo(file= plotNumberOfSignatures.pdf , type= pdf , units= in , width=9, height=8, dpi=72)\nplotNumberSignatures(gof_nmf)\ndev.off()   Open up the PDF and examine the curve. The plotNumberOfSignatures PDF\nthat will have been made in the working directory that you set up at the\nbeginning  [H]   [Figure 1:Number of\nsignatures plot]  Look at the y-axis scale on the bottom panel of Figure 1. The explained\nvariance is already very high and so close to finding the correct\nsolution for the number of signatures even with just 2. The error bars\naround each point are fairly small considering we have a very small\nsample set. Deciding how many signatures are present can be tricky but\nhere let\u2019s go for 3. This is where the gradient of both curves have\nstarted to flatten out.  Now run the NMF again but this time stipulating that you want to group\nthe data into 3 different mutational signatures.  1 sigs_nmf = identifySignatures(mm, 3, nmfDecomposition)   Visualise the shape of the profiles for these 3 signatures  1\n2\n3 Cairo(file= plot3Signatures.pdf , type= pdf , units= in , width=10, height=8, dpi=72)\nplotSignatures(sigs_nmf,normalize=TRUE, percent=FALSE) + ggtitle( Somatic Signatures: NMF - Barchart ) + scale_fill_brewer(palette =  Set2 )\ndev.off()   open up the plot3Signatures pdf that will have been made in the working\ndirectory.  You should have generated a plot with three signature profiles obtained\nfrom the NMF processing of the test dataset.  The 96 possible mutation/context combinations are plotted along the x\naxis arranged in blocks of 6 lots of 16 (see information above). The\nheight of the bars indicates the frequency of those particular mutation\nand context combinations in each signature.  Although the section colours are different to the plot you have\ngenerated the mutations are still in the same order across the plot.", 
            "title": "Running the NMF analysis"
        }, 
        {
            "location": "/modules/cancer-module-somatic/01_signatures/#interpreting-the-signature-results", 
            "text": "In their paper Alexandrov et al used this analysis to generate profiles\nfrom the data for more than 7000 tumour samples sequenced through both\nexome and whole genome approaches. They were able to group the data to\nreveal which genomes have been exposed to similar mutational processes\ncontributing to the genome mutations.  [H]   [Figure 2. 21 signature\npatterns identified from the analysis of more than 7000 different\ntumours from Alexandrov et al. Nature 2013.]  Can you match up, by eye, the profile shapes against a selection of\nknown mutational signatures supplied (figure 2)?  Try to match up the patterns made by the positions of the highest peaks\nfor each signature.    Alexandrov signature 7 matches with our signature 1    Alexandrov signature 13 matches with our signature 2    Alexandrov signature 3 matches with our signature 3    Now use the table from Alexandrov et al to identify which mutational\nprocesses our three generated signatures have been associated with.  [H]   [Figure 3 Table\nindicating the probable association of the identified signatures with\nmutational processes and the origin site of the cancer samples from\nAlexandrov et al. Nature 2013.]  What mutational mechanisms have been associated with the signatures that\nyou have generated?    Our signature 1 (AS7) is associated with Ultraviolet radiation\n    damage to DNA. This has previously been identified in Head and Neck\n    and Melanoma cancer samples.    Our signature 2 (AS 13) is associated with the activity of\n    anti-viral APOBEC enzymes. This has previously been seen in Breast\n    and Bladder cancer samples.    Our signature 3 (AS3) is associated with BRCA1 and BRCA2 mutations,\n    i.e. the homologous recombination repair pathway not working\n    properly. This has been seen in Breast, Ovarian and Pancreas cancer\n    samples.    Now we can plot out the results for the individual samples in our\ndataset to show what proportion of their mutations have been assigned to\neach of the signatures.  1\n2\n3 Cairo(file= PlotSampleContribution3Signatures.pdf , type= pdf , units= in , width=9, height=6, dpi=72)\nplotSamples(sigs_nmf, normalize=TRUE) + scale_y_continuous(breaks=seq(0, 1, 0.2), expand = c(0,0))+ theme(axis.text.x = element_text(size=6))\ndev.off()   If you don\u2019t have time to carry out the advanced questions you can exit\nR and return to the normal terminal command line.  1\n2 quit()\nn   Open the resulting PlotSampleContribution3Signatures.pdf.  This shows the results for the mutation grouping for each sample. The\nsamples are listed on the x-axis and the proportion of all mutations for\nthat sample is shown on the y-axis. The colours of the bars indicate\nwhat proportion of the mutations for that sample were grouped into each\nof the signatures. The colour that makes up most of the bar for each\nsample is called its  major signature .  The data you have been using contains samples from High Grade Serous\nOvarian Carcinomas and Cutaneous Melanoma.  Using the major signature found for each sample can you guess which are\novarian and which are melanoma samples?    Samples 9-12 have the majority signature of our signature 1. This is\n    the UV signature and so these are likely to be Melanoma samples.    Samples 4-8 have the majority signature of our signature 3. This is\n    the BRCA signature and these are most likely to be ovarian samples.    Samples 1-3 have the majority signature of our signature 2. This is\n    the APOBEC signature indicating activity of the anti-viral APOBEC\n    enzymes. These are less likely to be from cutaneous melanoma because\n    they have very few UV associated mutations although it could\n    possibly be from a different subtype. However it is much more likely\n    that these will be ovarian tumours as this APOBEC signature has been\n    seen in breast tumours which can be similar to ovarian cancers in\n    terms of the mutated genes.    This is an open question for discussion at the end of the practical.  How can this analysis be useful for cancer genomics studies?  Now rerun the process this time using 4 signatures as the solution.  Hint: you don\u2019t have to start back at the beginning but you can jump to\nthe step where you run the NMF but this time for 4 instead of 3\nsignatures. Then continue through making the plots.  You will need to change the name of each plot you remake with 4\nsignatures because Cairo won\u2019t let you overwrite and existing file.  Can you find a good match in the set of known signatures for all 4\npatterns?  Can you find a verified process for all of the profiles you are seeing?", 
            "title": "Interpreting the signature results"
        }, 
        {
            "location": "/modules/cancer-module-somatic/01_signatures/#references", 
            "text": "Alexandrov et al. Nature 2013:   http://www.nature.com/nature/journal/v500/n7463/pdf/nature12477.pdf  Gehring et al. Bioinformatics 2015:   http://bioinformatics.oxfordjournals.org/content/early/2015/07/31/bioinformatics.btv408.full", 
            "title": "References"
        }, 
        {
            "location": "/modules/cancer-module-somatic/02_intogen/", 
            "text": "Key Learning Outcomes\n\n\nAfter completing this practical the trainee should be able to:\n\n\n\n\n\n\nRun the IntOGen analysis software on cohort mutation data.\n\n\n\n\n\n\nHave gained experience of the structure of the analysis output files\n    in order to identify potential driver genes.\n\n\n\n\n\n\nHave gained overview knowledge of different methods for\n    identification of genes important in cancers.\n\n\n\n\n\n\nResources You\u2019ll be Using\n\n\nTools Used\n\n\nIntOGen mutations platform:\n\n\nhttps://www.intogen.org/search\n\n\nSources of Data\n\n\nTCGA melanoma somatic SNV data from 338 tumour samples:\n\n\nhttps://tcga-data.nci.nih.gov/tcga/\n\n\nUseful Links\n\n\nMutation Annotation Format (MAF) specification:\n\n\nhttps://wiki.nci.nih.gov/display/TCGA/Mutation+Annotation+Format+(MAF)+Specification\n\n\nIntroduction\n\n\nCancer driver genes are commonly described as genes that when mutated\ndirectly affect the potential of a cell to become cancerous. They are\nimportant to a tumour cell as they confer a growth or survival advantage\nover the normal surrounding cells. The mutations in these driver genes\nare then clonally selected for as the population of tumour cells\nincreases. We think of the key genes driving tumour initiation\n(development), progression, metastases, resistance and survival. Driver\ngene mutations are often described as \nearly\n events because they were\nkey in turning a normally functioning and regulated cell into a\ndysregulated one. The logical assumption is that these key mutations\nwill be present in all tumour cells in a patient\u2019s sample; although\nsometime this is not true.\n\n\nThere are two major research goals that underline the need to identify\ndriver genes:\n\n\n\n\n\n\nBy identifying the early changes that take place researchers might\n    be able to find a treatment to stop the root cause of why cells\n    become malignant.\n\n\n\n\n\n\nBy identifying groups of patients with the same genes mutated then\n    we can develop therapies that will work for all of them.\n\n\n\n\n\n\nWhen we sequence tumour samples we tend to use samples that come from\nfully developed cancers that can carry hundreds to thousands of\nmutations in genes and many more outside of genes. The accumulation of\nthese passenger mutations in cancer cells can happen because often the\nrepair mechanisms or damage sensing processes are amongst the first\npathways to become disrupted accelerating the mutational rate. Mutations\nthat occur in genes after the cell has become cancerous may still affect\nthe growth rate, invasiveness and even the response to chemotherapy but\nmay not be present in all cells of a tumour. These genes may be drivers\nof chemo-resistance or metastasis and are equally good targets for\ntherapies.\n\n\nIntOGen-mutations is a platform that aims to identify driver mutations\nusing two methodologies from cancer cohort mutation data: the first\nidentifies mutations that are most likely to have a functional impact\ncombined with identifying genes that are frequently mutated; and the\nsecond, genes that harbour clustered mutations. These measures are all\nindicators of positive selection that occurs in cancer evolution and may\nhelp the identification of driver genes.\n\n\nAnalysing cancer cohort data with IntOGen\n\n\nIntOGen-mutations is a web platform that can allow users to run their\nanalysis on the host\u2019s servers or it can be downloaded and run without\nany limits on the number of analyses on a local server.\n\n\nFor the purposes of the course we will be using a local version of\nIntOGen so that we don\u2019t encounter any issues sharing resources.\n\n\nTo start IntOGen open a terminal and navigate to the\nsomatic/intogen_mutations_analysis directory.\n\n\n1\ncd ~/somatic/intogen_mutations_analysis\n\n\n\n\n\n\nstart IntOGen by typing\n\n\n1\n./run web\n\n\n\n\n\n\nA browser window should open up with the IntOGen home page displayed.\nIntoGen requires you to log in to Mozilla persona.\n\n\nOn the top right hand side of the screen there will be a \nSign in\n\nbutton.\n\n\nClick the \nSign in\n button.\n\n\nIf you can access your email from the web you can create yourself an\naccount. You do need to verify your email address by logging in to your\nwebmail.\n\n\nAlternatively, you can log in using these dummy account details.\n\n\nEmail address:\n\n\n.\n\n\nPassword if required:\n\n\nletmepass.\n\n\nClick on the \nAnalysis\n button on the top tool bar. Click on the\n\nAnalyse your data\n button in the \nCohort analysis\n panel.\n\n\nA form will open up, click on the \nLoad\n button and navigate to the\nTCGA_Melanoma_SMgene.maf file located in the directory path\n\nsomatic/intogen/ TCGA_Melanoma_SMgene.maf\n.\n\n\nEnsure the Genome assembly is set to \nhg19(GRCh37)\n.\n\n\nThe click the \nStart analysis\n button.\n\n\nThe TCGA melanoma maf used in this practical has been modified from the\noriginal to reduce processing time and only contains data for the top\n680 mutated genes.\n\n\nThe tool will take around 10 minutes to run and the progress will be\nindicated by the green bar. You may need to refresh the browser if it\nlooks like there has been no progress for a while.\n\n\nOnce complete the output can be downloaded.\n\n\nClick on the download link to the left of the green bar or go through\nthe results button at the top of the page.\n\n\nSelect to \nsave\n the file which will end up in the download directory\n\n/Downloads\n.\n\n\nClick on the blue download arrow at the top right hand side of the\nbrowser window.\n\n\nClick on the file called \nTCGA_Melanoma_slimSMgene.zip\n.\n\n\nHighlight the file name in the pop-up window and click on \nExtract\n.\n\n\nSelect the \n/trainee/somatic/intogen/\n directory and click on the\n\nExtract\n button at the bottom of the window.\n\n\nClick to show files once extracted.\n\n\nDouble click on the directory called \nTCGA_Melanoma_slimSMgene\n.\n\n\nExploring the output of IntOGen\n\n\nWhen you run your data over the web on the remote site there is a browse\nfacility that allows you to explore your data using the web version of\nthe database. Running IntOGen locally provides the same tabular\ninformation but in a flat file format.\n\n\nThere are 7 files generated in a successful run of IntOGen:\n\n\n\n\n\n\ngenes.tsv - this is the main output summary table.\n\n\n\n\n\n\nvariant_genes.tsv - describes the variants identified in the genes\n\n\n\n\n\n\nvariant_samples.tsv - lists the variants together with the samples\n    Ids\n\n\n\n\n\n\npathways.tsv - lists the Kegg pathway ID for perturbed pathways\n\n\n\n\n\n\nconsequences.tsv - lists consequences of the variants in all known\n    transcripts\n\n\n\n\n\n\nproject.tsv - one line to summarise the input data\n\n\n\n\n\n\nfimpact.gitools.tdm - the output of the functional impact tool\n\n\n\n\n\n\nThe scope of this practical will concentrate on identification of driver\ngenes so we will look at the main output concerning genes. OncodriveFM\ndoes however also calculates a functional impact bias of high impact\nmutations in annotated Kegg pathways \nhttp://www.genome.jp/kegg/pathway.html\n\nas shown in the \npathways.tsv\n file but will not be used in this course.\n\n\nOpen up the \ngenes.tsv\n in the spreadsheet software by double clicking\non the file.\n\n\nThis file contains the overall summary results for the IntOGen pipeline\npresented by gene and reports Q values (i.e. multiple testing corrected\nP values) for the mutation frequency and cluster modules.\n\n\nSignificantly mutated genes from the cohort data are identified using\nthe OncodriveFM module of IntOGen. This tool detects genes that have\naccumulated mutations with a high functional impact. It uses annotations\nfrom the Ensembl variant effect predictor (VEP, V.70) that includes SIFT\nand Polyphen2 and precomputed MutationAssessor functional impacts. It\ncalculates a P value per gene from the number of mutations detected\nacross all possible coding bases of a gene with a positive weighting for\nmutations with a high functional impact.\n\n\nSort the data in this file by two levels starting with the 5\nth\n column,\nthe FM_QVALUE from smallest-to-biggest and then by the 12\nth\n column the\nCLUST_QVALUE also from smallest-to-biggest.\n\n\nThe top nine genes with the smallest Q values should be TP53, PTEN,\nPPP6C, CDKN2A, BRAF, NRAS, ARID2, TTN, IDH1.\n\n\nAll of these have very small FM_Qvalues which means they are all\nsignificantly mutated genes in this TCGA Melanoma cohort of 338\npatients.\n\n\nNow look at their sample frequency (column 9 SAMPLE_FREQ) values these\nare the number of samples that contain at least one mutation in the\ngene.\n\n\n\n\n\n\n\u200ba) Which gene has mutations in the most samples?\n\n\n\n\n\n\n\u200bb) Which gene had the lowest FM Q value?\n\n\n\n\n\n\n\u200bc) Why don\u2019t the genes with the lowest Q values also have the\n    highest sample frequency value?\n\n\n\n\n\n\n\u200ba) TTN has the highest number of samples with mutations. There are\n    265 out of 327 samples with mutations in TTN.\n\n\n\n\n\n\n\u200bb) TP53 or PTEN\n\n\n\n\n\n\n\u200bc) The P value calculation takes into account the length of the\n    coding sequence of the gene, the mutation rate of the nucleotides\n    contained within it and the functional consequences of those\n    changes. Therefore a small gene with a small number of deleterious\n    mutations may have a lower P value and also Q value than a large\n    gene with a high mutation frequency.\n\n\n\n\n\n\nThe results for the assessment of clustered mutations in genes carried\nout by the OncodriveCLUST module of IntOGen are shown in the 10-14\nth\n\ncolumns. The Q value is in column 12, CLUST_QVALUE which indicates if\nthere is a significant grouping of mutations identified. The positions\nof the mutations are now described in terms of the amino acid residue in\nthe encoded protein.\n\n\nThe three oncogenes BRAF, NRAS and IDH1 have very low CLUST_QVALUEs\n\\\n0.01 indicating that the mutations in these genes are highly\nclustered. The CLUST_COORDS column reports that there are 158 samples\nwith mutations between the amino acid positions 594-601 of BRAF; 84\nsamples with mutations at amino acid position 61 of NRAS; and 15 sample\nwith mutations at amino acid position 132 of IDH1.\n\n\nWhy are the oncogenes more likely to have clustered mutations and the\ntumour suppressor genes less likely?\n\n\nGain of function mutations are required to activate oncogenes and so\nonly key residues in the protein will result in activation. Tumour\nsuppressors are frequently affected by loss of function mutations and\ndeletions. A truncating mutation or frameshift indel can occur in any\nexon, except the last one, and have the same deleterious functional\nresult.\n\n\nThe INTOGEN_DRIVER column indicates if this gene is a known cancer\ndriver gene with 1 for yes and 0 for no so it is promising to see the\nmajority of our top hit genes are known drivers.\n\n\nThe XREFS column indicates a mutation match in external databases. This\nmeans the position and the base change has been seen before. For the\nknown Driver genes there are many COSMIC IDs indicating these mutations\nhave been recorded in the Catalogue of Somatic Mutations in Cancer\ndatabase \nhttp://cancer.sanger.ac.uk/cosmic\n. Remember the majority of\ndata in these databases have come from large scale cancer sequencing\nprojects carried out by TCGA and ICGC associated groups.\n\n\nHave a look at the values in the XREFS field for the gene TTN.\n\n\nThere are a large number of COSMIC entries for mutations in this gene\nbut also a large number of ESP ID numbers that refer to the National\nHeart, Lung, and Blood Institute (NHLBI), Exome Sequencing Project (ESP)\n\nhttp://evs.gs.washington.edu/EVS/\n.\n\n\nThis data comes from a diverse population that typically don\u2019t have\ncancer but focus on patients with heart, lung and blood disorders.\n\n\nCan we draw any conclusions from the high number of ESP mutations and\ndbSNP references for TTN?\n\n\nFunctional studies would be the only way to prove conclusively if TTN\nmutations were cancer driver mutations. The high number of samples in\nthe ESP cohort that have mutations in common with the cancer cohorts\ncould indicate that TTN mutation may not be unique to the cancer\nsetting. This conclusion seems to be backed up by the high number of\nmutations also with dbSNP ids that indicates the potential for some of\nthese mutations to be present in the general population.\n\n\nAmongst this high number of mutations there may be groups of patients\nwhere the mutations are purely passenger and other groups where the\nmutations could contribute to the tumour.\n\n\nFrom this data there is no way to tell.\n\n\nThe other files in the output support the information in this sheet.\n\n\nThe \nvariant_genes.tsv\n file includes a summary of all mutation found in\neach of the genes with a count of the number of samples identified to\nhave that mutation. It also reports the variant impact score and\ncategory of which there are four; high, medium, low and none.\n\n\nOpen up the \nvariant_genes.tsv\n file and explore the data.\n\n\nCan you find out what the nucleotide change details for the most common\nBRAF mutation that results in V600E amino acid change in the cohort?\nSort the data on the gene symbol column to make this easier.\n\n\nIt is an A>T at position chr7:140453136 identified in 127 samples.\n\n\nThe \nvariant_samples.tsv\n worksheet allows you to find out the sample\nidentification numbers for the samples with the mutation that you are\ninterested in.\n\n\nOpen up the \nvariant_samples.tsv\n file and explore the data.\n\n\nCan you use the position that you found in the question above to locate\nthe sample IDs for the 3 cases with a V600R BRAF mutation that is caused\nby a nucleotide change of AC/CT starting at chr7:140453136?\n\n\nTCGA-D3-A1QA-06A-11D-A196-08, TCGA-EB-A3XC-01A-11D-A23B-08,\nTCGA-ER-A193-06A-12D-A197-08\n\n\nReferences\n\n\nGunes et al. Nat. Methods 2010\n:   \nhttp://www.nature.com/nmeth/journal/v7/n2/pdf/nmeth0210-92.pdf\n\n\nGonzalez-Perez et al. Nat. Methods 2013\n:   \nhttp://www.nature.com/nmeth/journal/v10/n11/pdf/nmeth.2642.pdf", 
            "title": "Cohort analysis for the identification of driver genes"
        }, 
        {
            "location": "/modules/cancer-module-somatic/02_intogen/#key-learning-outcomes", 
            "text": "After completing this practical the trainee should be able to:    Run the IntOGen analysis software on cohort mutation data.    Have gained experience of the structure of the analysis output files\n    in order to identify potential driver genes.    Have gained overview knowledge of different methods for\n    identification of genes important in cancers.", 
            "title": "Key Learning Outcomes"
        }, 
        {
            "location": "/modules/cancer-module-somatic/02_intogen/#resources-youll-be-using", 
            "text": "", 
            "title": "Resources You\u2019ll be Using"
        }, 
        {
            "location": "/modules/cancer-module-somatic/02_intogen/#tools-used", 
            "text": "IntOGen mutations platform:  https://www.intogen.org/search", 
            "title": "Tools Used"
        }, 
        {
            "location": "/modules/cancer-module-somatic/02_intogen/#sources-of-data", 
            "text": "TCGA melanoma somatic SNV data from 338 tumour samples:  https://tcga-data.nci.nih.gov/tcga/", 
            "title": "Sources of Data"
        }, 
        {
            "location": "/modules/cancer-module-somatic/02_intogen/#useful-links", 
            "text": "Mutation Annotation Format (MAF) specification:  https://wiki.nci.nih.gov/display/TCGA/Mutation+Annotation+Format+(MAF)+Specification", 
            "title": "Useful Links"
        }, 
        {
            "location": "/modules/cancer-module-somatic/02_intogen/#introduction", 
            "text": "Cancer driver genes are commonly described as genes that when mutated\ndirectly affect the potential of a cell to become cancerous. They are\nimportant to a tumour cell as they confer a growth or survival advantage\nover the normal surrounding cells. The mutations in these driver genes\nare then clonally selected for as the population of tumour cells\nincreases. We think of the key genes driving tumour initiation\n(development), progression, metastases, resistance and survival. Driver\ngene mutations are often described as  early  events because they were\nkey in turning a normally functioning and regulated cell into a\ndysregulated one. The logical assumption is that these key mutations\nwill be present in all tumour cells in a patient\u2019s sample; although\nsometime this is not true.  There are two major research goals that underline the need to identify\ndriver genes:    By identifying the early changes that take place researchers might\n    be able to find a treatment to stop the root cause of why cells\n    become malignant.    By identifying groups of patients with the same genes mutated then\n    we can develop therapies that will work for all of them.    When we sequence tumour samples we tend to use samples that come from\nfully developed cancers that can carry hundreds to thousands of\nmutations in genes and many more outside of genes. The accumulation of\nthese passenger mutations in cancer cells can happen because often the\nrepair mechanisms or damage sensing processes are amongst the first\npathways to become disrupted accelerating the mutational rate. Mutations\nthat occur in genes after the cell has become cancerous may still affect\nthe growth rate, invasiveness and even the response to chemotherapy but\nmay not be present in all cells of a tumour. These genes may be drivers\nof chemo-resistance or metastasis and are equally good targets for\ntherapies.  IntOGen-mutations is a platform that aims to identify driver mutations\nusing two methodologies from cancer cohort mutation data: the first\nidentifies mutations that are most likely to have a functional impact\ncombined with identifying genes that are frequently mutated; and the\nsecond, genes that harbour clustered mutations. These measures are all\nindicators of positive selection that occurs in cancer evolution and may\nhelp the identification of driver genes.", 
            "title": "Introduction"
        }, 
        {
            "location": "/modules/cancer-module-somatic/02_intogen/#analysing-cancer-cohort-data-with-intogen", 
            "text": "IntOGen-mutations is a web platform that can allow users to run their\nanalysis on the host\u2019s servers or it can be downloaded and run without\nany limits on the number of analyses on a local server.  For the purposes of the course we will be using a local version of\nIntOGen so that we don\u2019t encounter any issues sharing resources.  To start IntOGen open a terminal and navigate to the\nsomatic/intogen_mutations_analysis directory.  1 cd ~/somatic/intogen_mutations_analysis   start IntOGen by typing  1 ./run web   A browser window should open up with the IntOGen home page displayed.\nIntoGen requires you to log in to Mozilla persona.  On the top right hand side of the screen there will be a  Sign in \nbutton.  Click the  Sign in  button.  If you can access your email from the web you can create yourself an\naccount. You do need to verify your email address by logging in to your\nwebmail.  Alternatively, you can log in using these dummy account details.  Email address:  .  Password if required:  letmepass.  Click on the  Analysis  button on the top tool bar. Click on the Analyse your data  button in the  Cohort analysis  panel.  A form will open up, click on the  Load  button and navigate to the\nTCGA_Melanoma_SMgene.maf file located in the directory path somatic/intogen/ TCGA_Melanoma_SMgene.maf .  Ensure the Genome assembly is set to  hg19(GRCh37) .  The click the  Start analysis  button.  The TCGA melanoma maf used in this practical has been modified from the\noriginal to reduce processing time and only contains data for the top\n680 mutated genes.  The tool will take around 10 minutes to run and the progress will be\nindicated by the green bar. You may need to refresh the browser if it\nlooks like there has been no progress for a while.  Once complete the output can be downloaded.  Click on the download link to the left of the green bar or go through\nthe results button at the top of the page.  Select to  save  the file which will end up in the download directory /Downloads .  Click on the blue download arrow at the top right hand side of the\nbrowser window.  Click on the file called  TCGA_Melanoma_slimSMgene.zip .  Highlight the file name in the pop-up window and click on  Extract .  Select the  /trainee/somatic/intogen/  directory and click on the Extract  button at the bottom of the window.  Click to show files once extracted.  Double click on the directory called  TCGA_Melanoma_slimSMgene .", 
            "title": "Analysing cancer cohort data with IntOGen"
        }, 
        {
            "location": "/modules/cancer-module-somatic/02_intogen/#exploring-the-output-of-intogen", 
            "text": "When you run your data over the web on the remote site there is a browse\nfacility that allows you to explore your data using the web version of\nthe database. Running IntOGen locally provides the same tabular\ninformation but in a flat file format.  There are 7 files generated in a successful run of IntOGen:    genes.tsv - this is the main output summary table.    variant_genes.tsv - describes the variants identified in the genes    variant_samples.tsv - lists the variants together with the samples\n    Ids    pathways.tsv - lists the Kegg pathway ID for perturbed pathways    consequences.tsv - lists consequences of the variants in all known\n    transcripts    project.tsv - one line to summarise the input data    fimpact.gitools.tdm - the output of the functional impact tool    The scope of this practical will concentrate on identification of driver\ngenes so we will look at the main output concerning genes. OncodriveFM\ndoes however also calculates a functional impact bias of high impact\nmutations in annotated Kegg pathways  http://www.genome.jp/kegg/pathway.html \nas shown in the  pathways.tsv  file but will not be used in this course.  Open up the  genes.tsv  in the spreadsheet software by double clicking\non the file.  This file contains the overall summary results for the IntOGen pipeline\npresented by gene and reports Q values (i.e. multiple testing corrected\nP values) for the mutation frequency and cluster modules.  Significantly mutated genes from the cohort data are identified using\nthe OncodriveFM module of IntOGen. This tool detects genes that have\naccumulated mutations with a high functional impact. It uses annotations\nfrom the Ensembl variant effect predictor (VEP, V.70) that includes SIFT\nand Polyphen2 and precomputed MutationAssessor functional impacts. It\ncalculates a P value per gene from the number of mutations detected\nacross all possible coding bases of a gene with a positive weighting for\nmutations with a high functional impact.  Sort the data in this file by two levels starting with the 5 th  column,\nthe FM_QVALUE from smallest-to-biggest and then by the 12 th  column the\nCLUST_QVALUE also from smallest-to-biggest.  The top nine genes with the smallest Q values should be TP53, PTEN,\nPPP6C, CDKN2A, BRAF, NRAS, ARID2, TTN, IDH1.  All of these have very small FM_Qvalues which means they are all\nsignificantly mutated genes in this TCGA Melanoma cohort of 338\npatients.  Now look at their sample frequency (column 9 SAMPLE_FREQ) values these\nare the number of samples that contain at least one mutation in the\ngene.    \u200ba) Which gene has mutations in the most samples?    \u200bb) Which gene had the lowest FM Q value?    \u200bc) Why don\u2019t the genes with the lowest Q values also have the\n    highest sample frequency value?    \u200ba) TTN has the highest number of samples with mutations. There are\n    265 out of 327 samples with mutations in TTN.    \u200bb) TP53 or PTEN    \u200bc) The P value calculation takes into account the length of the\n    coding sequence of the gene, the mutation rate of the nucleotides\n    contained within it and the functional consequences of those\n    changes. Therefore a small gene with a small number of deleterious\n    mutations may have a lower P value and also Q value than a large\n    gene with a high mutation frequency.    The results for the assessment of clustered mutations in genes carried\nout by the OncodriveCLUST module of IntOGen are shown in the 10-14 th \ncolumns. The Q value is in column 12, CLUST_QVALUE which indicates if\nthere is a significant grouping of mutations identified. The positions\nof the mutations are now described in terms of the amino acid residue in\nthe encoded protein.  The three oncogenes BRAF, NRAS and IDH1 have very low CLUST_QVALUEs\n\\ 0.01 indicating that the mutations in these genes are highly\nclustered. The CLUST_COORDS column reports that there are 158 samples\nwith mutations between the amino acid positions 594-601 of BRAF; 84\nsamples with mutations at amino acid position 61 of NRAS; and 15 sample\nwith mutations at amino acid position 132 of IDH1.  Why are the oncogenes more likely to have clustered mutations and the\ntumour suppressor genes less likely?  Gain of function mutations are required to activate oncogenes and so\nonly key residues in the protein will result in activation. Tumour\nsuppressors are frequently affected by loss of function mutations and\ndeletions. A truncating mutation or frameshift indel can occur in any\nexon, except the last one, and have the same deleterious functional\nresult.  The INTOGEN_DRIVER column indicates if this gene is a known cancer\ndriver gene with 1 for yes and 0 for no so it is promising to see the\nmajority of our top hit genes are known drivers.  The XREFS column indicates a mutation match in external databases. This\nmeans the position and the base change has been seen before. For the\nknown Driver genes there are many COSMIC IDs indicating these mutations\nhave been recorded in the Catalogue of Somatic Mutations in Cancer\ndatabase  http://cancer.sanger.ac.uk/cosmic . Remember the majority of\ndata in these databases have come from large scale cancer sequencing\nprojects carried out by TCGA and ICGC associated groups.  Have a look at the values in the XREFS field for the gene TTN.  There are a large number of COSMIC entries for mutations in this gene\nbut also a large number of ESP ID numbers that refer to the National\nHeart, Lung, and Blood Institute (NHLBI), Exome Sequencing Project (ESP) http://evs.gs.washington.edu/EVS/ .  This data comes from a diverse population that typically don\u2019t have\ncancer but focus on patients with heart, lung and blood disorders.  Can we draw any conclusions from the high number of ESP mutations and\ndbSNP references for TTN?  Functional studies would be the only way to prove conclusively if TTN\nmutations were cancer driver mutations. The high number of samples in\nthe ESP cohort that have mutations in common with the cancer cohorts\ncould indicate that TTN mutation may not be unique to the cancer\nsetting. This conclusion seems to be backed up by the high number of\nmutations also with dbSNP ids that indicates the potential for some of\nthese mutations to be present in the general population.  Amongst this high number of mutations there may be groups of patients\nwhere the mutations are purely passenger and other groups where the\nmutations could contribute to the tumour.  From this data there is no way to tell.  The other files in the output support the information in this sheet.  The  variant_genes.tsv  file includes a summary of all mutation found in\neach of the genes with a count of the number of samples identified to\nhave that mutation. It also reports the variant impact score and\ncategory of which there are four; high, medium, low and none.  Open up the  variant_genes.tsv  file and explore the data.  Can you find out what the nucleotide change details for the most common\nBRAF mutation that results in V600E amino acid change in the cohort?\nSort the data on the gene symbol column to make this easier.  It is an A>T at position chr7:140453136 identified in 127 samples.  The  variant_samples.tsv  worksheet allows you to find out the sample\nidentification numbers for the samples with the mutation that you are\ninterested in.  Open up the  variant_samples.tsv  file and explore the data.  Can you use the position that you found in the question above to locate\nthe sample IDs for the 3 cases with a V600R BRAF mutation that is caused\nby a nucleotide change of AC/CT starting at chr7:140453136?  TCGA-D3-A1QA-06A-11D-A196-08, TCGA-EB-A3XC-01A-11D-A23B-08,\nTCGA-ER-A193-06A-12D-A197-08", 
            "title": "Exploring the output of IntOGen"
        }, 
        {
            "location": "/modules/cancer-module-somatic/02_intogen/#references", 
            "text": "Gunes et al. Nat. Methods 2010\n:    http://www.nature.com/nmeth/journal/v7/n2/pdf/nmeth0210-92.pdf  Gonzalez-Perez et al. Nat. Methods 2013\n:    http://www.nature.com/nmeth/journal/v10/n11/pdf/nmeth.2642.pdf", 
            "title": "References"
        }, 
        {
            "location": "/modules/denovo-module-cli/commandline/", 
            "text": "Key Learning Outcomes\n\n\nAfter completing this practical the trainee should be able to:\n\n\n\n\n\n\nFamiliarise yourself with the command line environment on a Linux\n    operating system.\n\n\n\n\n\n\nRun some basic linux system and file operation commands\n\n\n\n\n\n\nNavigration of biological data files structure and manipulation\n\n\n\n\n\n\nResources You\u2019ll be Using\n\n\nTools Used\n\n\n[style=multiline,labelindent=0cm,align=left,leftmargin=0.5cm]\n\n\nBasic Linux system commands on an Ubuntu OS.\n\n\nBasic file operation commands\n\n\nUseful Links\n\n\nSoftware Carpentry:\n\n\nhttps://software-carpentry.org\n\n\n1000Genome Project data for example:\n\n\nhttp://www.1000genomes.org\n\n\nShell Exercise\n\n\nLet\u2019s try out your new shell skills on some real data.\n\n\nThe file \n1000gp.vcf\n is a small sample (1%) of a very large text file\ncontaining human genetics data. Specifically, it describes genetic\nvariation in three African individuals sequenced as part of the 1000\nGenomes Project(\nhttp://www.1000genomes.org\n). The \u2018vcf\u2019 extension lets\nus know that it\u2019s in a specific text format, namely \u2018Variant Call\nFormat\u2019 The file starts with a bunch of comment lines (they start with\n\u2018#\u2019 or \u2018##\u2019), and then a large number of data lines. This VCF file\nlists the differences between the three African individuals and a\nstandard \u2018individual\u2019 called the reference (actually based upon a few\ndifferent people). Each line in the file corresponds to a difference.\nThe line tells us the position of the difference (chromosome and\nposition), the genetic sequence in the reference, and the corresponding\nsequence in each of the three Africans. Before we start processing the\nfile, let\u2019s get a high-level view of the file that we\u2019re about to work\nwith.\n\n\nOpen the Terminal and go to the directory where the data are stored:\n\n\n1\n2\n3\n4\n5\ncd /home/trainee/cli\nls\npwd\nls -lh 1000gp.vcf\nwc -l 1000gp.vcf\n\n\n\n\n\n\nWhat\u2019s the file size (in kilo-bytes), and how many lines are in the\nfile?. (Hint: \nman ls\n, \nman wc\n)\\\n\n\n3.6M\\\n45034 lines\\\n\n\nBecause this file is so large, you\u2019re going to almost always want to\npipe (\n`) the result of any command to less (a simple text viewer, type\u2018\nq`\u2019 to exit) or head (to print the first 10 lines) so that you don\u2019t\naccidentally print 45,000 lines to the screen.\n\n\nLet\u2019s start by printing the first 5 lines to see what it looks like.\n\n\n1\nhead -5 1000gp.vcf\n\n\n\n\n\n\nThat isn\u2019t very interesting; it\u2019s just a bunch of the comments at the\nbeginning of the file (they all start with \uff40#\u2019)!\n\n\nPrint the first 20 lines to see more of the file.\n\n\n1\nhead -20 1000gp.vcf\n\n\n\n\n\n\nOkay, so now we can see the basic structure of the file. A few comment\nlines that start with \uff40#\u2019 or \uff40##\u2019 and then a bunch of lines of data\nthat contain all the data and are pretty hard to understand. Each line\nof data contains the same number of fields, and all fields are separated\nwith TABs. These fields are:\n\n\n\n\n\n\nthe chromosome (which volume the difference is in)\n\n\n\n\n\n\nthe position (which character in the volume the difference starts\n    at)\n\n\n\n\n\n\nthe ID of the difference\n\n\n\n\n\n\nthe sequence in the reference human(s)\n\n\n\n\n\n\nThe rest of the columns tell us, in a rather complex way, a bunch of\nadditional information about that position, including: the predicted\nsequence for each of the three Africans and how confident the scientists\nare that these sequences are correct.\n\n\nTo start analyzing the actual data, we have to remove the header.\n\n\nHow can we print the first 10 non-header lines (those that don\u2019t start\nwith a \uff40#\u2019)?(Hint: \nman grep\n (remember to use pipes ``))\n\n\n1\ngrep -v \n\\^\\#\n 1000gp.vcf | head\n\n\n\n\n\n\nHow many lines of data are in the file (rather than counting the number\nof header lines and subtracting, try just counting the number of data\nlines)?\\\n\n\n1\ngrep -v \n\\^\\#\n 1000gp.vcf | wc -l (should print 45024)\n\n\n\n\n\n\nWhere these differences are located can be important. If all the\ndifferences between two encyclopedias were in just the first volume,\nthat would be interesting. The first field of each data line is the name\nof the chromosome that the difference occurs on (which volume we\u2019re on).\n\n\nPrint the first 10 chromosomes, one per line.\n\n\nHint: \nman cut\n (remember to remove header lines first)\n\n\n1\ngrep -v \n\\^\\#\n 1000gp.vcf | cut -f 1 | head\n\n\n\n\n\n\nAs you should have observed, the first 10 lines are on numbered\nchromosomes. Every normal cell in your body has 23 pairs of chromosomes,\n22 pairs of \u2018autosomal\u2019 chromosomes (these are numbered 1-22) and a pair\nof sex chromosomes (two Xs if you\u2019re female, an X and a Y if you\u2019re\nmale).\n\n\nLet\u2019s look at which chromosomes these variations are on.\n\n\nPrint a list of the chromosomes that are in the file (each chromosome\nname should only be printed once, so you should only print 23 lines).\n\n\nHint: remove all duplicates from your previous answer (\nman sort\n)\n\n\n1\ngrep -v \n\\^\\#\n 1000gp.vcf | cut -f 1 | sort -u\n\n\n\n\n\n\nRather than using \nsort\n to print unique results, a common pipeline is\nto first sort and then pipe to another UNIX command, \nuniq\n. The \nuniq\n\ncommand takes sorted input and prints only unique lines, but it provides\nmore flexibility than just using sort by itself. Keep in mind, if the\ninput isn\u2019t sorted, \nuniq\n won\u2019t work properly.\n\n\nUsing \nsort\n and \nuniq\n, print the number of times each chromosome\noccurs in the file.\n\n\nHint: \nman uniq\n\n\n1\ngrep -v \n\\^\\#\n 1000gp.vcf | cut -f 1 | sort | uniq -c\n\n\n\n\n\n\nAdd to your previous solution to list the chromosomes from most\nfrequently observed to least frequently observed.\n\n\nHint: Make sure you\u2019re sorting in descending order. By default, sort\nsorts in ascending order.\n\n\n1\ngrep -v \n\\^\\#\n 1000gp.vcf | cut -f 1 | sort | uniq -c | sort -n -r\n\n\n\n\n\n\nThis is great, but biologists might also like to see the chromosomes\nordered by their number (not dictionary order), since different\nchromosomes have different attributes and this ordering allows them to\nfind a specific chromosome more easily.\n\n\nSort the previous output by chromosome number\n\n\nHint: A lot of the power of sort comes from the fact that you can\nspecify which fields to sort on, and the order in which to sort them. In\nthis case you only need to sort on one field.\n\n\n1\ngrep -v \n\\^\\#\n 1000gp.vcf | cut -f 1 | sort | uniq -c | sort -k 2n", 
            "title": "DeNovo Command Line"
        }, 
        {
            "location": "/modules/denovo-module-cli/commandline/#key-learning-outcomes", 
            "text": "After completing this practical the trainee should be able to:    Familiarise yourself with the command line environment on a Linux\n    operating system.    Run some basic linux system and file operation commands    Navigration of biological data files structure and manipulation", 
            "title": "Key Learning Outcomes"
        }, 
        {
            "location": "/modules/denovo-module-cli/commandline/#resources-youll-be-using", 
            "text": "", 
            "title": "Resources You\u2019ll be Using"
        }, 
        {
            "location": "/modules/denovo-module-cli/commandline/#tools-used", 
            "text": "[style=multiline,labelindent=0cm,align=left,leftmargin=0.5cm]  Basic Linux system commands on an Ubuntu OS.  Basic file operation commands", 
            "title": "Tools Used"
        }, 
        {
            "location": "/modules/denovo-module-cli/commandline/#useful-links", 
            "text": "Software Carpentry:  https://software-carpentry.org  1000Genome Project data for example:  http://www.1000genomes.org", 
            "title": "Useful Links"
        }, 
        {
            "location": "/modules/denovo-module-cli/commandline/#shell-exercise", 
            "text": "Let\u2019s try out your new shell skills on some real data.  The file  1000gp.vcf  is a small sample (1%) of a very large text file\ncontaining human genetics data. Specifically, it describes genetic\nvariation in three African individuals sequenced as part of the 1000\nGenomes Project( http://www.1000genomes.org ). The \u2018vcf\u2019 extension lets\nus know that it\u2019s in a specific text format, namely \u2018Variant Call\nFormat\u2019 The file starts with a bunch of comment lines (they start with\n\u2018#\u2019 or \u2018##\u2019), and then a large number of data lines. This VCF file\nlists the differences between the three African individuals and a\nstandard \u2018individual\u2019 called the reference (actually based upon a few\ndifferent people). Each line in the file corresponds to a difference.\nThe line tells us the position of the difference (chromosome and\nposition), the genetic sequence in the reference, and the corresponding\nsequence in each of the three Africans. Before we start processing the\nfile, let\u2019s get a high-level view of the file that we\u2019re about to work\nwith.  Open the Terminal and go to the directory where the data are stored:  1\n2\n3\n4\n5 cd /home/trainee/cli\nls\npwd\nls -lh 1000gp.vcf\nwc -l 1000gp.vcf   What\u2019s the file size (in kilo-bytes), and how many lines are in the\nfile?. (Hint:  man ls ,  man wc )\\  3.6M\\\n45034 lines\\  Because this file is so large, you\u2019re going to almost always want to\npipe ( `) the result of any command to less (a simple text viewer, type\u2018 q`\u2019 to exit) or head (to print the first 10 lines) so that you don\u2019t\naccidentally print 45,000 lines to the screen.  Let\u2019s start by printing the first 5 lines to see what it looks like.  1 head -5 1000gp.vcf   That isn\u2019t very interesting; it\u2019s just a bunch of the comments at the\nbeginning of the file (they all start with \uff40#\u2019)!  Print the first 20 lines to see more of the file.  1 head -20 1000gp.vcf   Okay, so now we can see the basic structure of the file. A few comment\nlines that start with \uff40#\u2019 or \uff40##\u2019 and then a bunch of lines of data\nthat contain all the data and are pretty hard to understand. Each line\nof data contains the same number of fields, and all fields are separated\nwith TABs. These fields are:    the chromosome (which volume the difference is in)    the position (which character in the volume the difference starts\n    at)    the ID of the difference    the sequence in the reference human(s)    The rest of the columns tell us, in a rather complex way, a bunch of\nadditional information about that position, including: the predicted\nsequence for each of the three Africans and how confident the scientists\nare that these sequences are correct.  To start analyzing the actual data, we have to remove the header.  How can we print the first 10 non-header lines (those that don\u2019t start\nwith a \uff40#\u2019)?(Hint:  man grep  (remember to use pipes ``))  1 grep -v  \\^\\#  1000gp.vcf | head   How many lines of data are in the file (rather than counting the number\nof header lines and subtracting, try just counting the number of data\nlines)?\\  1 grep -v  \\^\\#  1000gp.vcf | wc -l (should print 45024)   Where these differences are located can be important. If all the\ndifferences between two encyclopedias were in just the first volume,\nthat would be interesting. The first field of each data line is the name\nof the chromosome that the difference occurs on (which volume we\u2019re on).  Print the first 10 chromosomes, one per line.  Hint:  man cut  (remember to remove header lines first)  1 grep -v  \\^\\#  1000gp.vcf | cut -f 1 | head   As you should have observed, the first 10 lines are on numbered\nchromosomes. Every normal cell in your body has 23 pairs of chromosomes,\n22 pairs of \u2018autosomal\u2019 chromosomes (these are numbered 1-22) and a pair\nof sex chromosomes (two Xs if you\u2019re female, an X and a Y if you\u2019re\nmale).  Let\u2019s look at which chromosomes these variations are on.  Print a list of the chromosomes that are in the file (each chromosome\nname should only be printed once, so you should only print 23 lines).  Hint: remove all duplicates from your previous answer ( man sort )  1 grep -v  \\^\\#  1000gp.vcf | cut -f 1 | sort -u   Rather than using  sort  to print unique results, a common pipeline is\nto first sort and then pipe to another UNIX command,  uniq . The  uniq \ncommand takes sorted input and prints only unique lines, but it provides\nmore flexibility than just using sort by itself. Keep in mind, if the\ninput isn\u2019t sorted,  uniq  won\u2019t work properly.  Using  sort  and  uniq , print the number of times each chromosome\noccurs in the file.  Hint:  man uniq  1 grep -v  \\^\\#  1000gp.vcf | cut -f 1 | sort | uniq -c   Add to your previous solution to list the chromosomes from most\nfrequently observed to least frequently observed.  Hint: Make sure you\u2019re sorting in descending order. By default, sort\nsorts in ascending order.  1 grep -v  \\^\\#  1000gp.vcf | cut -f 1 | sort | uniq -c | sort -n -r   This is great, but biologists might also like to see the chromosomes\nordered by their number (not dictionary order), since different\nchromosomes have different attributes and this ordering allows them to\nfind a specific chromosome more easily.  Sort the previous output by chromosome number  Hint: A lot of the power of sort comes from the fact that you can\nspecify which fields to sort on, and the order in which to sort them. In\nthis case you only need to sort on one field.  1 grep -v  \\^\\#  1000gp.vcf | cut -f 1 | sort | uniq -c | sort -k 2n", 
            "title": "Shell Exercise"
        }, 
        {
            "location": "/modules/denovo-module-bac/bac_asm/", 
            "text": "Key Learning Outcomes\n\n\nAfter completing this module the trainee should be able to:\n\n\n\n\n\n\nPerform a simple genome assembly for a small organism using \nVelvet\n\n\n\n\n\n\nVisualise the assembly graph using \nBandage\n\n\n\n\n\n\nBe aware of the effects and trade-offs of the parameter \nK\n on the\n    genome assembly\n\n\n\n\n\n\nUnderstand that genome assembly produces a graph structure\n\n\n\n\n\n\nResources You\u2019ll be Using\n\n\nTools Used\n\n\nVelvet:\n\n\nhttps://www.ebi.ac.uk/~zerbino/velvet/\n\n\nBandage:\n\n\nhttps://rrwick.github.io/Bandage/\n\n\nData Source and Useful Links\n\n\nData used in this tutorial is from ENA accession SRR2054105\n\n\nhttp://www.ebi.ac.uk/ena/data/view/SRR2054105\n\n\nSpreadsheet used in the group activity:\n\n\nhttps://docs.google.com/spreadsheets/d/1iFbCCihawpY1LClsAB-OJ66lyeW7EsdJyaMo1HCetF8/edit?usp=sharing\n\n\nIntroduction\n\n\nIn this tutorial we will take raw sequencing reads and \nde novo\n\nassemble them into contigs. We will also explore the internal assembly\ngraph structure to aid our understanding of why assemblies are\nincomplete.\n\n\nBefore the assembly\n\n\nSequence data\n\n\nWe have sequenced the genome of a bacterium using paired-end chemistry\non an Illumina HiSeq 2000 instrument at the University of Oxford and\npublicly available as SRR2054105 the Europen Nucleotide Archive (ENA).\n\n\nLet\u2019s have a look at the datasets by doing:\n\n\n1\n2\n cd /home/trainee/bacterial\n ls\n\n\n\n\n\n\nYou should see the following two files:\n\n\nR1.fastq.gz\n\n\nR2.fastq.gz\n\n\nHow many reads are in this data set?\\\n\n\n2100778 pairs\n\n\nWhat is the yield in basepairs?\\\n\n\n210077800 bp = 210 Mbp\n\n\nAssuming an average bacterial genome size of 4 Mbp, what depth of\ncoverage do we have?\\\n\n\n210 / 4 = 53x\n\n\nTrim or clip reads\n\n\nThere are two distinct reasons one may wish to trim or clip the raw\nreads.\n\n\n\n\n\n\nLow quality bases typically occur toward the end of Illumina reads.\n    The lower the quality score, the higher the chance that the base is\n    incorrect. This introduces false k-mers into the assembly process. A\n    good assembler should handle these gracefully.\n\n\n\n\n\n\nSequencing adaptors are artificial sequence that can occur at the\n    end of reads that came from fragments of DNA that were shorter than\n    desired. They were not in the original genome and their presence in\n    lots of reads will totally confuse the assembler.\n\n\n\n\n\n\nNormally one would use \nfastqc\n or similar to determine whether quality\ntrimming is warranted and for the presence of sequencing adaptors. The\ndata set in this exercise is 99% free of adaptors and is good quality\noverall so we will skip those steps.\n\n\nHow do you know which adaptor sequence to trim?\\\n\n\nYou either ask the sequencing provider or use a tool that tries all\nknown adapters.\n\n\nHow are are quality values in \nFASTQ\n files calculated? Can we trust\nthem?\\\n\n\nThe are calculated using formulas calibrated to the physical\nmeasurements the sequencing instrument takes. In the case of Illumina\nsequencing, these measurements will include pixel light intensities from\nthe images the camera/scanner takes of the flow cell.\n\n\nAllocate yourself to a \nK\n value on the spreadsheet\n\n\nAn important parameter to most genome assemblers is \nK\n which is the\nk-mer size used to construct the \nde Bruijn\n graph (pronounced \nde\nBrown\n) .\n\n\nPlease go to this shared online\nspreadsheet \nhttps://docs.google.com/spreadsheets/d/1iFbCCihawpY1LClsAB-OJ66lyeW7EsdJyaMo1HCetF8/edit?usp=sharing\n\nand choose \na K value\n and put your name next to it.\n\n\nAssemble the reads using Velvet\n\n\nThe \nVelvet\n software performs the assembly in two steps. The first\n\nvelveth\n step hashes the reads. The second \nvelvetg\n step builds,\ncleans and traverses the graph.\n\n\nChoose an output folder \nDIR\n and use the \nK\n value you were allocated\non the spreadsheet and assemble the reads.\n\n\n1\nvelveth DIR K -shortPaired -fastq.gz -separate R1.fastq.gz R2.fastq.gz\n\n\n\n\n\n\nDescription of the arguments used in the command:\n\n\nDIR\n:   = Directory name you chose to write the output to\n\n\nK\n:   = K-mer value; it must be an odd number\n\n\n-shortPaired\n:   = The read types are short and paired-end\n\n\n-fastq.gz\n:   = The format of the input files\n\n\n-separate\n:   = R1 and R2 reads are in separate files\n\n\ninput file names\n:   = input FASTQ file names\n\n\nRun the velvetg step now:\n\n\n1\n/usr/bin/time -f \n%e\n velvetg DIR -exp_cov auto -cov_cutoff auto\n\n\n\n\n\n\nDescription of the arguments used in the command:\n\n\ntime\n:   = to capture the \nvelvetg\n command run time\n\n\nDIR\n:   = Directory name you chose to write the velveth output to\n\n\n-exp_cov\n:   = expected coverage is set to \nauto\n. To be determined by \nvelvet\n\n\n-cov_cutoff\n:   = coverage cutoff is set to \nauto\n. To be determined by \nvelvet\n\n\nAdd your results to the spreadsheet\n\n\n[H]\n\n\nll\n\n\nColumn\n \n \nWhere to find the value\n\\\n\nK-mer size\n \n You chose this earlier and used it in the velveth\ncommand\\\n\nHow long it took to run\n \n Look at the final output line for the user\ntime in seconds: NN.N\\\n\nAverage K-mer coverage\n \n Look for Estimated Coverage = NN.N\\\n\nNumber of contigs\n \n Look for text Final graph has NNN nodes\\\n\nN50 contig size\n \n Look for n50 of NNNNN\\\n\nLargest contig size\n \n Look for max NNNNN\\\n\nTotal number of basepairs (sum of contig lengths)\n \n Look for total\nNNNNNNN\\\n\n\n[tab:results]\n\n\nEffect of K on assembly\n\n\nExamine the spreadsheet and look for patterns in the tabular data and\ncorresponding bar/line charts.\n\n\nHow does K affect each of the statistics? Which value of K do you think\nis doing the best job? Why?\n\n\nOutput Files\n\n\n[H]\n\n\nll\n\n\nFilename\n \n \nDescription\n\\\n\ncontigs.fa\n \n this is a FASTA file with your contigs\\\n\nLog\n \n has most of the metrics in it that we recorded\\\n\nstats.txt\n \n TSV file of length and coverage of individual contigs\\\n\nSequences\n \n A copy of the input FASTQ sequences in FASTA format\\\n\nPregraph Roadmaps Graph2\n \n Interim assembly graph structure\\\n\nLastGraph\n \n Final graph structure\\\n\n\n[tab:velvet\no\nut]\n\n\nLet\u2019s examine the \nstats.txt\n file and look at the \nshort1_cov\n column\nwhich is the k-mer coverage of each contig:\n\n\n1\ncut -f6 dir/stats.txt | less\n\n\n\n\n\n\nPress \nSPACE\n and \nb\n to scroll up and down, and press \nq\n to quote.\n\n\nWhat do you notice about the k-mer coverages?\\\n\n\nThey all appear to have a similar value, but there are some that do not\nfit the pattern.\n\n\nWhat do the outliers correspond to?\\\n\n\nRepetitive elements of the genome including gene duplications. Also\nreplicons with differing copy numbers, like bacterial plasmids.\n\n\n1\ngrep NN dir/contigs.fa\n\n\n\n\n\n\nWhy is there N letters in the assembly?\\\n\n\nPaired-end reads come from each end of the same original fragment of\nDNA. The unsequenced gap in the middle is variable, and unknown. The\nassembler sometimes struggles to resolve these gaps. So it knows two\nsections of the genome are connected, but it doesn\u2019t quite know what is\nbetween them. So it pads them with its best guess of how many bases\nthere are, and uses N to denote them as unknown.\n\n\nVisualising the assembly graph using Bandage\n\n\nThe final graph that \nVelvet\n uses is stored in the file \nLastGraph\n.\nThe \nBandage\n software allows us to view and explore the assembly graph.\n\n\n\n\n\n\nStart \nBandage\n.\n\n\n\n\n\n\nGo to \nFile -\n Load Graph\n and load the \nLastGraph\n from your\n    assembly in \nDIR\n. This may take a little while so be patient.\n\n\n\n\n\n\nMaximize your window to fill up the whole screen.\n\n\n\n\n\n\nClick \nDraw graph\n on the left hand panel.\n\n\n\n\n\n\nChange \nRandom colours\n to \nColour by read depth\n on the left hand\n    panel.\n\n\n\n\n\n\nNow get up out of your chair and walks around and look at all the\ndifferent graphs the other participants obtained with different values\nof \nK\n.\n\n\nHow does \nK\n affect the graph?\\\n\n\nSmall K produces higher k-mer coverage but also more connectivity as\nsmaller K is more ambiguous.\n\n\nWhat would the graph look like in an ideal situtation?\\\n\n\nIf the genome had M replicons, we would like to see only M sub-graphs.\nEach sub-graph would be linear or circular, depending on the form of the\nreplicon in the original organism.\n\n\nWhy didn\u2019t anyone achieve perfection?\\\n\n\nShort reads are unable to disambiguate repeats longer than the read\nlength (or read span). Most genomes have repeats beyond 500 bp.\n\n\nHere is another example\n\nhttps://github.com/rrwick/Bandage/wiki/Effect-of-kmer-size\n from the\nBandage web site on how K can affect assembly graphs.\n\n\nExplore the graph\n\n\nBandage\n is designed to be an interactive tool. It allows you to see\nrelationships between parts of your genome that are lost when you only\nlook at the FASTA file of contigs.\n\n\n\n\n\n\nZoom in using the \nZoom\n up/down button in the left hand panel\n\n\n\n\n\n\nPan around by holding down the Option/Windows key and dragging on\n    the background\n\n\n\n\n\n\nMove nodes out of the way by selecting and dragging\n\n\n\n\n\n\nFeel free to play around a bit and ask questions.\n\n\nFeatures of the assembly graph\n\n\nThe graph is quite tangled. The long stretches correspond to contigs.\nThe intersections correspond to shared k-mers in the graph, which occur\ndue to repeated sequences within the genome.\n\n\nSelect an node (rectangle) in the graph. It\u2019s length is reported in the\nright hand panel as \nLength\n:\n \nNNN\n.\n\n\nOn the menu choose \nOutput -\n Web BLAST selected node\n. Your browser\nshould open the NCBI BLAST Website\n\nhttp://blast.ncbi.nlm.nih.gov/Blast.cgi?PROGRAM=blastn\nPAGE_TYPE=BlastSearch\nLINK_LOC=blasthome\n.\n\n\nClick the \nBLAST\n button at the bottom, and wait for the result.\n\n\nDid your node match anything in the Genbank database?\\\n\n\nYou will need to examine the BLAST report on the trainee browser window.\n\n\nCan you determine what species of bacteria was sequenced?\\\n\n\nThe top hit of the small segment of DNA the trainee chose may not\nnecessarily reflect the true species of bacteria. But if multiple\nsegments produce consistent top hits to the same species you would have\nsome confidence.\n\n\nConclusion\n\n\nYou should now:\n\n\n\n\n\n\nknow how to use Velvet to assemble a simpel genome from Illumina\n    sequences\n\n\n\n\n\n\nunderstand the role of the k-mer length K in the assembly process\n\n\n\n\n\n\nbe able to relate the graph structure to the final contigs\n\n\n\n\n\n\nrealize the limitations of short read sequences with respect to\n    genome complexity\n\n\n\n\n\n\nThank you.", 
            "title": "Bacterial Assembly"
        }, 
        {
            "location": "/modules/denovo-module-bac/bac_asm/#key-learning-outcomes", 
            "text": "After completing this module the trainee should be able to:    Perform a simple genome assembly for a small organism using  Velvet    Visualise the assembly graph using  Bandage    Be aware of the effects and trade-offs of the parameter  K  on the\n    genome assembly    Understand that genome assembly produces a graph structure", 
            "title": "Key Learning Outcomes"
        }, 
        {
            "location": "/modules/denovo-module-bac/bac_asm/#resources-youll-be-using", 
            "text": "", 
            "title": "Resources You\u2019ll be Using"
        }, 
        {
            "location": "/modules/denovo-module-bac/bac_asm/#tools-used", 
            "text": "Velvet:  https://www.ebi.ac.uk/~zerbino/velvet/  Bandage:  https://rrwick.github.io/Bandage/", 
            "title": "Tools Used"
        }, 
        {
            "location": "/modules/denovo-module-bac/bac_asm/#data-source-and-useful-links", 
            "text": "Data used in this tutorial is from ENA accession SRR2054105  http://www.ebi.ac.uk/ena/data/view/SRR2054105  Spreadsheet used in the group activity:  https://docs.google.com/spreadsheets/d/1iFbCCihawpY1LClsAB-OJ66lyeW7EsdJyaMo1HCetF8/edit?usp=sharing", 
            "title": "Data Source and Useful Links"
        }, 
        {
            "location": "/modules/denovo-module-bac/bac_asm/#introduction", 
            "text": "In this tutorial we will take raw sequencing reads and  de novo \nassemble them into contigs. We will also explore the internal assembly\ngraph structure to aid our understanding of why assemblies are\nincomplete.", 
            "title": "Introduction"
        }, 
        {
            "location": "/modules/denovo-module-bac/bac_asm/#before-the-assembly", 
            "text": "", 
            "title": "Before the assembly"
        }, 
        {
            "location": "/modules/denovo-module-bac/bac_asm/#sequence-data", 
            "text": "We have sequenced the genome of a bacterium using paired-end chemistry\non an Illumina HiSeq 2000 instrument at the University of Oxford and\npublicly available as SRR2054105 the Europen Nucleotide Archive (ENA).  Let\u2019s have a look at the datasets by doing:  1\n2  cd /home/trainee/bacterial\n ls   You should see the following two files:  R1.fastq.gz  R2.fastq.gz  How many reads are in this data set?\\  2100778 pairs  What is the yield in basepairs?\\  210077800 bp = 210 Mbp  Assuming an average bacterial genome size of 4 Mbp, what depth of\ncoverage do we have?\\  210 / 4 = 53x", 
            "title": "Sequence data"
        }, 
        {
            "location": "/modules/denovo-module-bac/bac_asm/#trim-or-clip-reads", 
            "text": "There are two distinct reasons one may wish to trim or clip the raw\nreads.    Low quality bases typically occur toward the end of Illumina reads.\n    The lower the quality score, the higher the chance that the base is\n    incorrect. This introduces false k-mers into the assembly process. A\n    good assembler should handle these gracefully.    Sequencing adaptors are artificial sequence that can occur at the\n    end of reads that came from fragments of DNA that were shorter than\n    desired. They were not in the original genome and their presence in\n    lots of reads will totally confuse the assembler.    Normally one would use  fastqc  or similar to determine whether quality\ntrimming is warranted and for the presence of sequencing adaptors. The\ndata set in this exercise is 99% free of adaptors and is good quality\noverall so we will skip those steps.  How do you know which adaptor sequence to trim?\\  You either ask the sequencing provider or use a tool that tries all\nknown adapters.  How are are quality values in  FASTQ  files calculated? Can we trust\nthem?\\  The are calculated using formulas calibrated to the physical\nmeasurements the sequencing instrument takes. In the case of Illumina\nsequencing, these measurements will include pixel light intensities from\nthe images the camera/scanner takes of the flow cell.", 
            "title": "Trim or clip reads"
        }, 
        {
            "location": "/modules/denovo-module-bac/bac_asm/#allocate-yourself-to-a-k-value-on-the-spreadsheet", 
            "text": "An important parameter to most genome assemblers is  K  which is the\nk-mer size used to construct the  de Bruijn  graph (pronounced  de\nBrown ) .  Please go to this shared online\nspreadsheet  https://docs.google.com/spreadsheets/d/1iFbCCihawpY1LClsAB-OJ66lyeW7EsdJyaMo1HCetF8/edit?usp=sharing \nand choose  a K value  and put your name next to it.", 
            "title": "Allocate yourself to a K value on the spreadsheet"
        }, 
        {
            "location": "/modules/denovo-module-bac/bac_asm/#assemble-the-reads-using-velvet", 
            "text": "The  Velvet  software performs the assembly in two steps. The first velveth  step hashes the reads. The second  velvetg  step builds,\ncleans and traverses the graph.  Choose an output folder  DIR  and use the  K  value you were allocated\non the spreadsheet and assemble the reads.  1 velveth DIR K -shortPaired -fastq.gz -separate R1.fastq.gz R2.fastq.gz   Description of the arguments used in the command:  DIR\n:   = Directory name you chose to write the output to  K\n:   = K-mer value; it must be an odd number  -shortPaired\n:   = The read types are short and paired-end  -fastq.gz\n:   = The format of the input files  -separate\n:   = R1 and R2 reads are in separate files  input file names\n:   = input FASTQ file names  Run the velvetg step now:  1 /usr/bin/time -f  %e  velvetg DIR -exp_cov auto -cov_cutoff auto   Description of the arguments used in the command:  time\n:   = to capture the  velvetg  command run time  DIR\n:   = Directory name you chose to write the velveth output to  -exp_cov\n:   = expected coverage is set to  auto . To be determined by  velvet  -cov_cutoff\n:   = coverage cutoff is set to  auto . To be determined by  velvet", 
            "title": "Assemble the reads using Velvet"
        }, 
        {
            "location": "/modules/denovo-module-bac/bac_asm/#add-your-results-to-the-spreadsheet", 
            "text": "[H]  ll  Column     Where to find the value \\ K-mer size    You chose this earlier and used it in the velveth\ncommand\\ How long it took to run    Look at the final output line for the user\ntime in seconds: NN.N\\ Average K-mer coverage    Look for Estimated Coverage = NN.N\\ Number of contigs    Look for text Final graph has NNN nodes\\ N50 contig size    Look for n50 of NNNNN\\ Largest contig size    Look for max NNNNN\\ Total number of basepairs (sum of contig lengths)    Look for total\nNNNNNNN\\  [tab:results]", 
            "title": "Add your results to the spreadsheet"
        }, 
        {
            "location": "/modules/denovo-module-bac/bac_asm/#effect-of-k-on-assembly", 
            "text": "Examine the spreadsheet and look for patterns in the tabular data and\ncorresponding bar/line charts.  How does K affect each of the statistics? Which value of K do you think\nis doing the best job? Why?", 
            "title": "Effect of K on assembly"
        }, 
        {
            "location": "/modules/denovo-module-bac/bac_asm/#output-files", 
            "text": "[H]  ll  Filename     Description \\ contigs.fa    this is a FASTA file with your contigs\\ Log    has most of the metrics in it that we recorded\\ stats.txt    TSV file of length and coverage of individual contigs\\ Sequences    A copy of the input FASTQ sequences in FASTA format\\ Pregraph Roadmaps Graph2    Interim assembly graph structure\\ LastGraph    Final graph structure\\  [tab:velvet o ut]  Let\u2019s examine the  stats.txt  file and look at the  short1_cov  column\nwhich is the k-mer coverage of each contig:  1 cut -f6 dir/stats.txt | less   Press  SPACE  and  b  to scroll up and down, and press  q  to quote.  What do you notice about the k-mer coverages?\\  They all appear to have a similar value, but there are some that do not\nfit the pattern.  What do the outliers correspond to?\\  Repetitive elements of the genome including gene duplications. Also\nreplicons with differing copy numbers, like bacterial plasmids.  1 grep NN dir/contigs.fa   Why is there N letters in the assembly?\\  Paired-end reads come from each end of the same original fragment of\nDNA. The unsequenced gap in the middle is variable, and unknown. The\nassembler sometimes struggles to resolve these gaps. So it knows two\nsections of the genome are connected, but it doesn\u2019t quite know what is\nbetween them. So it pads them with its best guess of how many bases\nthere are, and uses N to denote them as unknown.", 
            "title": "Output Files"
        }, 
        {
            "location": "/modules/denovo-module-bac/bac_asm/#visualising-the-assembly-graph-using-bandage", 
            "text": "The final graph that  Velvet  uses is stored in the file  LastGraph .\nThe  Bandage  software allows us to view and explore the assembly graph.    Start  Bandage .    Go to  File -  Load Graph  and load the  LastGraph  from your\n    assembly in  DIR . This may take a little while so be patient.    Maximize your window to fill up the whole screen.    Click  Draw graph  on the left hand panel.    Change  Random colours  to  Colour by read depth  on the left hand\n    panel.    Now get up out of your chair and walks around and look at all the\ndifferent graphs the other participants obtained with different values\nof  K .  How does  K  affect the graph?\\  Small K produces higher k-mer coverage but also more connectivity as\nsmaller K is more ambiguous.  What would the graph look like in an ideal situtation?\\  If the genome had M replicons, we would like to see only M sub-graphs.\nEach sub-graph would be linear or circular, depending on the form of the\nreplicon in the original organism.  Why didn\u2019t anyone achieve perfection?\\  Short reads are unable to disambiguate repeats longer than the read\nlength (or read span). Most genomes have repeats beyond 500 bp.  Here is another example https://github.com/rrwick/Bandage/wiki/Effect-of-kmer-size  from the\nBandage web site on how K can affect assembly graphs.", 
            "title": "Visualising the assembly graph using Bandage"
        }, 
        {
            "location": "/modules/denovo-module-bac/bac_asm/#explore-the-graph", 
            "text": "Bandage  is designed to be an interactive tool. It allows you to see\nrelationships between parts of your genome that are lost when you only\nlook at the FASTA file of contigs.    Zoom in using the  Zoom  up/down button in the left hand panel    Pan around by holding down the Option/Windows key and dragging on\n    the background    Move nodes out of the way by selecting and dragging    Feel free to play around a bit and ask questions.", 
            "title": "Explore the graph"
        }, 
        {
            "location": "/modules/denovo-module-bac/bac_asm/#features-of-the-assembly-graph", 
            "text": "The graph is quite tangled. The long stretches correspond to contigs.\nThe intersections correspond to shared k-mers in the graph, which occur\ndue to repeated sequences within the genome.  Select an node (rectangle) in the graph. It\u2019s length is reported in the\nright hand panel as  Length :   NNN .  On the menu choose  Output -  Web BLAST selected node . Your browser\nshould open the NCBI BLAST Website http://blast.ncbi.nlm.nih.gov/Blast.cgi?PROGRAM=blastn PAGE_TYPE=BlastSearch LINK_LOC=blasthome .  Click the  BLAST  button at the bottom, and wait for the result.  Did your node match anything in the Genbank database?\\  You will need to examine the BLAST report on the trainee browser window.  Can you determine what species of bacteria was sequenced?\\  The top hit of the small segment of DNA the trainee chose may not\nnecessarily reflect the true species of bacteria. But if multiple\nsegments produce consistent top hits to the same species you would have\nsome confidence.", 
            "title": "Features of the assembly graph"
        }, 
        {
            "location": "/modules/denovo-module-bac/bac_asm/#conclusion", 
            "text": "You should now:    know how to use Velvet to assemble a simpel genome from Illumina\n    sequences    understand the role of the k-mer length K in the assembly process    be able to relate the graph structure to the final contigs    realize the limitations of short read sequences with respect to\n    genome complexity    Thank you.", 
            "title": "Conclusion"
        }, 
        {
            "location": "/modules/denovo-module-euk/eukaryotic_assem/", 
            "text": "Key Learning Outcomes\n\n\nAfter completing this module the trainee should be able to:\n\n\n\n\n\n\nAssess general assembly approach, kmer spectra and biases.\n\n\n\n\n\n\nVisually inspect the kmer spectra and KAT plots\n\n\n\n\n\n\nRun a first pass eukaryotic assembly and do goal checks\n\n\n\n\n\n\nDevelop validation metrics or tools for NGS data and assembly.\n\n\n\n\n\n\nImproving methods and pipelines for genome assembly.\n\n\n\n\n\n\nConvince the lab guys to tweak protocols.\n\n\n\n\n\n\nResources You\u2019ll be Using\n\n\nTools Used\n\n\nKmer Analysis Tool kit:\n\n\nhttps://github.com/TGAC/KAT\n\n\nNextclip:\n\n\nhttps://github.com/richardmleggett/nextclip\n\n\nAbyss:\n\n\nhttp://www.bcgsc.ca/platform/bioinfo/software/abyss\n\n\nSoap Denovo:\n\n\nhttp://soap.genomics.org.cn/soapdenovo.html\n\n\nSOAPec:\n\n\nhttp://soap.genomics.org.cn/about.html\n\n\nBLAST:\n\n\nhttp://blast.ncbi.nlm.nih.gov/Blast.cgi\n\n\nFirst Pass Genome Assembly\n\n\nAssuming by now you are familiar with the general concept of \nde novo\n\nassembly, kmers and the de Bruijn graph based assembler. In this\ntutorial we will use ABySS to perform the first pass assembly of a\neukaryotic genome and look at various parameters to assess the\ninformation content of the input data and choice of assembly parameters.\nsequence data.\\\nGenome assembly is a challenging problem requiring heavy computational\nresources, expertise and time. Before you beging the process of denovo\nassembly there are a number of points you need to consider:\n\n\nWhat is the objective of your assembly experiment? What biological\nquestion(s) you have?\n\n\nIs assembly strictly neccessary for the purpose in question?\n\n\nDo you have right kind of data and enough coverage to start with?\n\n\nDo you have suitable computaitonal resources to run this assembly?\n\n\nRemember that the assembly is just a probabilistic model of a genome,\ncondensing the information from the experimental evidence. All the\ninformation is already present in the experimental results. The goal of\nthe assembly is to find the right motifs, the correct number of times,\nin correct order and position.\n\n\nFusarium first pass with a goal\n\n\nGoal: Identify a fusarium sample is ``closer\n to \nF. graminearum\n or\n\nF.pseudograminearum\n\n\n\n\n\n\nPrevious knowledge\n\n\n\n\nF. graminearum\n has a cluster producing PKS6 and NRSP7, while\n    \nF. pseudograminearum\n produces PKS40 and NRPS32\n\n\n\n\n\n\n\n\nData\n\n\n\n\n\n\nProteins sequences for:\n\n\n\n\n\n\nF. graminearum\n (non necrotrophic): PKS6 and NRSP7\n\n\n\n\n\n\nF. pseudograminearum\n (necrotrophic): PKS40 and NRPS32\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStrategy:\n\n\n\n\nCheck PKS6-NRSP7 and PKS40-NRPS32 cluster presence.\n\n\n\n\n\n\n\n\nAssembly goals:\n\n\n\n\n\n\nAssembly goal (I): to capture a good enough representation of\n    the protein-coding space to get blast matches\n\n\n\n\n\n\nAssembly goal (II): to accurately represent the relevant whole\n    cluster loci in a single sequence.\n\n\n\n\n\n\n\n\n\n\nPrepare the Environment\n\n\nOpen the Terminal. First, go to the right folder, where the data are\nstored.\n\n\n1\n2\ncd /home/trainee/eukaryotic\nls\n\n\n\n\n\n\nTask1.1: First pass assembly, k=71\n\n\nLet\u2019s assemble \nFusarium\n with abyss, k=71\n\n\n1\n2\n3\n4\ncd /home/trainee/eukaryotic\nmkdir abyss_k71\ncd abyss_k71\nabyss-pe in=\n../CS3270_A8733_GCCAAT_L001_R1.fastq ../CS3270_A8733_GCCAAT_L001_R2.fastq\n k=71 name=CS3270_abyss_k71 np=4 \n CS3270_abyss_k71.log 2\n1\n\n\n\n\n\n\nDescription of the arguments used in the command:\n\n\nk\n:   = kmer size\n\n\nnp\n:   = number of processors to be used\n\n\nsequence file names\n:   = R1 and R2 reads of a paired end sequence data\n\n\nLet\u2019s look at the statistics of the assembly we just did\n\n\nOk, there is no stats available in the folder, but we can always use\n\nabyss-fac\n to get the stats:\n\n\n1\n2\nabyss-fac CS3270_abyss_k71-*tigs.fa  | tee CS3270_abyss_k71-stats.tab\nless CS3270_abyss_k71-stats.tab\n\n\n\n\n\n\n[H]\n\n\nlllllllllll\n\n\nn\n \n \nn:500\n \n \nL50\n \n \nmin\n \n \nN80\n \nN50\n \nN20\n\n\nE-size\n \nmax\n \n \nsum\n \nname\n\\\n27 \n 13 \n 2 \n 970 \n 6004 \n 13202 \n 52602 \n 28712 \n 52602 \n 112849 \n\nCS3270_abyss_k71-unitigs.fa%\\\n5 \n 1 \n 1 \n 128429 \n 128429 \n 128429 \n 128429 \n 128429 \n 128429 \n 128429\n\n CS3270_abyss_k71-contigs.fa%\\\n\n\n[tab:fusariumk71]\n\n\nHow many unitigs/contigs do you have in the assembly?\\\n\n\n27/5\\\n\n\nWhat are the length statistics of your assembly?\\\n\n\nin the table above\\\n\n\nDoes it match what you think before the assembly and why?\\\n\n\nNo\\\n\n\nThe assembly is looking strange! It\u2019s time for some analysis:\n\n\nCheck frequencies for kmers kept/discarded/etc.\n\n\nCheck spectra-cn and compare with expectations. Let\u2019s do this by the\nfollowing commands:\n\n\n1\n2\nless CS3270_abyss_k71.log\nless coverage.hist\n\n\n\n\n\n\nWe will now plot the values from the \ncoverage.hist\n:\n\n\n1\n2\n3\n4\n5\n6\ngnuplot \nPress enter\n\ngnuplot\n set xrange [0:200]\ngnuplot\n set yrange [0:5000]\ngnuplot\n plot \ncoverage.hist\n\nType exit or quit to leave the gnuplot\ngnuplot\n exit\n\n\n\n\n\n\nLooks like we are not assembling this bit, let\u2019s have another look at\nthe spectra\n\n\n1\nkat comp -o reads_vs_abyss_k71 -t 4 -C -D --d1_bins 2000 \n../*.fastq\n CS3270_abyss_k71-contigs.fa\n\n\n\n\n\n\nDescription of the arguments used in the command:\n\n\no\n:   = Path prefix for files generated by this program.\n\n\nt\n:   = The number of threads to use.\n\n\nC\n:   = Whether the jellyfish hash for input 1 contains K-mers produced\n    for both strands\n\n\nD\n:   = Whether the jellyfish hash for input 2 contains K-mers produced\n    for both strands\n\n\n\u2013d1_bins\n:   = Number of bins for the first dataset. i.e. number of rows in the\n    matrix\n\n\n\n\n\n1\nkat plot spectra-cn -y 600 -x 2000 -o reads_vs_abyss1-main.mx.spectra-cn_2000.png reads_vs_abyss_k71-main.mx\n\n\n\n\n\n\nDescription of the arguments used in the command:\n\n\nx\n:   = Maximum value for the x-axis (default value auto calculated from\n    matrix, otherwise 1000)\n\n\ny\n:   = Maximum value for the y-axis (default value auto calculated from\n    matrix if possible, otherwise, 1000000)\n\n\no\n:   = The path to the output file\n\n\nThe kmer spectra for \nFusarium\n assembly with abyss, k=71 should be\nlooking like this:\n\n\n[H] \n [fig:fusariumk71]\n\n\nTake the output and BLAST it in NCBI. What is it? Surprising?\n\n\nChoosing a wrong k value (too large in this case) and just running a\ntypical assembly job, we can end up with something quite more\ninteresting. It is easy by comparison to spot some missing content,\nalongside duplications and triplications (and quadruplications and so\non) that should not be there. This assembly will get us nowhere, let\u2019s\nchoose a lower K to gain coverage and start again.\n\n\nTask1.2: First pass assembly, k=27\n\n\nWe now assemble \nfusarium\n with \nabyss\n and k=27:\n\n\n1\n2\n3\n4\ncd /home/trainee/eukaryotic\nmkdir abyss_k27\ncd abyss_k27\nabyss-pe in=\n../CS3270_A8733_GCCAAT_L001_R1.fastq ../CS3270_A8733_GCCAAT_L001_R2.fastq\n k=27 name=CS3270_abyss_k27 np=4 \n CS3270_abyss_k27.log 2\n1\n\n\n\n\n\n\nLet\u2019s look at the stats by doing:\n\n\n1\nless CS3270_abyss_k27-stats.tab\n\n\n\n\n\n\nStats look better:\n\n\n[H]\n\n\nlllllllllll\n\n\nn\n \n \nn:500\n \n \nL50\n \n \nmin\n \n \nN80\n \nN50\n \nN20\n\n\nE-size\n \nmax\n \n \nsum\n \nname\n\\\n30645 \n 2717 \n 430 \n 502 \n 11354 \n 25336 \n 47966 \n 31027 \n 147694 \n\n36.14e6 \n CS3270_abyss_k27-unitigs.fa\\\n21511 \n 350 \n 33 \n 527 \n 157565 \n 338989 \n 630228 \n 407098 \n 1265237 \n\n36.52e6 \n CS3270_abyss_k27-contigs.fa\\\n21327 \n 205 \n 17 \n 527 \n 332444 \n 716132 \n 1265237 \n 791882 \n 1880850 \n\n36.51e6 \n CS3270_abyss_k27-scaffolds.fa\\\n\n\n[tab:fusariumk27]\n\n\nLet\u2019s check a bit anyway:\n\n\n1\n2\nless CS3270_abyss_k27.log\nless coverage.hist\n\n\n\n\n\n\nHow is the coverage plot looking now?\n\n\n1\n2\n3\n4\n5\ngnuplot\ngnuplot\n set xrange [0:50]\ngnuplot\n set xrange [0:4000000]\ngnuplot\n plot \ncoverage.hist\n\ngnuplot\n exit\n\n\n\n\n\n\nK-mer spectrum:\n\n\n1\n2\nkat plot spectra-cn -y 1000 -x 1000 -o reads_vs_abyss1-main.mx.spectra-cn_noabsent.png reads_vs_abyss1-main.mx\nkat comp -o reads_vs_abyss_k27 -t 4 -C -D \n../*.fastq\n CS3270_abyss_k27-scaffolds.fa\n\n\n\n\n\n\n[H] \n [fig:fusariumk27]\n\n\nAny tools you can use to check kmer spectra at any K before assembling?\\\n\n\nKAT\\\n\n\nCan you predict what will happen if you use KAT with larger K values?\\\n\n\nWill the assembly answer the biological question?\n\n\nUse BLAST or BLAT and the databases to check\n\n\n1\n2\nmakeblastdb -in CS3270_abyss_k27-scaffolds.fa -dbtype nucl\nblat \u2013t=dnax \u2013q=prot \u2013minIdentity=90 CS3270_abyss_k27-scaffolds.fa test_genes.fasta out.psl\n\n\n\n\n\n\nReferences\n\n\n\n\nDe novo genome assembly: what every biologist should know Nature\n    Methods 9, 333 \u2013 337 (2012) doi:10.1038/nmeth.1935", 
            "title": "Eukaryote Assembly"
        }, 
        {
            "location": "/modules/denovo-module-euk/eukaryotic_assem/#key-learning-outcomes", 
            "text": "After completing this module the trainee should be able to:    Assess general assembly approach, kmer spectra and biases.    Visually inspect the kmer spectra and KAT plots    Run a first pass eukaryotic assembly and do goal checks    Develop validation metrics or tools for NGS data and assembly.    Improving methods and pipelines for genome assembly.    Convince the lab guys to tweak protocols.", 
            "title": "Key Learning Outcomes"
        }, 
        {
            "location": "/modules/denovo-module-euk/eukaryotic_assem/#resources-youll-be-using", 
            "text": "", 
            "title": "Resources You\u2019ll be Using"
        }, 
        {
            "location": "/modules/denovo-module-euk/eukaryotic_assem/#tools-used", 
            "text": "Kmer Analysis Tool kit:  https://github.com/TGAC/KAT  Nextclip:  https://github.com/richardmleggett/nextclip  Abyss:  http://www.bcgsc.ca/platform/bioinfo/software/abyss  Soap Denovo:  http://soap.genomics.org.cn/soapdenovo.html  SOAPec:  http://soap.genomics.org.cn/about.html  BLAST:  http://blast.ncbi.nlm.nih.gov/Blast.cgi", 
            "title": "Tools Used"
        }, 
        {
            "location": "/modules/denovo-module-euk/eukaryotic_assem/#first-pass-genome-assembly", 
            "text": "Assuming by now you are familiar with the general concept of  de novo \nassembly, kmers and the de Bruijn graph based assembler. In this\ntutorial we will use ABySS to perform the first pass assembly of a\neukaryotic genome and look at various parameters to assess the\ninformation content of the input data and choice of assembly parameters.\nsequence data.\\\nGenome assembly is a challenging problem requiring heavy computational\nresources, expertise and time. Before you beging the process of denovo\nassembly there are a number of points you need to consider:  What is the objective of your assembly experiment? What biological\nquestion(s) you have?  Is assembly strictly neccessary for the purpose in question?  Do you have right kind of data and enough coverage to start with?  Do you have suitable computaitonal resources to run this assembly?  Remember that the assembly is just a probabilistic model of a genome,\ncondensing the information from the experimental evidence. All the\ninformation is already present in the experimental results. The goal of\nthe assembly is to find the right motifs, the correct number of times,\nin correct order and position.", 
            "title": "First Pass Genome Assembly"
        }, 
        {
            "location": "/modules/denovo-module-euk/eukaryotic_assem/#fusarium-first-pass-with-a-goal", 
            "text": "Goal: Identify a fusarium sample is ``closer  to  F. graminearum  or F.pseudograminearum    Previous knowledge   F. graminearum  has a cluster producing PKS6 and NRSP7, while\n     F. pseudograminearum  produces PKS40 and NRPS32     Data    Proteins sequences for:    F. graminearum  (non necrotrophic): PKS6 and NRSP7    F. pseudograminearum  (necrotrophic): PKS40 and NRPS32        Strategy:   Check PKS6-NRSP7 and PKS40-NRPS32 cluster presence.     Assembly goals:    Assembly goal (I): to capture a good enough representation of\n    the protein-coding space to get blast matches    Assembly goal (II): to accurately represent the relevant whole\n    cluster loci in a single sequence.", 
            "title": "Fusarium first pass with a goal"
        }, 
        {
            "location": "/modules/denovo-module-euk/eukaryotic_assem/#prepare-the-environment", 
            "text": "Open the Terminal. First, go to the right folder, where the data are\nstored.  1\n2 cd /home/trainee/eukaryotic\nls", 
            "title": "Prepare the Environment"
        }, 
        {
            "location": "/modules/denovo-module-euk/eukaryotic_assem/#task11-first-pass-assembly-k71", 
            "text": "Let\u2019s assemble  Fusarium  with abyss, k=71  1\n2\n3\n4 cd /home/trainee/eukaryotic\nmkdir abyss_k71\ncd abyss_k71\nabyss-pe in= ../CS3270_A8733_GCCAAT_L001_R1.fastq ../CS3270_A8733_GCCAAT_L001_R2.fastq  k=71 name=CS3270_abyss_k71 np=4   CS3270_abyss_k71.log 2 1   Description of the arguments used in the command:  k\n:   = kmer size  np\n:   = number of processors to be used  sequence file names\n:   = R1 and R2 reads of a paired end sequence data  Let\u2019s look at the statistics of the assembly we just did  Ok, there is no stats available in the folder, but we can always use abyss-fac  to get the stats:  1\n2 abyss-fac CS3270_abyss_k71-*tigs.fa  | tee CS3270_abyss_k71-stats.tab\nless CS3270_abyss_k71-stats.tab   [H]  lllllllllll  n     n:500     L50     min     N80   N50   N20  E-size   max     sum   name \\\n27   13   2   970   6004   13202   52602   28712   52602   112849  \nCS3270_abyss_k71-unitigs.fa%\\\n5   1   1   128429   128429   128429   128429   128429   128429   128429  CS3270_abyss_k71-contigs.fa%\\  [tab:fusariumk71]  How many unitigs/contigs do you have in the assembly?\\  27/5\\  What are the length statistics of your assembly?\\  in the table above\\  Does it match what you think before the assembly and why?\\  No\\  The assembly is looking strange! It\u2019s time for some analysis:  Check frequencies for kmers kept/discarded/etc.  Check spectra-cn and compare with expectations. Let\u2019s do this by the\nfollowing commands:  1\n2 less CS3270_abyss_k71.log\nless coverage.hist   We will now plot the values from the  coverage.hist :  1\n2\n3\n4\n5\n6 gnuplot  Press enter \ngnuplot  set xrange [0:200]\ngnuplot  set yrange [0:5000]\ngnuplot  plot  coverage.hist \nType exit or quit to leave the gnuplot\ngnuplot  exit   Looks like we are not assembling this bit, let\u2019s have another look at\nthe spectra  1 kat comp -o reads_vs_abyss_k71 -t 4 -C -D --d1_bins 2000  ../*.fastq  CS3270_abyss_k71-contigs.fa   Description of the arguments used in the command:  o\n:   = Path prefix for files generated by this program.  t\n:   = The number of threads to use.  C\n:   = Whether the jellyfish hash for input 1 contains K-mers produced\n    for both strands  D\n:   = Whether the jellyfish hash for input 2 contains K-mers produced\n    for both strands  \u2013d1_bins\n:   = Number of bins for the first dataset. i.e. number of rows in the\n    matrix   1 kat plot spectra-cn -y 600 -x 2000 -o reads_vs_abyss1-main.mx.spectra-cn_2000.png reads_vs_abyss_k71-main.mx   Description of the arguments used in the command:  x\n:   = Maximum value for the x-axis (default value auto calculated from\n    matrix, otherwise 1000)  y\n:   = Maximum value for the y-axis (default value auto calculated from\n    matrix if possible, otherwise, 1000000)  o\n:   = The path to the output file  The kmer spectra for  Fusarium  assembly with abyss, k=71 should be\nlooking like this:  [H]   [fig:fusariumk71]  Take the output and BLAST it in NCBI. What is it? Surprising?  Choosing a wrong k value (too large in this case) and just running a\ntypical assembly job, we can end up with something quite more\ninteresting. It is easy by comparison to spot some missing content,\nalongside duplications and triplications (and quadruplications and so\non) that should not be there. This assembly will get us nowhere, let\u2019s\nchoose a lower K to gain coverage and start again.", 
            "title": "Task1.1: First pass assembly, k=71"
        }, 
        {
            "location": "/modules/denovo-module-euk/eukaryotic_assem/#task12-first-pass-assembly-k27", 
            "text": "We now assemble  fusarium  with  abyss  and k=27:  1\n2\n3\n4 cd /home/trainee/eukaryotic\nmkdir abyss_k27\ncd abyss_k27\nabyss-pe in= ../CS3270_A8733_GCCAAT_L001_R1.fastq ../CS3270_A8733_GCCAAT_L001_R2.fastq  k=27 name=CS3270_abyss_k27 np=4   CS3270_abyss_k27.log 2 1   Let\u2019s look at the stats by doing:  1 less CS3270_abyss_k27-stats.tab   Stats look better:  [H]  lllllllllll  n     n:500     L50     min     N80   N50   N20  E-size   max     sum   name \\\n30645   2717   430   502   11354   25336   47966   31027   147694  \n36.14e6   CS3270_abyss_k27-unitigs.fa\\\n21511   350   33   527   157565   338989   630228   407098   1265237  \n36.52e6   CS3270_abyss_k27-contigs.fa\\\n21327   205   17   527   332444   716132   1265237   791882   1880850  \n36.51e6   CS3270_abyss_k27-scaffolds.fa\\  [tab:fusariumk27]  Let\u2019s check a bit anyway:  1\n2 less CS3270_abyss_k27.log\nless coverage.hist   How is the coverage plot looking now?  1\n2\n3\n4\n5 gnuplot\ngnuplot  set xrange [0:50]\ngnuplot  set xrange [0:4000000]\ngnuplot  plot  coverage.hist \ngnuplot  exit   K-mer spectrum:  1\n2 kat plot spectra-cn -y 1000 -x 1000 -o reads_vs_abyss1-main.mx.spectra-cn_noabsent.png reads_vs_abyss1-main.mx\nkat comp -o reads_vs_abyss_k27 -t 4 -C -D  ../*.fastq  CS3270_abyss_k27-scaffolds.fa   [H]   [fig:fusariumk27]  Any tools you can use to check kmer spectra at any K before assembling?\\  KAT\\  Can you predict what will happen if you use KAT with larger K values?\\  Will the assembly answer the biological question?  Use BLAST or BLAT and the databases to check  1\n2 makeblastdb -in CS3270_abyss_k27-scaffolds.fa -dbtype nucl\nblat \u2013t=dnax \u2013q=prot \u2013minIdentity=90 CS3270_abyss_k27-scaffolds.fa test_genes.fasta out.psl", 
            "title": "Task1.2: First pass assembly, k=27"
        }, 
        {
            "location": "/modules/denovo-module-euk/eukaryotic_assem/#references", 
            "text": "De novo genome assembly: what every biologist should know Nature\n    Methods 9, 333 \u2013 337 (2012) doi:10.1038/nmeth.1935", 
            "title": "References"
        }, 
        {
            "location": "/modules/denovo-module-scaffolding/scaffold/", 
            "text": "Key Learning Outcomes\n\n\nAfter completing this module the trainee should be able to:\n\n\n\n\n\n\nRun an assembly using long mate pair (LMP) reads.\n\n\n\n\n\n\nCompare results of LMP assemblies with those from the short reads.\n\n\n\n\n\n\nRun scaffolding and assess the assembly statistics`\n\n\n\n\n\n\nResources You\u2019ll be Using\n\n\nTools Used\n\n\nKmer Analysis Tool kit:\n\n\nhttps://github.com/TGAC/KAT\n\n\nNextclip:\n\n\nhttps://github.com/richardmleggett/nextclip\n\n\nAbyss:\n\n\nhttp://www.bcgsc.ca/platform/bioinfo/software/abyss\n\n\nSoap Denovo:\n\n\nhttp://soap.genomics.org.cn/soapdenovo.html\n\n\nSOAPec:\n\n\nhttp://soap.genomics.org.cn/about.html\n\n\nScaffolding with long mate pair libraries: Chalara scaffolding using LMP\n\n\nScaffold is made of two or more contigs joined together using the read\npair information. In the absence of a high-quality reference genome, new\ngenome assemblies are often evaluated on the basis of the number of\nscaffolds and contigs required to represent the genome. Other criteria\nsuch as the alignment of reads back to assemblies, N50, and the length\nof contigs and scaffolds relative to the size of the genome can also be\nused to assess the quality of assemblies. In this excercise we will be\nusing a long mate pair data to perform assembly and scaffolding and we\nwill focus on how using LMP reads can affect the assemblies. Most part\nof this tutorial has been precomputed and made available to you in\ninterest of time. We will spend more time exploring and discussing the\nresults.\n\n\nPair-end assembly using Chalara dataset\n\n\nBefore doing a scaffolding, we need to build an assembly using the\npair-end reads first.\n\n\nLets go to the correct location where the files are:\n\n\n1\ncd /home/trainee/scaffolding/cha_raw/test_1\n\n\n\n\n\n\nYou DO NOT need to run this command. This has already been done for you.\n\n\n1\nabyss-pe name=cha1 k=27 in=\n../LIB2570_raw_R1.fastq ../LIB2570_raw_R2.fastq\n np=4\n\n\n\n\n\n\nLet\u2019s look at the stats by doing:\n\n\n1\nless cha1-stats.tab\n\n\n\n\n\n\n[H]\n\n\nlllllllllll\n\n\nn\n \n \nn:500\n \n \nL50\n \n \nmin\n \n \nN80\n \nN50\n \nN20\n\n\nE-size\n \nmax\n \n \nsum\n \nname\n\\\n394789 \n 11681 \n 2094 \n 500 \n 2880 \n 6254 \n 12303 \n 7936 \n 44414 \n\n44.07e6 \n cha1-unitigs.fa\\\n394199 \n 11673 \n 2097 \n 500 \n 2887 \n 6255 \n 12303 \n 7937 \n 44414 \n\n44.11e6 \n cha1-contigs.fa\\\n394161 \n 11647 \n 2095 \n 500 \n 2898 \n 6269 \n 12303 \n 7944 \n 44414 \n\n44.12e6 \n cha1-scaffolds.fa\\\n\n\n[tab:chak27]\n\n\n1\ncd /home/trainee/scaffolding/cha_raw/test_2\n\n\n\n\n\n\nThis is also a pre-computed example for you:\n\n\n1\nabyss-pe name=cha2 k=61 in=\n../LIB2570_raw_R1.fastq ../LIB2570_raw_R2.fastq\n np=4\n\n\n\n\n\n\nAgain, we need to look at the statstics of our new assembly using k=61:\n\n\n1\nless cha2-stats.tab\n\n\n\n\n\n\nIt should be looking like this:\n\n\n[H]\n\n\nlllllllllll\n\n\nn\n \n \nn:500\n \n \nL50\n \n \nmin\n \n \nN80\n \nN50\n \nN20\n\n\nE-size\n \nmax\n \n \nsum\n \nname\n\\\n130547 \n 12596 \n 1770 \n 500 \n 3352 \n 8379 \n 16380 \n 10518 \n 54300 \n\n50.75e6 \n cha2-unitigs.fa\\\n130210 \n 12575 \n 1771 \n 500 \n 3363 \n 8382 \n 16380 \n 10525 \n 54300 \n\n50.78e6 \n cha2-contigs.fa\\\n130182 \n 12555 \n 1769 \n 500 \n 3377 \n 8394 \n 16380 \n 10534 \n 54300 \n\n50.78e6 \n cha2-scaffolds.fa\\\n\n\n[tab:chak61]\n\n\nThe assembly contiguity is worse than fusarium, why?\n\n\nGenome characteristics.\n\n\nData:\n\n\nCoverage\n\n\nError distributions\n\n\nRead sizes\n\n\nFragment sizes\n\n\nKmer spectra\n\n\nChalara scaffolding using long mate-pair reads(LMP)\n\n\nLet\u2019s put some LMP in there.\n\n\n1\ncd /home/trainee/scaffolding/cha_raw/test_3\n\n\n\n\n\n\nThis is also a pre-computed example for you:\n\n\n1\n2\n3\nabyss-pe name=chalmp1 k=61 in=\n../LIB2570_raw_R1.fastq ../LIB2570_raw_R2.fastq\n mp=\nlmp1\n lmp1=\n../LIB8209_raw_R1.fastq ../LIB8209_raw_R2.fastq\n np=4\n\nless chalmp1-stats.tab\n\n\n\n\n\n\n[H]\n\n\nlllllllllll\n\n\nn\n \n \nn:500\n \n \nL50\n \n \nmin\n \n \nN80\n \nN50\n \nN20\n\n\nE-size\n \nmax\n \n \nsum\n \nname\n\\\n130548 \n 12596 \n 1770 \n 500 \n 3352 \n 8379 \n 16380 \n 10518 \n 54300 \n\n50.75e6 \n chalmp1-unitigs.fa\\\n130211 \n 12575 \n 1771 \n 500 \n 3363 \n 8382 \n 16380 \n 10525 \n 54300 \n\n50.78e6 \n chalmp1-contigs.fa\\\n130148 \n 12545 \n 1769 \n 500 \n 3380 \n 8400 \n 16380 \n 10535 \n 54300 \n\n50.78e6 \n chalmp1-scaffolds.fa\\\n\n\n[tab:chak27]\n\n\nSo\n what happened? Let\u2019s check the data by:\n\n\nKmer spectra\n\n\nFragment sizes\n\n\nAny hints on the protocol? And a not-so-obvious property:\n\n\n1\nkat plot spectra-mx --intersection -x 30 -y 15000000 -o pe_vs_lmp-main.mx.spectra-mx.png pe_vs_lmp-main.mx\n\n\n\n\n\n\nDo you remember LMP require pre-processing? Let\u2019s try with processed\nLMP.\n\n\nPrior task (already made) preprocess the LMP with nextclip.\n\n\n1\ncd /home/trainee/scaffolding/cha_proc/test_1\n\n\n\n\n\n\nThis is also a pre-computed example for you:\n\n\n1\n2\n3\nabyss-pe name=chalmpproc1 k=61 in=\n../../cha_raw/LIB2570_raw_R1.fastq ../../cha_raw/LIB2570_raw_R2.fastq\n mp=\nproclmp1\n proclmp1=\n../LIB8209_preproc_R1.fastq ../LIB8209_preproc_R2.fastq\n np=4\n\nless chalmpproc1-stats.tab\n\n\n\n\n\n\n[H]\n\n\nlllllllllll\n\n\nn\n \n \nn:500\n \n \nL50\n \n \nmin\n \n \nN80\n \nN50\n \nN20\n\n\nE-size\n \nmax\n \n \nsum\n \nname\n\\\n130548 \n 12596 \n 1770 \n 500 \n 3352 \n 8379 \n 16380 \n 10518 \n 54300 \n\n50.75e6 \n chalmpproc1-unitigs.fa\\\n130211 \n 12575 \n 1771 \n 500 \n 3363 \n 8382 \n 16380 \n 10525 \n 54300 \n\n50.78e6 \n chalmpproc1-contigs.fa\\\n122061 \n 6679 \n 167 \n 500 \n 18609 \n 87510 \n 187171 \n 106178 \n 397967 \n\n51.15e6 \n chalmpproc1-scaffolds.fa\\\n\n\n[tab:chaklmpk61]\n\n\nThat\u2019s much better!\n\n\nChalara: beyond first pass\n\n\nDo you remember these?\n\n\nGenome characteristics. Data:\n\n\n\n\n\n\nCoverage\n\n\n\n\n\n\nError distributions\n\n\n\n\n\n\nRead sizes\n\n\n\n\n\n\nFragment sizes\n\n\n\n\n\n\nKmer spectra\n\n\n\n\n\n\nLet\u2019s think a bit and try to improve the assembly\n\n\nIf you look at the pre-processed LMP, do you notice anything peculiar?\\\n\n\nNeeds to fill\n\n\nTesting the inclussion of heavily pre-procesed LMP coverage into the DBG\n\n\n1\ncd /home/trainee/scaffolding/cha_proc/test_2\n\n\n\n\n\n\nThis is also a pre-computed example for you:\n\n\n1\n2\n3\nabyss-pe name=chalmp2 k=61 se=\n../LIB8209_preproc_single.fastq\n lib=\nlmp1\n lmp1=\n../LIB8209_preproc_R1.fastq ../LIB8209_preproc_R2.fastq\n np=4 \nchaproclmp2.log 2\n1\n\nless chalmp2-stats.tab\n\n\n\n\n\n\nLet\u2019s look at the stats.\n\n\n[H]\n\n\nlllllllllll\n\n\nn\n \n \nn:500\n \n \nL50\n \n \nmin\n \n \nN80\n \nN50\n \nN20\n\n\nE-size\n \nmax\n \n \nsum\n \nname\n\\\n128306 \n 10150 \n 1182 \n 500 \n 4954 \n 12585 \n 24777 \n 15693 \n 68684 \n\n51.03e6 \n chaproclmp4-unitigs.fa\\\n118939 \n 5772 \n 362 \n 500 \n 15717 \n 41870 \n 82033 \n 52819 \n 252066 \n\n52.24e6 \n chaproclmp4-contigs.fa\\\n116139 \n 4014 \n 141 \n 500 \n 41145 \n 109273 \n 224986 \n 133450 \n 460979 \n\n52.56e6 \n chaproclmp4-scaffolds.fa\\\n\n\n[tab:chaklmp2-k61]\n\n\nReferences\n\n\n\n\n\n\nRobert Ekblom* andJochen B. W. Wolf. \nA field guide to whole-genome\n    sequencing, assembly and annotation\n Evolutionary Applications\n    Special Issue: Evolutionary Conservation Volume 7, Issue 9, pages\n    1026 \u2013 1042, November 2014\n\n\n\n\n\n\nDe novo genome assembly: what every biologist should know Nature\n    Methods 9, 333 \u2013 337 (2012) doi:10.1038/nmeth.1935", 
            "title": "Scaffolding"
        }, 
        {
            "location": "/modules/denovo-module-scaffolding/scaffold/#key-learning-outcomes", 
            "text": "After completing this module the trainee should be able to:    Run an assembly using long mate pair (LMP) reads.    Compare results of LMP assemblies with those from the short reads.    Run scaffolding and assess the assembly statistics`", 
            "title": "Key Learning Outcomes"
        }, 
        {
            "location": "/modules/denovo-module-scaffolding/scaffold/#resources-youll-be-using", 
            "text": "", 
            "title": "Resources You\u2019ll be Using"
        }, 
        {
            "location": "/modules/denovo-module-scaffolding/scaffold/#tools-used", 
            "text": "Kmer Analysis Tool kit:  https://github.com/TGAC/KAT  Nextclip:  https://github.com/richardmleggett/nextclip  Abyss:  http://www.bcgsc.ca/platform/bioinfo/software/abyss  Soap Denovo:  http://soap.genomics.org.cn/soapdenovo.html  SOAPec:  http://soap.genomics.org.cn/about.html", 
            "title": "Tools Used"
        }, 
        {
            "location": "/modules/denovo-module-scaffolding/scaffold/#scaffolding-with-long-mate-pair-libraries-chalara-scaffolding-using-lmp", 
            "text": "Scaffold is made of two or more contigs joined together using the read\npair information. In the absence of a high-quality reference genome, new\ngenome assemblies are often evaluated on the basis of the number of\nscaffolds and contigs required to represent the genome. Other criteria\nsuch as the alignment of reads back to assemblies, N50, and the length\nof contigs and scaffolds relative to the size of the genome can also be\nused to assess the quality of assemblies. In this excercise we will be\nusing a long mate pair data to perform assembly and scaffolding and we\nwill focus on how using LMP reads can affect the assemblies. Most part\nof this tutorial has been precomputed and made available to you in\ninterest of time. We will spend more time exploring and discussing the\nresults.", 
            "title": "Scaffolding with long mate pair libraries: Chalara scaffolding using LMP"
        }, 
        {
            "location": "/modules/denovo-module-scaffolding/scaffold/#pair-end-assembly-using-chalara-dataset", 
            "text": "Before doing a scaffolding, we need to build an assembly using the\npair-end reads first.  Lets go to the correct location where the files are:  1 cd /home/trainee/scaffolding/cha_raw/test_1   You DO NOT need to run this command. This has already been done for you.  1 abyss-pe name=cha1 k=27 in= ../LIB2570_raw_R1.fastq ../LIB2570_raw_R2.fastq  np=4   Let\u2019s look at the stats by doing:  1 less cha1-stats.tab   [H]  lllllllllll  n     n:500     L50     min     N80   N50   N20  E-size   max     sum   name \\\n394789   11681   2094   500   2880   6254   12303   7936   44414  \n44.07e6   cha1-unitigs.fa\\\n394199   11673   2097   500   2887   6255   12303   7937   44414  \n44.11e6   cha1-contigs.fa\\\n394161   11647   2095   500   2898   6269   12303   7944   44414  \n44.12e6   cha1-scaffolds.fa\\  [tab:chak27]  1 cd /home/trainee/scaffolding/cha_raw/test_2   This is also a pre-computed example for you:  1 abyss-pe name=cha2 k=61 in= ../LIB2570_raw_R1.fastq ../LIB2570_raw_R2.fastq  np=4   Again, we need to look at the statstics of our new assembly using k=61:  1 less cha2-stats.tab   It should be looking like this:  [H]  lllllllllll  n     n:500     L50     min     N80   N50   N20  E-size   max     sum   name \\\n130547   12596   1770   500   3352   8379   16380   10518   54300  \n50.75e6   cha2-unitigs.fa\\\n130210   12575   1771   500   3363   8382   16380   10525   54300  \n50.78e6   cha2-contigs.fa\\\n130182   12555   1769   500   3377   8394   16380   10534   54300  \n50.78e6   cha2-scaffolds.fa\\  [tab:chak61]  The assembly contiguity is worse than fusarium, why?  Genome characteristics.  Data:  Coverage  Error distributions  Read sizes  Fragment sizes  Kmer spectra", 
            "title": "Pair-end assembly using Chalara dataset"
        }, 
        {
            "location": "/modules/denovo-module-scaffolding/scaffold/#chalara-scaffolding-using-long-mate-pair-readslmp", 
            "text": "Let\u2019s put some LMP in there.  1 cd /home/trainee/scaffolding/cha_raw/test_3   This is also a pre-computed example for you:  1\n2\n3 abyss-pe name=chalmp1 k=61 in= ../LIB2570_raw_R1.fastq ../LIB2570_raw_R2.fastq  mp= lmp1  lmp1= ../LIB8209_raw_R1.fastq ../LIB8209_raw_R2.fastq  np=4\n\nless chalmp1-stats.tab   [H]  lllllllllll  n     n:500     L50     min     N80   N50   N20  E-size   max     sum   name \\\n130548   12596   1770   500   3352   8379   16380   10518   54300  \n50.75e6   chalmp1-unitigs.fa\\\n130211   12575   1771   500   3363   8382   16380   10525   54300  \n50.78e6   chalmp1-contigs.fa\\\n130148   12545   1769   500   3380   8400   16380   10535   54300  \n50.78e6   chalmp1-scaffolds.fa\\  [tab:chak27]  So  what happened? Let\u2019s check the data by:  Kmer spectra  Fragment sizes  Any hints on the protocol? And a not-so-obvious property:  1 kat plot spectra-mx --intersection -x 30 -y 15000000 -o pe_vs_lmp-main.mx.spectra-mx.png pe_vs_lmp-main.mx   Do you remember LMP require pre-processing? Let\u2019s try with processed\nLMP.  Prior task (already made) preprocess the LMP with nextclip.  1 cd /home/trainee/scaffolding/cha_proc/test_1   This is also a pre-computed example for you:  1\n2\n3 abyss-pe name=chalmpproc1 k=61 in= ../../cha_raw/LIB2570_raw_R1.fastq ../../cha_raw/LIB2570_raw_R2.fastq  mp= proclmp1  proclmp1= ../LIB8209_preproc_R1.fastq ../LIB8209_preproc_R2.fastq  np=4\n\nless chalmpproc1-stats.tab   [H]  lllllllllll  n     n:500     L50     min     N80   N50   N20  E-size   max     sum   name \\\n130548   12596   1770   500   3352   8379   16380   10518   54300  \n50.75e6   chalmpproc1-unitigs.fa\\\n130211   12575   1771   500   3363   8382   16380   10525   54300  \n50.78e6   chalmpproc1-contigs.fa\\\n122061   6679   167   500   18609   87510   187171   106178   397967  \n51.15e6   chalmpproc1-scaffolds.fa\\  [tab:chaklmpk61]  That\u2019s much better!", 
            "title": "Chalara scaffolding using long mate-pair reads(LMP)"
        }, 
        {
            "location": "/modules/denovo-module-scaffolding/scaffold/#chalara-beyond-first-pass", 
            "text": "Do you remember these?  Genome characteristics. Data:    Coverage    Error distributions    Read sizes    Fragment sizes    Kmer spectra    Let\u2019s think a bit and try to improve the assembly  If you look at the pre-processed LMP, do you notice anything peculiar?\\  Needs to fill  Testing the inclussion of heavily pre-procesed LMP coverage into the DBG  1 cd /home/trainee/scaffolding/cha_proc/test_2   This is also a pre-computed example for you:  1\n2\n3 abyss-pe name=chalmp2 k=61 se= ../LIB8209_preproc_single.fastq  lib= lmp1  lmp1= ../LIB8209_preproc_R1.fastq ../LIB8209_preproc_R2.fastq  np=4  chaproclmp2.log 2 1\n\nless chalmp2-stats.tab   Let\u2019s look at the stats.  [H]  lllllllllll  n     n:500     L50     min     N80   N50   N20  E-size   max     sum   name \\\n128306   10150   1182   500   4954   12585   24777   15693   68684  \n51.03e6   chaproclmp4-unitigs.fa\\\n118939   5772   362   500   15717   41870   82033   52819   252066  \n52.24e6   chaproclmp4-contigs.fa\\\n116139   4014   141   500   41145   109273   224986   133450   460979  \n52.56e6   chaproclmp4-scaffolds.fa\\  [tab:chaklmp2-k61]", 
            "title": "Chalara: beyond first pass"
        }, 
        {
            "location": "/modules/denovo-module-scaffolding/scaffold/#references", 
            "text": "Robert Ekblom* andJochen B. W. Wolf.  A field guide to whole-genome\n    sequencing, assembly and annotation  Evolutionary Applications\n    Special Issue: Evolutionary Conservation Volume 7, Issue 9, pages\n    1026 \u2013 1042, November 2014    De novo genome assembly: what every biologist should know Nature\n    Methods 9, 333 \u2013 337 (2012) doi:10.1038/nmeth.1935", 
            "title": "References"
        }, 
        {
            "location": "/modules/metagenomics-module-qc/ngs-qc/", 
            "text": "Data Quality Control\n\n\nKey Learning Outcomes\n\n\n\n\nAfter completing this practical the trainee should be able to:\n\n\n\n\n\n\nAssess the overall quality of NGS (FastQ format) sequence reads\n\n\n\n\n\n\nVisualise the quality, and other associated matrices, of reads to decide on filters and cutoffs for cleaning up data ready for downstream analysis\n\n\n\n\n\n\nClean up adaptors and pre-process the sequence data for further analysis\n\n\n\n\n\n\nResources You\u2019ll be Using\n\n\n\n\nTools Used\n\n\nFastQC:\n\n\nhttp://www.bioinformatics.babraham.ac.uk/projects/fastqc/\n\n\nSkewer:\n\n\nhttp://sourceforge.net/projects/skewer/\n\n\nFASTX-Toolkit:\n\n\nhttp://hannonlab.cshl.edu/fastx_toolkit/\n\n\nPicard:\n\n\nhttp://picard.sourceforge.net/\n\n\nUseful Links\n\n\n\n\nFASTQ Encoding:\n\n\nhttp://en.wikipedia.org/wiki/FASTQ_format#Encoding\n\n\nIntroduction\n\n\n\n\nGoing on a blind date with your read set? For a better understanding of\nthe consequences please check the data quality!\n\n\nFor the purpose of this tutorial we are focusing only on Illumina\nsequencing which uses \u2019sequence by synthesis\u2019 technology in a highly\nparallel fashion. Although Illumina high throughput sequencing provides\nhighly accurate sequence data, several sequence artifacts, including\nbase calling errors and small insertions/deletions, poor quality reads\nand primer/adapter contamination are quite common in the high throughput\nsequencing data. The primary errors are substitution errors. The error\nrates can vary from 0.5-2.0% with errors mainly rising in frequency at\nthe 3\u2019 ends of reads.\n\n\nOne way to investigate sequence data quality is to visualize the quality\nscores and other metrics in a compact manner to get an idea about the\nquality of a read data set. Read data sets can be improved by pre\nprocessing in different ways like trimming off low quality bases,\ncleaning up any sequencing adapters, removing PCR duplicates and\nscreening for contamination. We can also look at other statistics such\nas, sequence length distribution, base composition, sequence complexity,\npresence of ambiguous bases etc. to assess the overall quality of the\ndata set.\n\n\nHighly redundant coverage (\n15X) of the genome can be used to correct\nsequencing errors in the reads before assembly. Various k-mer based\nerror correction methods exist but are beyond the scope of this\ntutorial.\n\n\nQuality Value Encoding Schema\n\n\nIn order to use a single character to encode Phred qualities, ASCII\ncharacters are used\n(\nhttp://shop.alterlinks.com/ascii-table/ascii-table-us.php\n). All ASCII\ncharacters have a decimal number associated with them but the first 32\ncharacters are non-printable (e.g. backspace, shift, return, escape).\nTherefore, the first printable ASCII character is number 33, the\nexclamation mark (!). In Phred+33 encoded quality values the exclamation\nmark takes the Phred quality score of zero.\n\n\nEarly Solexa (now Illumina) sequencing needed to encode negative quality\nvalues. Because ASCII characters $\n$ 33 are non-printable, using the\nPhred+33 encoding was not possible. Therefore, they simply moved the\noffset from 33 to 64 thus inventing the Phred+64 encoded quality values.\nIn this encoding a Phred quality of zero is denoted by the ASCII number\n64 (the @ character). Since Illumina 1.8, quality values are now encoded\nusing Phred+33.\n\n\nFASTQ does not provide a way to describe what quality encoding is used\nfor the quality values. Therefore, you should find this out from your\nsequencing provider. Alternatively, you may be able to figure this out\nby determining what ASCII characters are present in the FASTQ file. E.g\nthe presence of numbers in the quality strings, can only mean the\nquality values are Phred+33 encoded. However, due to the overlapping\nnature of the Phred+33 and Phred+64 encoding schema it is not always\npossible to identify what encoding is in use. For example, if the only\ncharacters seen in the quality string are (\n@ABCDEFGHI\n), then it is\nimpossible to know if you have really good Phred+33 encoded qualities or\nreally bad Phred+64 encoded qualities.\n\n\nFor a graphical representation of the different ASCII characters used in\nthe two encoding schema see:\n\nhttp://en.wikipedia.org/wiki/FASTQ_format#Encoding\n\n\nPrepare the Environment\n\n\n\n\nTo investigate sequence data quality we will demonstrate tools called\nFastQC and Skewer. FastQC will process and present the reports in a\nvisual manner. Based on the results, the sequence data can be processed\nusing the Skewer. We will use one data set in this practical, which can\nbe found in the QC directory on your desktop.\n\n\nOpen the Terminal and go to the directory where the data are stored:\n\n\n1\n2\n3\nls\ncd qc\npwd\n\n\n\n\n\n\nAt any time, help can be displayed for FastQC using the following\ncommand:\n\n\n1\nfastqc -h\n\n\n\n\n\n\nLook at SYNOPSIS (Usage) and options after typing fastqc -h\n\n\nQuality Visualisation\n\n\n\n\nWe have a file for a good quality and bad quality statistics. FastQC\ngenerates results in the form of a zipped and unzipped directory for\neach input file.\n\n\nExecute the following command on the two files:\n\n\n1\n2\nfastqc -f qcdemo_R1.fastq.gz\nfastqc -f qcdemo_R2.fastq.gz\n\n\n\n\n\n\nView the FastQC report file of the bad data using a web browser such as\nfirefox. The \u2019\n\u2019 sign puts the job in the background.\n\n\n1\nfirefox qcdemo_R2_fastqc.html \n\n\n\n\n\n\n\nThe report file will have a Basic Statistics table and various graphs\nand tables for different quality statistics. E.g.:\n\n\n\n\n\n\n\n\nFilename\n\n\nqcdemo_R2.fastq.gz\n\n\n\n\n\n\n\n\n\n\nFile type\n\n\nConventional base calls\n\n\n\n\n\n\nEncoding\n\n\nSanger / Illumina 1.9\n\n\n\n\n\n\nTotal Sequences\n\n\n1000000\n\n\n\n\n\n\nFiltered Sequences\n\n\n0\n\n\n\n\n\n\nSequence length\n\n\n150\n\n\n\n\n\n\n%GC\n\n\n37\n\n\n\n\n\n\n\n\nFastQC Basic Statistics table\n\n\n\n\nPer base sequence quality plot for\n\nqcdemo_R2.fastq.gz\n.\n\n\n\n\nA Phred quality score (or Q-score) expresses an error probability. In\nparticular, it serves as a convenient and compact way to communicate\nvery small error probabilities. The probability that base \nA\n is wrong \nP(sim A)\n is expressed by a quality score, \nQ(A)\n, according to the\nrelationship:\n\n\nQ(A) =-10 log10(P(sim A))\n\n\nThe relationship between the quality score and error probability is\ndemonstrated with the following table:\n\n\n\n\n\n\n\n\nQuality score\n\n\nError probability\n\n\nAccuracy of the base call\n\n\n\n\n\n\n\n\n\n\n10\n\n\n0.1\n\n\n90%\n\n\n\n\n\n\n20\n\n\n0.01\n\n\n99%\n\n\n\n\n\n\n30\n\n\n0.001\n\n\n99.9%\n\n\n\n\n\n\n40\n\n\n0.0001\n\n\n99.99%\n\n\n\n\n\n\n50\n\n\n0.00001\n\n\n99.999%\n\n\n\n\n\n\n\n\nError probabilities associated with various quality (Q) values\n\n\n[tab:quality_error_probs]\n\n\n\n\n\n\nHow many sequences were there in your file? What is the read length?\n\n\n\n\nThis is a spoiler: {%s%}Hello World.{%ends%}\n\n\n1\n 1,000,000. read length=150bp\n\n\n\n\n\n\n\n\n\n\nDoes the quality score values vary throughout the read length? (hint:\nlook at the \u2019per base sequence quality plot\u2019)\n\n\n\n\nYes. Quality scores are dropping towards the end of the reads.\n\n\n\n\n\n\n\n\nWhat is the quality score range you see?\n\n\n\n\n2-40\n\n\n\n\n\n\n\n\nAt around which position do the scores start falling below Q20 for the 25% quartile range (25%of reads below Q20)?\n\n\n\n\nAround 30 bp position\n\n\n\n\n\n\n\n\nHow can we trim the reads to filter out the low quality data?\n\n\n\n\nBy trimming off the bases after a fixed position of the read or by trimming off bases based on the quality score.\n\n\n\n\n\n\n\n\nGood Quality Data\n\n\nView the FastQC report files \nfastqc_report.html\n to see examples of a\ngood quality data and compare the quality plot with that of the\n\nbad_example_fastqc\n.\n\n\n1\nfirefox qcdemo_R1_fastqc.html \n\n\n\n\n\n\n\nSequencing errors can complicate the downstream analysis, which normally\nrequires that reads be aligned to each other (for genome assembly) or to\na reference genome (for detection of mutations). Sequence reads\ncontaining errors may lead to ambiguous paths in the assembly or\nimproper gaps. In variant analysis projects sequence reads are aligned\nagainst the reference genome. The errors in the reads may lead to more\nmismatches than expected from mutations alone. But if these errors can\nbe removed or corrected, the read alignments and hence the variant\ndetection will improve. The assemblies will also improve after\npre-processing the reads to remove errors.\n\n\nRead Trimming\n\n\n\n\nRead trimming can be done in a variety of different ways. Choose a\nmethod which best suits your data. Here we are giving examples of\nfixed-length trimming and quality-based trimming.\n\n\nQuality Based Trimming\n\n\nBase call quality scores can be used to dynamically determine the trim\npoints for each read. A quality score threshold and minimum read length\nfollowing trimming can be used to remove low quality data.\n\n\nThe previous FastQC results show R1 is fine but R2 has low quality at\nthe end. There is no adaptor contamination though. We will be using\nSkewer to perform the quality trimming.\n\n\nRun the following command to quality trim a set of paired end data.\n\n\n1\n2\ncd\n ~/qc\nskewer -t \n20\n -l \n50\n  -q \n30\n -Q \n25\n -m pe qcdemo_R1.fastq.gz qcdemo_R2.fastq.gz\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n-t :   number of threads to use\n\n-l :   min length to keep after trimming\n\n-q :   Quality threshold used for trimming at 3\u2019 end\n\n-Q :   mean quality threshold for a read\n\n-m :   pair-end mode\n\n\n\n\n\n\nRun FastQC on the quality trimmed file and visualise the quality scores.\n\n\n1\n2\n3\n4\nfastqc -f fastq qcdemo_R1.fastq-trimmed-pair1.fastq\nfastqc -f fastq qcdemo_R1.fastq-trimmed-pair2.fastq\nfirefox qcdemo_R1.fastq-trimmed-pair1_fastqc.html \n\nfirefox qcdemo_R1.fastq-trimmed-pair2_fastqc.html\n\n\n\n\n\n\n\nLet\u2019s look at the quality from the second reads. The output should look\nlike:\n\n\nFastQC Basic Statistics table\n\n\n\n\n\n\n\n\nFilename\n\n\nqcdemo_R1.fastq-trimmed-pair2.fastq\n\n\n\n\n\n\n\n\n\n\nFile type\n\n\nConventional base calls\n\n\n\n\n\n\nEncoding\n\n\nSanger / Illumina 1.9\n\n\n\n\n\n\nTotal Sequences\n\n\n742262\n\n\n\n\n\n\nFiltered Sequences\n\n\n0\n\n\n\n\n\n\nSequence length\n\n\n50\n\n\n\n\n\n\n%GC\n\n\n37\n\n\n\n\n\n\n\n\nPer base sequence quality plot for the quality-trimmed\n\n\nqcdemo_R2.fastq.gz\n\n\n\n\n\n\n\n\nDid the number of total reads in R1 and R2 change after trimming?\n\n\n\n\n\n\nQuality trimming discarded \n1000 reads. However, We retain a lot of maximal length reads which have good quality all the way to the ends.\n\n\n\n\n\n\nWhat reads lengths were obtained after quality based trimming?\n\n\n\n\n\n\n50-150 Reads \n50 bp, following quality trimming, were discarded.\n\n\n\n\n\n\nDid you observe adapter sequences in the data?\n\n\n\n\n\n\nNo. (Hint: look at the overrepresented sequences.\n\n\n\n\n\n\nHow can you use -a option with fastqc ? (Hint: try fastqc -h).\n\n\n\n\n\n\nAdaptors can be supplied in a file for screening.\n\n\n\n\nAdapter Clipping\n\n\nSometimes sequence reads may end up getting the leftover of adapters and\nprimers used in the sequencing process. It\u2019s good practice to screen\nyour data for these possible contamination for more sensitive alignment\nand assembly based analysis.\n\n\nThis is particularly important when read lengths can be longer than the\nmolecules being sequenced. For example when sequencing miRNAs.\n\n\nVarious QC tools are available to screen and/or clip these\nadapter/primer sequences from your data. Apart from skewer which will be\nusing today the following two tools are also useful for trimming and\nremoving adapter sequence.\n\n\nCutadapt: (\nhttp://code.google.com/p/cutadapt/\n)\nTrimmomatic:\n(\nhttp://www.usadellab.org/cms/?page=trimmomatic\n)\n\n\nHere we are demonstrating \nSkewer\n to trim a given adapter sequence.\n\n\n1\n2\n3\ncd ~/qc\nfastqc -f fastq  adaptorQC.fastq.gz\nskewer -x TGGAATTCTCGGGTGCCAAGGT -t 20 -l 10 -L 35 -q 30 adaptorQC.fastq.gz\n\n\n\n\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n-x :   adaptor sequence used\n\n-t :   number of threads to use\n\n-l :   min length to keep after trimming\n\n-L :   Max length to keep after trimming, in this experiment we were\n    expecting only small RNA fragments\n\n-Q :   Quality threshold used for trimming at 3\u2019 end. Use -m option to\n    control the end you want to trim\n\n\n\n\n\n\nRun FastQC on the adapter trimmed file and visualise the quality scores.\nFastqc now shows adaptor free results.\n\n\n1\n2\nfastqc -f fastq adaptorQC.fastq-trimmed.fastq\nfirefox adaptorQC.fastq-trimmed_fastqc.html \n\n\n\n\n\n\n\nAn alternative tool, not installed on this system, for adapter clipping\nis \nfastq-mcf\n. A list of adapters is provided in a text file. For more\ninformation, see FastqMcf at\n\nhttp://code.google.com/p/ea-utils/wiki/FastqMcf\n.\n\n\nFixed Length Trimming\n\n\nWe will not cover Fixed Length Trimming but provide the following for your information.\n Low quality read ends can be trimmed using a\nfixed-length trimming. We will use the \nfastx_trimmer\n from the\nFASTX-Toolkit. Usage message to find out various options you can use\nwith this tool. Type \nfastx_trimmer -h\n at anytime to display help.\n\n\nWe will now do fixed-length trimming of the \nbad_example.fastq\n file\nusing the following command. You should still be in the qc directory, if\nnot cd back in.\n\n\n1\n2\n3\n4\ncd ~/qc\nfastqc -f fastq bad_example.fastq\nfastx_trimmer -h\nfastx_trimmer -Q 33 -f 1 -l 80 -i bad_example.fastq -o bad_example_trimmed01.fastq\n\n\n\n\n\n\nWe used the following options in the command above:\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n-Q 33 :   Indicates the input quality scores are Phred+33 encoded\n\n-f :   First base to be retained in the output\n\n-l :   Last base to be retained in the output\n\n-i :   Input FASTQ file name\n\n-o :   Output file name\n\n\n\n\n\nRun FastQC on the trimmed file and visualise the quality scores of the\ntrimmed file.\n\n\n1\n2\nfastqc -f fastq bad_example_trimmed01.fastq\nfirefox bad_example_trimmed01_fastqc.html \n\n\n\n\n\n\n\nThe output should look like:\n\n\n\n\n\n\n\n\nFilename\n\n\nbad_example_trimmed01.fastq\n\n\n\n\n\n\n\n\n\n\nFile type\n\n\nConventional base call\n\n\n\n\n\n\nEncoding\n\n\nSanger / Illumina 1.9\n\n\n\n\n\n\nTotal Sequences\n\n\n40000\n\n\n\n\n\n\nFiltered Sequences\n\n\n0\n\n\n\n\n\n\nSequence length\n\n\n80\n\n\n\n\n\n\n%GC\n\n\n48\n\n\n\n\n\n\n\n\n: FastQC Basic Statistics table\n\n\n\n\ntab:badexampletrimmed\n\n\n\n\n![Per base sequence quality plot for the fixed-length trimmed\n\nbad_example.fastq\n\n\n\n\nWhat values would you use for \n-f\n if you wanted to trim off 10 bases at\nthe 5\u2019 end of the reads?\n\n\n-f 11\n\n\nRemoving Duplicates\n\n\nDuplicate reads are the ones having the same start and end coordinates.\nThis may be the result of technical duplication (too many PCR cycles),\nor over-sequencing (very high fold coverage). It is very important to\nput the duplication level in context of your experiment. For example,\nduplication level in targeted or re-sequencing projects may mean\nsomething different in RNA-seq experiments. In RNA-seq experiments\noversequencing is usually necessary when detecting low abundance\ntranscripts.\n\n\nThe duplication level computed by FastQC is based on sequence identity\nat the end of reads. Another tool, Picard, determines duplicates based\non identical start and end positions in SAM/BAM alignment files.\n\n\nWe will not cover Picard but provide the following for your\ninformation.\n\n\nPicard is a suite of tools for performing many common tasks with SAM/BAM\nformat files. For more information see the Picard website and\ninformation about the various command-line tools available:\n\n\nhttp://picard.sourceforge.net/command-line-overview.shtml\n\n\nA good list of tools for filtering PCR duplication can also be found at\n\nhttp://omictools.com/duplicate-reads-removal-c495-p1.html\n\n\nPicard is installed on this system in \n/usr/share/java\n\n\nOne of the Picard tools (MarkDuplicates) can be used to analyse and\nremove duplicates from the raw sequence data. The input for Picard is a\nsorted alignment file in BAM format. Short read aligners such as,\nbowtie, BWA and tophat can be used to align FASTQ files against a\nreference genome to generate SAM/BAM alignment format.\n\n\nInterested users can use the following general command to run the\nMarkDuplicates tool at their leisure. You only need to provide a BAM\nfile for the INPUT argument (not provided):\n\n\n1\n2\ncd ~/qc\njava -jar /usr/share/java/MarkDuplicates.jar INPUT=\nalignment_file.bam\n VALIDATION_STRINGENCY=LENIENT OUTPUT=alignment_file.dup METRICS_FILE=alignment_file.matric ASSUME_SORTED=true REMOVE_DUPLICATES=true", 
            "title": "Metagenomics QC"
        }, 
        {
            "location": "/modules/metagenomics-module-qc/ngs-qc/#data-quality-control", 
            "text": "", 
            "title": "Data Quality Control"
        }, 
        {
            "location": "/modules/metagenomics-module-qc/ngs-qc/#key-learning-outcomes", 
            "text": "After completing this practical the trainee should be able to:    Assess the overall quality of NGS (FastQ format) sequence reads    Visualise the quality, and other associated matrices, of reads to decide on filters and cutoffs for cleaning up data ready for downstream analysis    Clean up adaptors and pre-process the sequence data for further analysis", 
            "title": "Key Learning Outcomes"
        }, 
        {
            "location": "/modules/metagenomics-module-qc/ngs-qc/#resources-youll-be-using", 
            "text": "", 
            "title": "Resources You\u2019ll be Using"
        }, 
        {
            "location": "/modules/metagenomics-module-qc/ngs-qc/#tools-used", 
            "text": "FastQC:  http://www.bioinformatics.babraham.ac.uk/projects/fastqc/  Skewer:  http://sourceforge.net/projects/skewer/  FASTX-Toolkit:  http://hannonlab.cshl.edu/fastx_toolkit/  Picard:  http://picard.sourceforge.net/", 
            "title": "Tools Used"
        }, 
        {
            "location": "/modules/metagenomics-module-qc/ngs-qc/#useful-links", 
            "text": "FASTQ Encoding:  http://en.wikipedia.org/wiki/FASTQ_format#Encoding", 
            "title": "Useful Links"
        }, 
        {
            "location": "/modules/metagenomics-module-qc/ngs-qc/#introduction", 
            "text": "Going on a blind date with your read set? For a better understanding of\nthe consequences please check the data quality!  For the purpose of this tutorial we are focusing only on Illumina\nsequencing which uses \u2019sequence by synthesis\u2019 technology in a highly\nparallel fashion. Although Illumina high throughput sequencing provides\nhighly accurate sequence data, several sequence artifacts, including\nbase calling errors and small insertions/deletions, poor quality reads\nand primer/adapter contamination are quite common in the high throughput\nsequencing data. The primary errors are substitution errors. The error\nrates can vary from 0.5-2.0% with errors mainly rising in frequency at\nthe 3\u2019 ends of reads.  One way to investigate sequence data quality is to visualize the quality\nscores and other metrics in a compact manner to get an idea about the\nquality of a read data set. Read data sets can be improved by pre\nprocessing in different ways like trimming off low quality bases,\ncleaning up any sequencing adapters, removing PCR duplicates and\nscreening for contamination. We can also look at other statistics such\nas, sequence length distribution, base composition, sequence complexity,\npresence of ambiguous bases etc. to assess the overall quality of the\ndata set.  Highly redundant coverage ( 15X) of the genome can be used to correct\nsequencing errors in the reads before assembly. Various k-mer based\nerror correction methods exist but are beyond the scope of this\ntutorial.", 
            "title": "Introduction"
        }, 
        {
            "location": "/modules/metagenomics-module-qc/ngs-qc/#quality-value-encoding-schema", 
            "text": "In order to use a single character to encode Phred qualities, ASCII\ncharacters are used\n( http://shop.alterlinks.com/ascii-table/ascii-table-us.php ). All ASCII\ncharacters have a decimal number associated with them but the first 32\ncharacters are non-printable (e.g. backspace, shift, return, escape).\nTherefore, the first printable ASCII character is number 33, the\nexclamation mark (!). In Phred+33 encoded quality values the exclamation\nmark takes the Phred quality score of zero.  Early Solexa (now Illumina) sequencing needed to encode negative quality\nvalues. Because ASCII characters $ $ 33 are non-printable, using the\nPhred+33 encoding was not possible. Therefore, they simply moved the\noffset from 33 to 64 thus inventing the Phred+64 encoded quality values.\nIn this encoding a Phred quality of zero is denoted by the ASCII number\n64 (the @ character). Since Illumina 1.8, quality values are now encoded\nusing Phred+33.  FASTQ does not provide a way to describe what quality encoding is used\nfor the quality values. Therefore, you should find this out from your\nsequencing provider. Alternatively, you may be able to figure this out\nby determining what ASCII characters are present in the FASTQ file. E.g\nthe presence of numbers in the quality strings, can only mean the\nquality values are Phred+33 encoded. However, due to the overlapping\nnature of the Phred+33 and Phred+64 encoding schema it is not always\npossible to identify what encoding is in use. For example, if the only\ncharacters seen in the quality string are ( @ABCDEFGHI ), then it is\nimpossible to know if you have really good Phred+33 encoded qualities or\nreally bad Phred+64 encoded qualities.  For a graphical representation of the different ASCII characters used in\nthe two encoding schema see: http://en.wikipedia.org/wiki/FASTQ_format#Encoding", 
            "title": "Quality Value Encoding Schema"
        }, 
        {
            "location": "/modules/metagenomics-module-qc/ngs-qc/#prepare-the-environment", 
            "text": "To investigate sequence data quality we will demonstrate tools called\nFastQC and Skewer. FastQC will process and present the reports in a\nvisual manner. Based on the results, the sequence data can be processed\nusing the Skewer. We will use one data set in this practical, which can\nbe found in the QC directory on your desktop.  Open the Terminal and go to the directory where the data are stored:  1\n2\n3 ls\ncd qc\npwd   At any time, help can be displayed for FastQC using the following\ncommand:  1 fastqc -h   Look at SYNOPSIS (Usage) and options after typing fastqc -h", 
            "title": "Prepare the Environment"
        }, 
        {
            "location": "/modules/metagenomics-module-qc/ngs-qc/#quality-visualisation", 
            "text": "We have a file for a good quality and bad quality statistics. FastQC\ngenerates results in the form of a zipped and unzipped directory for\neach input file.  Execute the following command on the two files:  1\n2 fastqc -f qcdemo_R1.fastq.gz\nfastqc -f qcdemo_R2.fastq.gz   View the FastQC report file of the bad data using a web browser such as\nfirefox. The \u2019 \u2019 sign puts the job in the background.  1 firefox qcdemo_R2_fastqc.html     The report file will have a Basic Statistics table and various graphs\nand tables for different quality statistics. E.g.:     Filename  qcdemo_R2.fastq.gz      File type  Conventional base calls    Encoding  Sanger / Illumina 1.9    Total Sequences  1000000    Filtered Sequences  0    Sequence length  150    %GC  37     FastQC Basic Statistics table   Per base sequence quality plot for qcdemo_R2.fastq.gz .   A Phred quality score (or Q-score) expresses an error probability. In\nparticular, it serves as a convenient and compact way to communicate\nvery small error probabilities. The probability that base  A  is wrong  P(sim A)  is expressed by a quality score,  Q(A) , according to the\nrelationship:  Q(A) =-10 log10(P(sim A))  The relationship between the quality score and error probability is\ndemonstrated with the following table:     Quality score  Error probability  Accuracy of the base call      10  0.1  90%    20  0.01  99%    30  0.001  99.9%    40  0.0001  99.99%    50  0.00001  99.999%     Error probabilities associated with various quality (Q) values  [tab:quality_error_probs]    How many sequences were there in your file? What is the read length?   This is a spoiler: {%s%}Hello World.{%ends%}  1  1,000,000. read length=150bp     Does the quality score values vary throughout the read length? (hint:\nlook at the \u2019per base sequence quality plot\u2019)   Yes. Quality scores are dropping towards the end of the reads.     What is the quality score range you see?   2-40     At around which position do the scores start falling below Q20 for the 25% quartile range (25%of reads below Q20)?   Around 30 bp position     How can we trim the reads to filter out the low quality data?   By trimming off the bases after a fixed position of the read or by trimming off bases based on the quality score.", 
            "title": "Quality Visualisation"
        }, 
        {
            "location": "/modules/metagenomics-module-qc/ngs-qc/#good-quality-data", 
            "text": "View the FastQC report files  fastqc_report.html  to see examples of a\ngood quality data and compare the quality plot with that of the bad_example_fastqc .  1 firefox qcdemo_R1_fastqc.html     Sequencing errors can complicate the downstream analysis, which normally\nrequires that reads be aligned to each other (for genome assembly) or to\na reference genome (for detection of mutations). Sequence reads\ncontaining errors may lead to ambiguous paths in the assembly or\nimproper gaps. In variant analysis projects sequence reads are aligned\nagainst the reference genome. The errors in the reads may lead to more\nmismatches than expected from mutations alone. But if these errors can\nbe removed or corrected, the read alignments and hence the variant\ndetection will improve. The assemblies will also improve after\npre-processing the reads to remove errors.", 
            "title": "Good Quality Data"
        }, 
        {
            "location": "/modules/metagenomics-module-qc/ngs-qc/#read-trimming", 
            "text": "Read trimming can be done in a variety of different ways. Choose a\nmethod which best suits your data. Here we are giving examples of\nfixed-length trimming and quality-based trimming.", 
            "title": "Read Trimming"
        }, 
        {
            "location": "/modules/metagenomics-module-qc/ngs-qc/#quality-based-trimming", 
            "text": "Base call quality scores can be used to dynamically determine the trim\npoints for each read. A quality score threshold and minimum read length\nfollowing trimming can be used to remove low quality data.  The previous FastQC results show R1 is fine but R2 has low quality at\nthe end. There is no adaptor contamination though. We will be using\nSkewer to perform the quality trimming.  Run the following command to quality trim a set of paired end data.  1\n2 cd  ~/qc\nskewer -t  20  -l  50   -q  30  -Q  25  -m pe qcdemo_R1.fastq.gz qcdemo_R2.fastq.gz   1\n2\n3\n4\n5\n6\n7\n8\n9 -t :   number of threads to use\n\n-l :   min length to keep after trimming\n\n-q :   Quality threshold used for trimming at 3\u2019 end\n\n-Q :   mean quality threshold for a read\n\n-m :   pair-end mode   Run FastQC on the quality trimmed file and visualise the quality scores.  1\n2\n3\n4 fastqc -f fastq qcdemo_R1.fastq-trimmed-pair1.fastq\nfastqc -f fastq qcdemo_R1.fastq-trimmed-pair2.fastq\nfirefox qcdemo_R1.fastq-trimmed-pair1_fastqc.html  \nfirefox qcdemo_R1.fastq-trimmed-pair2_fastqc.html    Let\u2019s look at the quality from the second reads. The output should look\nlike:  FastQC Basic Statistics table     Filename  qcdemo_R1.fastq-trimmed-pair2.fastq      File type  Conventional base calls    Encoding  Sanger / Illumina 1.9    Total Sequences  742262    Filtered Sequences  0    Sequence length  50    %GC  37     Per base sequence quality plot for the quality-trimmed  qcdemo_R2.fastq.gz     Did the number of total reads in R1 and R2 change after trimming?    Quality trimming discarded  1000 reads. However, We retain a lot of maximal length reads which have good quality all the way to the ends.    What reads lengths were obtained after quality based trimming?    50-150 Reads  50 bp, following quality trimming, were discarded.    Did you observe adapter sequences in the data?    No. (Hint: look at the overrepresented sequences.    How can you use -a option with fastqc ? (Hint: try fastqc -h).    Adaptors can be supplied in a file for screening.", 
            "title": "Quality Based Trimming"
        }, 
        {
            "location": "/modules/metagenomics-module-qc/ngs-qc/#adapter-clipping", 
            "text": "Sometimes sequence reads may end up getting the leftover of adapters and\nprimers used in the sequencing process. It\u2019s good practice to screen\nyour data for these possible contamination for more sensitive alignment\nand assembly based analysis.  This is particularly important when read lengths can be longer than the\nmolecules being sequenced. For example when sequencing miRNAs.  Various QC tools are available to screen and/or clip these\nadapter/primer sequences from your data. Apart from skewer which will be\nusing today the following two tools are also useful for trimming and\nremoving adapter sequence.  Cutadapt: ( http://code.google.com/p/cutadapt/ )\nTrimmomatic:\n( http://www.usadellab.org/cms/?page=trimmomatic )  Here we are demonstrating  Skewer  to trim a given adapter sequence.  1\n2\n3 cd ~/qc\nfastqc -f fastq  adaptorQC.fastq.gz\nskewer -x TGGAATTCTCGGGTGCCAAGGT -t 20 -l 10 -L 35 -q 30 adaptorQC.fastq.gz    1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11 -x :   adaptor sequence used\n\n-t :   number of threads to use\n\n-l :   min length to keep after trimming\n\n-L :   Max length to keep after trimming, in this experiment we were\n    expecting only small RNA fragments\n\n-Q :   Quality threshold used for trimming at 3\u2019 end. Use -m option to\n    control the end you want to trim   Run FastQC on the adapter trimmed file and visualise the quality scores.\nFastqc now shows adaptor free results.  1\n2 fastqc -f fastq adaptorQC.fastq-trimmed.fastq\nfirefox adaptorQC.fastq-trimmed_fastqc.html     An alternative tool, not installed on this system, for adapter clipping\nis  fastq-mcf . A list of adapters is provided in a text file. For more\ninformation, see FastqMcf at http://code.google.com/p/ea-utils/wiki/FastqMcf .", 
            "title": "Adapter Clipping"
        }, 
        {
            "location": "/modules/metagenomics-module-qc/ngs-qc/#fixed-length-trimming", 
            "text": "We will not cover Fixed Length Trimming but provide the following for your information.  Low quality read ends can be trimmed using a\nfixed-length trimming. We will use the  fastx_trimmer  from the\nFASTX-Toolkit. Usage message to find out various options you can use\nwith this tool. Type  fastx_trimmer -h  at anytime to display help.  We will now do fixed-length trimming of the  bad_example.fastq  file\nusing the following command. You should still be in the qc directory, if\nnot cd back in.  1\n2\n3\n4 cd ~/qc\nfastqc -f fastq bad_example.fastq\nfastx_trimmer -h\nfastx_trimmer -Q 33 -f 1 -l 80 -i bad_example.fastq -o bad_example_trimmed01.fastq   We used the following options in the command above: 1\n2\n3\n4\n5\n6\n7\n8\n9 -Q 33 :   Indicates the input quality scores are Phred+33 encoded\n\n-f :   First base to be retained in the output\n\n-l :   Last base to be retained in the output\n\n-i :   Input FASTQ file name\n\n-o :   Output file name   Run FastQC on the trimmed file and visualise the quality scores of the\ntrimmed file.  1\n2 fastqc -f fastq bad_example_trimmed01.fastq\nfirefox bad_example_trimmed01_fastqc.html     The output should look like:     Filename  bad_example_trimmed01.fastq      File type  Conventional base call    Encoding  Sanger / Illumina 1.9    Total Sequences  40000    Filtered Sequences  0    Sequence length  80    %GC  48     : FastQC Basic Statistics table   tab:badexampletrimmed   ![Per base sequence quality plot for the fixed-length trimmed bad_example.fastq   What values would you use for  -f  if you wanted to trim off 10 bases at\nthe 5\u2019 end of the reads?  -f 11", 
            "title": "Fixed Length Trimming"
        }, 
        {
            "location": "/modules/metagenomics-module-qc/ngs-qc/#removing-duplicates", 
            "text": "Duplicate reads are the ones having the same start and end coordinates.\nThis may be the result of technical duplication (too many PCR cycles),\nor over-sequencing (very high fold coverage). It is very important to\nput the duplication level in context of your experiment. For example,\nduplication level in targeted or re-sequencing projects may mean\nsomething different in RNA-seq experiments. In RNA-seq experiments\noversequencing is usually necessary when detecting low abundance\ntranscripts.  The duplication level computed by FastQC is based on sequence identity\nat the end of reads. Another tool, Picard, determines duplicates based\non identical start and end positions in SAM/BAM alignment files.  We will not cover Picard but provide the following for your\ninformation.  Picard is a suite of tools for performing many common tasks with SAM/BAM\nformat files. For more information see the Picard website and\ninformation about the various command-line tools available:  http://picard.sourceforge.net/command-line-overview.shtml  A good list of tools for filtering PCR duplication can also be found at http://omictools.com/duplicate-reads-removal-c495-p1.html  Picard is installed on this system in  /usr/share/java  One of the Picard tools (MarkDuplicates) can be used to analyse and\nremove duplicates from the raw sequence data. The input for Picard is a\nsorted alignment file in BAM format. Short read aligners such as,\nbowtie, BWA and tophat can be used to align FASTQ files against a\nreference genome to generate SAM/BAM alignment format.  Interested users can use the following general command to run the\nMarkDuplicates tool at their leisure. You only need to provide a BAM\nfile for the INPUT argument (not provided):  1\n2 cd ~/qc\njava -jar /usr/share/java/MarkDuplicates.jar INPUT= alignment_file.bam  VALIDATION_STRINGENCY=LENIENT OUTPUT=alignment_file.dup METRICS_FILE=alignment_file.matric ASSUME_SORTED=true REMOVE_DUPLICATES=true", 
            "title": "Removing Duplicates"
        }, 
        {
            "location": "/modules/metagenomics-module-tax/tax/", 
            "text": "An introduction to taxonomic analysis of amplicon and shotgun data using QIIME\n\n\nKey Learning Outcomes\n\n\n\n\nAfter completing this practical the trainee should be able to:\n\n\n\n\n\n\nUnderstand the open source software package QIIME for analysis\n\n\n\n\n\n\nPerform a taxonomic analysis on a 16S rRNA amplicon dataset\n\n\n\n\n\n\nConduct 16S taxonomic analysis on shotgun data\n\n\n\n\n\n\nResources You\u2019ll be Using\n\n\n\n\nTools Used\n\n\nQIIME:\n\n\nhttp://qiime.org/\n\n\nrRNASelector: \n\n\nhttp://www.ezbiocloud.net/sw/rrnaselector\n\n\nMEGAN:\n\n\nhttp://ab.inf.uni-tuebingen.de/software/megan5\n\n\nUseful Links\n\n\n\n\nFASTQ Encoding: \n\n\nhttp://en.wikipedia.org/wiki/FASTQ_format#Encoding\n\n\nSommer data: \n\n\nhttps://www.ebi.ac.uk/metagenomics/projects/ERP013648#samples_id\n\n\nSources of Data\n\n\n\n\n\n\nFelix Sommer et al. (2016). The Gut Microbiota Modulates Energy\n    Metabolism in the Hibernating Brown Bear Ursus arctos\n\n\n\n\n\n\nSutcliffe et al. (2013). Draft Genome Sequence of Thermotoga\n    maritima A7A Reconstructed from Metagenomic Sequencing Analysis of a\n    Hydrocarbon Reservoir in the Bass Strait, Australia. Genome Announc.\n    1(5): e00688-13.\n\n\n\n\n\n\nLi et al. (2013). Draft Genome Sequence of Thermoanaerobacter sp.\n    Strain A7A, Reconstructed from a Metagenome Obtained from a\n    High-Temperature Hydrocarbon Reservoir in the Bass Strait,\n    Australia. Genome Announc. 1(5): e00701-13.\n\n\n\n\n\n\nLee et al. (2011). rRNASelector: a computer program for selecting\n    ribosomal RNA encoding sequences from metagenomic and\n    metatranscriptomic shotgun libraries. J. Microbiol. 49(4):689-691.\n\n\n\n\n\n\nIntroduction\n\n\n\n\nIn this tutorial we will look at the open source software package QIIME\n(pronounced \u2019chime\u2019).\n\n\nQIIME stands for Quantitative Insights Into Microbial Ecology. The\npackage contains many tools that enable users to analyse and compare\nmicrobial communities. QIIME was originally developed to analyse of\nRoche 454 amplicon sequencing data. In the latest versions workflows\nhave been added to analyze data from different sequencing platforms,\nsuch as Illumina, and different types of data, such as shotgun data. In\nthis course we will use QIIME 1.8, which is the latest version.\n\n\nAfter completion of this tutorial, you should be able to perform a\ntaxonomic analysis on a Illumina pair end MiSeq 16S rRNA amplicon\ndataset. In addition you should be able to do 16S taxonomic analysis on\nshotgun data using the tool rRNASelector in combination with QIIME\n\n\nDe novo OTU picking and diversity analysis using Illumina data\n\n\n\n\nWe will re-analyze the data from Sommer et al. (2016). The Gut\nMicrobiota Modulates Energy Metabolism in the Hibernating Brown Bear\nUrsus arctos. Sommer et al., 2016, Cell Reports 14, 1655\u20131661 February\n23, 2016. An electronic copy of the paper can be found in your Taxonomy\nfolder. When you have to wait a few minutes for commands to complete,\nuse the time to acquaint yourself with the study. It is a good example\nof a study that combines the power of next-generation sequencing with\nenvironmental observations/measurements.\n\n\nThe analysis we do follows the pipeline described in the QIIME general\ntutorial (\nhttp://qiime.org/tutorials/tutorial.html\n). Feel free to look\nat this tutorial for further background information. The dataset used in\nthe tutorial is a subset of the Sommer data. In some parts of the\nanalysis steps we have precomputed analysis on the complete data set for\ncomparison.\n\n\nJoin directory of PE reads\n\n\nFor this tutorial we will analyse the bear data dataset to complete the\nnecessary steps in a reasonable amount of time.\n\n\nFirst we need to join the Illumina Pair End (PE) reads contained in the\nbear read directory.\n\n\n1\n2\ncd ~/taxonomy\nmultiple_join_paired_ends.py -i bear_reads -o bear_join --read1_indicator _1  --read2_indicator _2\n\n\n\n\n\n\nThe Mapping File\n\n\nWe will use the mapping file \u2019mapping.txt\u2019 to associate read data with\none of 23 samples. Note that this file has to be created for each\nanalysis, as the information is specific for an experiment. The mapping\nfile can also contain information on your experimental design. The\nformat is very strict; columns are separated with a single TAB\ncharacter; the header names have to be typed exactly as specified in the\ndocumentation. A good sample description is useful as it is used in the\nlegends of the figures QIIME generates. We could also specify the\nreverse primer and remove it from the reads. Unfortunately, the reverse\nprimer sequence was not in the paper, and we\u2019ll ignore it though we\ncould probably deduce it from longer reads: as a 466 bp region of the\n16S ribosomal RNA gene flanking the V3 and V4 regions was amplified,\nyou\u2019ll have a clue where to look for the reverse primer.\n\n\nExecute the following command and test the mapping file for potential\nerrors:\n\n\n1\n2\nless -S mapping.txt\nvalidate_mapping_file.py -m mapping.txt -o mapping_output\n\n\n\n\n\n\nThere shouldn\u2019t be any errors. If there are errors, a corrected mapping\nfile will be written to the directory mapping_output\n\n\nAssign samples to the reads\n\n\nThis step sets up and quality filters the sample reads by sample\nidentities\n\n\nNext we will assign reads to the samples and quality filter at phred\nquality threshold \n= Q20 to joined reads for analysis\n\n\n1\nmultiple_split_libraries_fastq.py -i bear_join -o bear_split -m sampleid_by_file --include_input_dir_path  -p parameter_file.txt\n\n\n\n\n\n\nWhen completed please enter the directory \u2019bear_split\u2019 and have a look\nat the log file. It contains detailed information what was done during\nthe step we\u2019ve just performed. Note that the number of reads assigned to\nthe different samples varies considerably. Knowing where the individual\nsamples were taken may give a clue why this may be! Next have a quick\nlook at the file seqs.fna. What has changed to the header of the reads?\n\n\nLook at the split libraries results\n\n\nYou will see the reads are now batched to their sample.\n\n\n1\n2\n3\n4\ncd split_library_output\nls -l\nless split_library_log.txt\nless -S seqs.fna\n\n\n\n\n\n\nPicking Operational Taxonomic Units (OTUs)\n\n\n\n\nWe will now use a workflow for de novo OTU picking, taxonomy assignment,\nphylogenetic tree construction, and OTU table construction QIIME has\nseveral workflows to pick OTUs, we will be using the one described in\nthe general overview tutorial\n(\nhttp://qiime.org/tutorials/tutorial.html\n) It has 7 steps, which are\ndescribed in some detail in this tutorial. The described procedure is\nrun with the command from the Taxonomy directory. This step takes about\n12mins to run. Please read through the different steps\n(\nhttp://qiime.org/tutorials/tutorial.html\n) and try to understand the\nprocedure. Remember that an OTU is not the same as a species, but a\n\u2019bag/cluster\u2019 of highly similar sequences (at least 97% is common for\nbacteria/archaea), or a single sequence in case of rare OTUs.\n\n\nPick de novo OTUs:\n\n\n1\npick_de_novo_otus.py -i bear_split/seqs.fna -o otus\n\n\n\n\n\n\nPlease do spend some time looking at the output of this pipeline. In\nparticular the file \u2019seqs_rep_set_tax_assignments.txt\u2019 in the\n\u2019uclust_assigned_taxonomy\u2019 directory. By default QIIME uses the\nGreengenes 16S reference database to assign taxonomy.\n\n\nIt has the following levels: kingdom, phylum, class, order, family, genus, and species. It will be immediately clear that most reads cannot be classified up to species level. As described in step 6 of the QIIME overview tutorial, the pipeline creates a Newick-formatted phylogenetic\ntree (rep_set.tre) in the otus directory.\n\n\nYou can run the program \u2019figtree\u2019 from the terminal, a graphic interface will be launched by\ntyping \u2019figtree\u2019 then hit the return key.\n\n\n1\nfigtree\n\n\n\n\n\n\nView the tree by opening the file \u2019rep_set.tre\u2019 in the \u2019otus\u2019 folder (Desktop-\nTaxonomy-\notus). The tree that is produced is too complex to be of much use. We will look at a different tool, Megan 5, which produces a far more useful tree. In step 7 of the QIIME overview tutorial a file called otu_table.biom is generated.\n\n\nIt is in biom-format, which is increasingly supported by taxonomic software developers. One of the tools that support the BIOM format is Megan\n(\nhttp://ab.inf.uni-tuebingen.de/software/megan5/\n).\n\n\nMegan is a standalone tool for analyzing both taxonomic and functional content of datasets. It is free for academic use, but you will need to request a licence first. We will use Megan version 5 to display a taxonomic tree using the BIOM output we have just produced.\n\n\nNote: Sequence errors cangive rise to spurious OTUs, we can filter out OTUs that only contain a single sequence (singletons). QIIME allows you to do this quite easily, or you could also remove abundant taxa if you are more interested in rare taxa.\n\n\nTo remove singletons, run the following commands:\n\n\n1\n2\ncd otus\nfilter_otus_from_otu_table.py -i otu_table.biom -o otu_table_no_singletons.biom -n 2\n\n\n\n\n\n\nThis removes OTUs with less than 2 sequences. If you use the \u2013k option instead of the \u2013n option, OTUs with more than the specified number of sequences will be removed.\n\n\nMegan can be opened from the terminal by typing MEGAN. If you are asked for a licence select the following file /mnt/workshop/data/HT_MEGAN5_registration_for_academic_use.txt.\n\n\nFrom the File menu select Import -\n BIOM format.\nFind your biom file and import it.\n\n\nMegan will generate a tree that is far more informative than the one produced with FigTree. You can change the way Megan displays the data by clicking on the various icons and menu items. Please spend some time exploring your data.\n\n\nThe Word Cloud visualization is interesting, too, if you want to find out which samples are similar and which samples stand out.\n\n\nView OTU statistics\n\n\n\n\nYou can generate some statistics, e.g. the number of reads assigned,\ndistribution among samples. Some of the statistics are useful for\nfurther downstream analysis, e.g. beta-diversity analysis. Run the\nfollowing now, again from within the Taxonomy directory, and look at the\nresults. Write down the minimum value under Counts/sample summary. We\nneed it for beta-diversity analysis\n\n\n1\n2\n3\ncd ../\nbiom summarize-table -i otus/otu_table.biom -o otus/otu_table_summary.txt\nless otus/otu_table_summary.txt\n\n\n\n\n\n\nVisualize taxonomic composition\n\n\n\n\nWe will now group sequences by taxonomic assignment at various levels.\nThe following command produces a number of charts that can be viewed in\na browser. The command takes about 5 minutes to complete\n\n\n1\nsummarize_taxa_through_plots.py -i otus/otu_table.biom -o  wf_taxa_summary -m mapping.txt\n\n\n\n\n\n\nTo view the output, open a web browser from the Applications -\nInternet menu. You can use Google chrome, Firefox or Chromium. In Google chrome or Chromium, type CTRL-O, or in Firefox use the File menu to\nselect Desktop -\n;Taxonomy -\n; wf_taxa_summary -\n;\ntaxa_summary_plots and open either area_charts.html or\nbar_chars.html.\nI prefer the bar charts myself. The top chart visualizes taxonomic composition at phylum level for each of the\nsamples.\n\n\nThe next chart goes down to class level and following charts go another level up again. The charts (particularly the ones more at the\ntop) are very useful for discovering how the communities in your samples\ndiffer from each other. There is a similar plot in the paper, if you\nhave time, see how our analysis compares with the one described in the\npaper.\n\n\nAlpha diversity within samples and rarefaction curves\n\n\n\n\nAlpha diversity is the microbial diversity within a sample. QIIME can\ncalculate a lot of metrics, but for our tutorial, we generate 3 metrics\nfrom the alpha rarefaction workflow: chao1 (estimates species richness);\nobserved species metric (the count of unique OTUs); phylogenetic\ndistance. The following workflow generates rarefaction plots to\nvisualize alpha diversity.\n\n\nRun the following command from within your taxonomy directory, this\nshould take a few minutes:\n\n\n1\nalpha_rarefaction.py -i otus/otu_table.biom -m mapping.txt -o wf_arare -t otus/rep_set.tre\n\n\n\n\n\n\nFirst we are going to view the rarefaction curves in a web browser by\nopening\n/home/trainee/Desktop/Taxonomy/sutton/wf_arare/alpha_rarefaction_plots/rarefaction_plots.html.\nTo start select as metric \u2019chao1\u2019 and select as category \u2019Description\u2019.\nIt is clear that the microbial diversity in some samples is much higher\nthan in other samples. Click around in the legend as this will help you\nwork out which line corresponds with which sample. If you have time you\ncould try to correlate species richness with environmental data from the\npaper and establish whether our analysis confirms the findings of the\nauthors. Next view the precomputed rarefaction curves which show an\nincreased sequencing depth.\n\n\nIn general the more reads you have, the more OTUs you will observe. If a\nrarefaction curve start to flatten, it means that you have probably\nsequenced at sufficient depth, in other words, producing more reads will\nnot significantly add more OTUs. If on the other hand hasn\u2019t flattened,\nyou have not sampled enough to capture enough of the microbial diversity\nand by extrapolating the curve you may be able to estimate how many more\nreads you will need. Consult the QIIME overview tutorial for further\ninformation.\n\n\nRun the following command from within your taxonomy directory, this\nshould take a few minutes to generate a heatmap of the level three\ntaxonomy:\n\n\n1\nmake_otu_heatmap.py -i taxa_summary/otu_table_L3.biom -o taxa_summary/otu_table_L3_heatmap.pdf -c Treatment -m mapping.txt\n\n\n\n\n\n\nBeta diversity and beta diversity plots\n\n\n\n\nBefore we have a quick look at taxonomic analysis of shotgun data, we\nhave a quick look at beta diversity analysis, which is the assessment of\ndifferences between microbial communities. As we have already observed,\nour samples contain different numbers of sequences. The first step is to\nremove sample heterogeneity by randomly selecting the same number of\nreads from every sample. This number corresponds to the \u2019minimum\u2019 number\nrecorded when you looked at the OTU statistics.\n\n\nNow run the following command\n\n\n1\nbeta_diversity_through_plots.py -i otus/otu_table.biom -m mapping.txt -o bdiv_even -t otus/rep_set.tre -e 23183\n\n\n\n\n\n\nRead through the beta diversity compute section of the QIIME overview\ntutorial and try to understand this workflow. Tomorrow we will look at\nvisualization of beta diversity analysis results in more detail.\nUnfortunately we cannot view the PCoA plots that we have just generated\nusing the NeCTAR image as WebGL is not supported. Precomputed plots can\nbe viewed using the browser on your computer, we will make the link\navailable.\n\n\nClosed reference OTU picking of 16S ribosomal rRNA fragments selected from a shotgun data set\n\n\n\n\nIn a closed-reference OTU picking process, reads are clustered against a\nreference sequence collection and any reads, which do not hit a sequence\nin the reference sequence collection, are excluded from downstream\nanalyses. In QIIME, pick_closed_reference_otus.py is the primary\ninterface for closed-reference OTU picking in QIIME. If the user\nprovides taxonomic assignments for sequences in the reference database,\nthose are assigned to OTUs. We could use this approach to perform\ntaxonomic analysis on shotgun data. We need to perform the following\nsteps:\n\n\n\n\nExtract those reads from the data set that contain 16S ribosomal RNA\nsequence. If there are less than (e.g.) 100 nucleotides of rRNA\nsequence, the read should be discarded.\n\n\nRemove non-rRNA sequence (flanking regions) from those reads\n\n\nRun closed-reference OTU picking workflow\n\n\nVisualise the results, e.g. in Megan\n\n\n\n\nExtraction of 16S rRNA sequence-containing reads with rRNASelector\n\n\n\n\nWe will analyze an Illumina paired-end dataset that has been drastically\nreduced in size for this tutorial, while preserving the majority of the\n16S containing reads. The dataset is from the metagenome described at\n\nhttp://www.ncbi.nlm.nih.gov/pmc/articles/PMC3772140/\n. There is a pdf\nin the working directory for this part of the tutorial. This is a paired\nend dataset, and where read pairs overlapped, they were merged into a\nsingle sequence. If read pairs did not overlap, both reads were included\nin the analysis. QC was performed using the EBI Metagenomics pipeline.\nWe will use a tool called rRNASelector, which is freely available\n(\nhttp://www.ncbi.nlm.nih.gov/pubmed/21887657\n) to select our 16S rRNA\nsequence containing reads. The tool invokes hmmsearch and uses trained\nhidden Markov models to detect reads with 16S rRNA sequence. The tool\nalso trims the reads so that only 16S rRNA sequence is present in the\nfasta file we will feed into the QIIME workflow.\n\n\nFirst, we need to go to our working directory. You will find a file\ncalled A7A-paired.fasta containing the sequence reads. Fire up\nrRNASelector from the command line.\n\n\n1\n2\ncd ~/Desktop/Taxonomy/A7A/\nrRNASelector\n\n\n\n\n\n\nA graphical interface should appear. Note interaction with the interface\nmay have a few seconds lag. Load the sequence file by clicking on \u2019File\nChoose\u2019 at the top and navigate to the file A7A-paired.fasta. Select the\nfile and click \u2019Open\u2019. The tool will automatically fill in file names\nfor the result files. Change the Number of CPUs to \u20192\u2019, select\nProkaryote 16S (to include both bacterial and archaeal 16S sequences)\nand specify the location of the hmmsearch file by clicking the second\n\u2019File Choose\u2019 button. Type in manually the location\n\u2019/usr/bin/hmmsearch\u2019, then click process. The run should take a few\nminutes to complete.\n\n\nIf all went well, you can close rRNASelector by clicking on Exit. You\nwill have 3 new files in your directory, one containing untrimmed 16S\nreads, one containing trimmed 16S reads (A7A-paired.prok.16s.trim.fasta;\nthat\u2019s the one we want) and a file containing reads that do not contain\n(sufficient) 16S sequence.\n\n\nClosed-reference OTU picking workflow and visualization of results in Megan 5\n\n\n\n\nWe are now ready to pick our OTUs. We do that by running the following\ncommand (all on one line and no space after gg_otus-12-10):\n\n\n1\npick_closed_reference_otus.py -i A7A-paired.prok.16s.trim.fasta -o ./cr_uc -r /mnt/workshop/tools/qiime_software/gg_otus-12_10-release/rep_set/97_otus.fasta -t /mnt/workshop/tools/qiime_software/gg_otus-12_10-release/taxonomy/97_otu_taxonomy.txt\n\n\n\n\n\n\nWe need to specify the following options. The command will take several\nminutes to run. When finished open Megan as described before, import the\notu_table.biom file and explore the results.\n\n\n1\n2\n3\n4\n-i input_file.fasta\n-o output_directory\n-r /path/to/reference_sequences\n-t /path/to/reference_taxonomy\n\n\n\n\n\n\nBonus\n\n\n\n\nIf there is time left you could go back to the Polar bear study. The aim\nof this study was to understand interrelationship among microbial\ncommunity composition, in hibernating bears and effects on laboratory\nmice. With additional information from the paper, could you come up with\nsome conclusions?\n\n\nCheck the results from the ribosomal database classifier, how do these\ndiffer from the QIIME default GreenGenes (gg_otus_97) database\nclassifier results? What can you conclude?\n\n\nThe QIIME overview tutorial at\n(\nhttp://qiime.org/tutorials/tutorial.html\n) has a number of additional\nsteps that you may find interesting; so feel free to try some of them\nout. Note hat we have not installed Cytoscape, so we cannot visualize\nOTU networks.\n\n\nWe will end this tutorial with a 15-minute summary of what we have done\nand how well our analysis compares with the one in the paper.\n\n\nHopefully you will have acquired new skills that allow you to tackle\nyour own taxonomic analyses. There are many more tutorials on the QIIME\nwebsite that can help you pick the best strategy for your project\n(\nhttp://qiime.org/tutorials/\n). We picked QIIME for this tutorial as it\nis widely used and supported, but there are alternatives that might suit\nyour need better (e.g. VAMPS at \nhttp://vamps.mbl.ed\n; mothur at\n\nhttp://www.mothur.org\n and others).", 
            "title": "Metagenomics Taxonomic"
        }, 
        {
            "location": "/modules/metagenomics-module-tax/tax/#an-introduction-to-taxonomic-analysis-of-amplicon-and-shotgun-data-using-qiime", 
            "text": "", 
            "title": "An introduction to taxonomic analysis of amplicon and shotgun data using QIIME"
        }, 
        {
            "location": "/modules/metagenomics-module-tax/tax/#key-learning-outcomes", 
            "text": "After completing this practical the trainee should be able to:    Understand the open source software package QIIME for analysis    Perform a taxonomic analysis on a 16S rRNA amplicon dataset    Conduct 16S taxonomic analysis on shotgun data", 
            "title": "Key Learning Outcomes"
        }, 
        {
            "location": "/modules/metagenomics-module-tax/tax/#resources-youll-be-using", 
            "text": "", 
            "title": "Resources You\u2019ll be Using"
        }, 
        {
            "location": "/modules/metagenomics-module-tax/tax/#tools-used", 
            "text": "QIIME:  http://qiime.org/  rRNASelector:   http://www.ezbiocloud.net/sw/rrnaselector  MEGAN:  http://ab.inf.uni-tuebingen.de/software/megan5", 
            "title": "Tools Used"
        }, 
        {
            "location": "/modules/metagenomics-module-tax/tax/#useful-links", 
            "text": "FASTQ Encoding:   http://en.wikipedia.org/wiki/FASTQ_format#Encoding  Sommer data:   https://www.ebi.ac.uk/metagenomics/projects/ERP013648#samples_id", 
            "title": "Useful Links"
        }, 
        {
            "location": "/modules/metagenomics-module-tax/tax/#sources-of-data", 
            "text": "Felix Sommer et al. (2016). The Gut Microbiota Modulates Energy\n    Metabolism in the Hibernating Brown Bear Ursus arctos    Sutcliffe et al. (2013). Draft Genome Sequence of Thermotoga\n    maritima A7A Reconstructed from Metagenomic Sequencing Analysis of a\n    Hydrocarbon Reservoir in the Bass Strait, Australia. Genome Announc.\n    1(5): e00688-13.    Li et al. (2013). Draft Genome Sequence of Thermoanaerobacter sp.\n    Strain A7A, Reconstructed from a Metagenome Obtained from a\n    High-Temperature Hydrocarbon Reservoir in the Bass Strait,\n    Australia. Genome Announc. 1(5): e00701-13.    Lee et al. (2011). rRNASelector: a computer program for selecting\n    ribosomal RNA encoding sequences from metagenomic and\n    metatranscriptomic shotgun libraries. J. Microbiol. 49(4):689-691.", 
            "title": "Sources of Data"
        }, 
        {
            "location": "/modules/metagenomics-module-tax/tax/#introduction", 
            "text": "In this tutorial we will look at the open source software package QIIME\n(pronounced \u2019chime\u2019).  QIIME stands for Quantitative Insights Into Microbial Ecology. The\npackage contains many tools that enable users to analyse and compare\nmicrobial communities. QIIME was originally developed to analyse of\nRoche 454 amplicon sequencing data. In the latest versions workflows\nhave been added to analyze data from different sequencing platforms,\nsuch as Illumina, and different types of data, such as shotgun data. In\nthis course we will use QIIME 1.8, which is the latest version.  After completion of this tutorial, you should be able to perform a\ntaxonomic analysis on a Illumina pair end MiSeq 16S rRNA amplicon\ndataset. In addition you should be able to do 16S taxonomic analysis on\nshotgun data using the tool rRNASelector in combination with QIIME", 
            "title": "Introduction"
        }, 
        {
            "location": "/modules/metagenomics-module-tax/tax/#de-novo-otu-picking-and-diversity-analysis-using-illumina-data", 
            "text": "We will re-analyze the data from Sommer et al. (2016). The Gut\nMicrobiota Modulates Energy Metabolism in the Hibernating Brown Bear\nUrsus arctos. Sommer et al., 2016, Cell Reports 14, 1655\u20131661 February\n23, 2016. An electronic copy of the paper can be found in your Taxonomy\nfolder. When you have to wait a few minutes for commands to complete,\nuse the time to acquaint yourself with the study. It is a good example\nof a study that combines the power of next-generation sequencing with\nenvironmental observations/measurements.  The analysis we do follows the pipeline described in the QIIME general\ntutorial ( http://qiime.org/tutorials/tutorial.html ). Feel free to look\nat this tutorial for further background information. The dataset used in\nthe tutorial is a subset of the Sommer data. In some parts of the\nanalysis steps we have precomputed analysis on the complete data set for\ncomparison.", 
            "title": "De novo OTU picking and diversity analysis using Illumina data"
        }, 
        {
            "location": "/modules/metagenomics-module-tax/tax/#join-directory-of-pe-reads", 
            "text": "For this tutorial we will analyse the bear data dataset to complete the\nnecessary steps in a reasonable amount of time.  First we need to join the Illumina Pair End (PE) reads contained in the\nbear read directory.  1\n2 cd ~/taxonomy\nmultiple_join_paired_ends.py -i bear_reads -o bear_join --read1_indicator _1  --read2_indicator _2", 
            "title": "Join directory of PE reads"
        }, 
        {
            "location": "/modules/metagenomics-module-tax/tax/#the-mapping-file", 
            "text": "We will use the mapping file \u2019mapping.txt\u2019 to associate read data with\none of 23 samples. Note that this file has to be created for each\nanalysis, as the information is specific for an experiment. The mapping\nfile can also contain information on your experimental design. The\nformat is very strict; columns are separated with a single TAB\ncharacter; the header names have to be typed exactly as specified in the\ndocumentation. A good sample description is useful as it is used in the\nlegends of the figures QIIME generates. We could also specify the\nreverse primer and remove it from the reads. Unfortunately, the reverse\nprimer sequence was not in the paper, and we\u2019ll ignore it though we\ncould probably deduce it from longer reads: as a 466 bp region of the\n16S ribosomal RNA gene flanking the V3 and V4 regions was amplified,\nyou\u2019ll have a clue where to look for the reverse primer.  Execute the following command and test the mapping file for potential\nerrors:  1\n2 less -S mapping.txt\nvalidate_mapping_file.py -m mapping.txt -o mapping_output   There shouldn\u2019t be any errors. If there are errors, a corrected mapping\nfile will be written to the directory mapping_output", 
            "title": "The Mapping File"
        }, 
        {
            "location": "/modules/metagenomics-module-tax/tax/#assign-samples-to-the-reads", 
            "text": "This step sets up and quality filters the sample reads by sample\nidentities  Next we will assign reads to the samples and quality filter at phred\nquality threshold  = Q20 to joined reads for analysis  1 multiple_split_libraries_fastq.py -i bear_join -o bear_split -m sampleid_by_file --include_input_dir_path  -p parameter_file.txt   When completed please enter the directory \u2019bear_split\u2019 and have a look\nat the log file. It contains detailed information what was done during\nthe step we\u2019ve just performed. Note that the number of reads assigned to\nthe different samples varies considerably. Knowing where the individual\nsamples were taken may give a clue why this may be! Next have a quick\nlook at the file seqs.fna. What has changed to the header of the reads?  Look at the split libraries results  You will see the reads are now batched to their sample.  1\n2\n3\n4 cd split_library_output\nls -l\nless split_library_log.txt\nless -S seqs.fna", 
            "title": "Assign samples to the reads"
        }, 
        {
            "location": "/modules/metagenomics-module-tax/tax/#picking-operational-taxonomic-units-otus", 
            "text": "We will now use a workflow for de novo OTU picking, taxonomy assignment,\nphylogenetic tree construction, and OTU table construction QIIME has\nseveral workflows to pick OTUs, we will be using the one described in\nthe general overview tutorial\n( http://qiime.org/tutorials/tutorial.html ) It has 7 steps, which are\ndescribed in some detail in this tutorial. The described procedure is\nrun with the command from the Taxonomy directory. This step takes about\n12mins to run. Please read through the different steps\n( http://qiime.org/tutorials/tutorial.html ) and try to understand the\nprocedure. Remember that an OTU is not the same as a species, but a\n\u2019bag/cluster\u2019 of highly similar sequences (at least 97% is common for\nbacteria/archaea), or a single sequence in case of rare OTUs.  Pick de novo OTUs:  1 pick_de_novo_otus.py -i bear_split/seqs.fna -o otus   Please do spend some time looking at the output of this pipeline. In\nparticular the file \u2019seqs_rep_set_tax_assignments.txt\u2019 in the\n\u2019uclust_assigned_taxonomy\u2019 directory. By default QIIME uses the\nGreengenes 16S reference database to assign taxonomy.  It has the following levels: kingdom, phylum, class, order, family, genus, and species. It will be immediately clear that most reads cannot be classified up to species level. As described in step 6 of the QIIME overview tutorial, the pipeline creates a Newick-formatted phylogenetic\ntree (rep_set.tre) in the otus directory.  You can run the program \u2019figtree\u2019 from the terminal, a graphic interface will be launched by\ntyping \u2019figtree\u2019 then hit the return key.  1 figtree   View the tree by opening the file \u2019rep_set.tre\u2019 in the \u2019otus\u2019 folder (Desktop- Taxonomy- otus). The tree that is produced is too complex to be of much use. We will look at a different tool, Megan 5, which produces a far more useful tree. In step 7 of the QIIME overview tutorial a file called otu_table.biom is generated.  It is in biom-format, which is increasingly supported by taxonomic software developers. One of the tools that support the BIOM format is Megan\n( http://ab.inf.uni-tuebingen.de/software/megan5/ ).  Megan is a standalone tool for analyzing both taxonomic and functional content of datasets. It is free for academic use, but you will need to request a licence first. We will use Megan version 5 to display a taxonomic tree using the BIOM output we have just produced.  Note: Sequence errors cangive rise to spurious OTUs, we can filter out OTUs that only contain a single sequence (singletons). QIIME allows you to do this quite easily, or you could also remove abundant taxa if you are more interested in rare taxa.  To remove singletons, run the following commands:  1\n2 cd otus\nfilter_otus_from_otu_table.py -i otu_table.biom -o otu_table_no_singletons.biom -n 2   This removes OTUs with less than 2 sequences. If you use the \u2013k option instead of the \u2013n option, OTUs with more than the specified number of sequences will be removed.  Megan can be opened from the terminal by typing MEGAN. If you are asked for a licence select the following file /mnt/workshop/data/HT_MEGAN5_registration_for_academic_use.txt.  From the File menu select Import -  BIOM format.\nFind your biom file and import it.  Megan will generate a tree that is far more informative than the one produced with FigTree. You can change the way Megan displays the data by clicking on the various icons and menu items. Please spend some time exploring your data.  The Word Cloud visualization is interesting, too, if you want to find out which samples are similar and which samples stand out.", 
            "title": "Picking Operational Taxonomic Units (OTUs)"
        }, 
        {
            "location": "/modules/metagenomics-module-tax/tax/#view-otu-statistics", 
            "text": "You can generate some statistics, e.g. the number of reads assigned,\ndistribution among samples. Some of the statistics are useful for\nfurther downstream analysis, e.g. beta-diversity analysis. Run the\nfollowing now, again from within the Taxonomy directory, and look at the\nresults. Write down the minimum value under Counts/sample summary. We\nneed it for beta-diversity analysis  1\n2\n3 cd ../\nbiom summarize-table -i otus/otu_table.biom -o otus/otu_table_summary.txt\nless otus/otu_table_summary.txt", 
            "title": "View OTU statistics"
        }, 
        {
            "location": "/modules/metagenomics-module-tax/tax/#visualize-taxonomic-composition", 
            "text": "We will now group sequences by taxonomic assignment at various levels.\nThe following command produces a number of charts that can be viewed in\na browser. The command takes about 5 minutes to complete  1 summarize_taxa_through_plots.py -i otus/otu_table.biom -o  wf_taxa_summary -m mapping.txt   To view the output, open a web browser from the Applications - Internet menu. You can use Google chrome, Firefox or Chromium. In Google chrome or Chromium, type CTRL-O, or in Firefox use the File menu to\nselect Desktop - ;Taxonomy - ; wf_taxa_summary - ;\ntaxa_summary_plots and open either area_charts.html or\nbar_chars.html.\nI prefer the bar charts myself. The top chart visualizes taxonomic composition at phylum level for each of the\nsamples.  The next chart goes down to class level and following charts go another level up again. The charts (particularly the ones more at the\ntop) are very useful for discovering how the communities in your samples\ndiffer from each other. There is a similar plot in the paper, if you\nhave time, see how our analysis compares with the one described in the\npaper.", 
            "title": "Visualize taxonomic composition"
        }, 
        {
            "location": "/modules/metagenomics-module-tax/tax/#alpha-diversity-within-samples-and-rarefaction-curves", 
            "text": "Alpha diversity is the microbial diversity within a sample. QIIME can\ncalculate a lot of metrics, but for our tutorial, we generate 3 metrics\nfrom the alpha rarefaction workflow: chao1 (estimates species richness);\nobserved species metric (the count of unique OTUs); phylogenetic\ndistance. The following workflow generates rarefaction plots to\nvisualize alpha diversity.  Run the following command from within your taxonomy directory, this\nshould take a few minutes:  1 alpha_rarefaction.py -i otus/otu_table.biom -m mapping.txt -o wf_arare -t otus/rep_set.tre   First we are going to view the rarefaction curves in a web browser by\nopening\n/home/trainee/Desktop/Taxonomy/sutton/wf_arare/alpha_rarefaction_plots/rarefaction_plots.html.\nTo start select as metric \u2019chao1\u2019 and select as category \u2019Description\u2019.\nIt is clear that the microbial diversity in some samples is much higher\nthan in other samples. Click around in the legend as this will help you\nwork out which line corresponds with which sample. If you have time you\ncould try to correlate species richness with environmental data from the\npaper and establish whether our analysis confirms the findings of the\nauthors. Next view the precomputed rarefaction curves which show an\nincreased sequencing depth.  In general the more reads you have, the more OTUs you will observe. If a\nrarefaction curve start to flatten, it means that you have probably\nsequenced at sufficient depth, in other words, producing more reads will\nnot significantly add more OTUs. If on the other hand hasn\u2019t flattened,\nyou have not sampled enough to capture enough of the microbial diversity\nand by extrapolating the curve you may be able to estimate how many more\nreads you will need. Consult the QIIME overview tutorial for further\ninformation.  Run the following command from within your taxonomy directory, this\nshould take a few minutes to generate a heatmap of the level three\ntaxonomy:  1 make_otu_heatmap.py -i taxa_summary/otu_table_L3.biom -o taxa_summary/otu_table_L3_heatmap.pdf -c Treatment -m mapping.txt", 
            "title": "Alpha diversity within samples and rarefaction curves"
        }, 
        {
            "location": "/modules/metagenomics-module-tax/tax/#beta-diversity-and-beta-diversity-plots", 
            "text": "Before we have a quick look at taxonomic analysis of shotgun data, we\nhave a quick look at beta diversity analysis, which is the assessment of\ndifferences between microbial communities. As we have already observed,\nour samples contain different numbers of sequences. The first step is to\nremove sample heterogeneity by randomly selecting the same number of\nreads from every sample. This number corresponds to the \u2019minimum\u2019 number\nrecorded when you looked at the OTU statistics.  Now run the following command  1 beta_diversity_through_plots.py -i otus/otu_table.biom -m mapping.txt -o bdiv_even -t otus/rep_set.tre -e 23183   Read through the beta diversity compute section of the QIIME overview\ntutorial and try to understand this workflow. Tomorrow we will look at\nvisualization of beta diversity analysis results in more detail.\nUnfortunately we cannot view the PCoA plots that we have just generated\nusing the NeCTAR image as WebGL is not supported. Precomputed plots can\nbe viewed using the browser on your computer, we will make the link\navailable.", 
            "title": "Beta diversity and beta diversity plots"
        }, 
        {
            "location": "/modules/metagenomics-module-tax/tax/#closed-reference-otu-picking-of-16s-ribosomal-rrna-fragments-selected-from-a-shotgun-data-set", 
            "text": "In a closed-reference OTU picking process, reads are clustered against a\nreference sequence collection and any reads, which do not hit a sequence\nin the reference sequence collection, are excluded from downstream\nanalyses. In QIIME, pick_closed_reference_otus.py is the primary\ninterface for closed-reference OTU picking in QIIME. If the user\nprovides taxonomic assignments for sequences in the reference database,\nthose are assigned to OTUs. We could use this approach to perform\ntaxonomic analysis on shotgun data. We need to perform the following\nsteps:   Extract those reads from the data set that contain 16S ribosomal RNA\nsequence. If there are less than (e.g.) 100 nucleotides of rRNA\nsequence, the read should be discarded.  Remove non-rRNA sequence (flanking regions) from those reads  Run closed-reference OTU picking workflow  Visualise the results, e.g. in Megan", 
            "title": "Closed reference OTU picking of 16S ribosomal rRNA fragments selected from a shotgun data set"
        }, 
        {
            "location": "/modules/metagenomics-module-tax/tax/#extraction-of-16s-rrna-sequence-containing-reads-with-rrnaselector", 
            "text": "We will analyze an Illumina paired-end dataset that has been drastically\nreduced in size for this tutorial, while preserving the majority of the\n16S containing reads. The dataset is from the metagenome described at http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3772140/ . There is a pdf\nin the working directory for this part of the tutorial. This is a paired\nend dataset, and where read pairs overlapped, they were merged into a\nsingle sequence. If read pairs did not overlap, both reads were included\nin the analysis. QC was performed using the EBI Metagenomics pipeline.\nWe will use a tool called rRNASelector, which is freely available\n( http://www.ncbi.nlm.nih.gov/pubmed/21887657 ) to select our 16S rRNA\nsequence containing reads. The tool invokes hmmsearch and uses trained\nhidden Markov models to detect reads with 16S rRNA sequence. The tool\nalso trims the reads so that only 16S rRNA sequence is present in the\nfasta file we will feed into the QIIME workflow.  First, we need to go to our working directory. You will find a file\ncalled A7A-paired.fasta containing the sequence reads. Fire up\nrRNASelector from the command line.  1\n2 cd ~/Desktop/Taxonomy/A7A/\nrRNASelector   A graphical interface should appear. Note interaction with the interface\nmay have a few seconds lag. Load the sequence file by clicking on \u2019File\nChoose\u2019 at the top and navigate to the file A7A-paired.fasta. Select the\nfile and click \u2019Open\u2019. The tool will automatically fill in file names\nfor the result files. Change the Number of CPUs to \u20192\u2019, select\nProkaryote 16S (to include both bacterial and archaeal 16S sequences)\nand specify the location of the hmmsearch file by clicking the second\n\u2019File Choose\u2019 button. Type in manually the location\n\u2019/usr/bin/hmmsearch\u2019, then click process. The run should take a few\nminutes to complete.  If all went well, you can close rRNASelector by clicking on Exit. You\nwill have 3 new files in your directory, one containing untrimmed 16S\nreads, one containing trimmed 16S reads (A7A-paired.prok.16s.trim.fasta;\nthat\u2019s the one we want) and a file containing reads that do not contain\n(sufficient) 16S sequence.", 
            "title": "Extraction of 16S rRNA sequence-containing reads with rRNASelector"
        }, 
        {
            "location": "/modules/metagenomics-module-tax/tax/#closed-reference-otu-picking-workflow-and-visualization-of-results-in-megan-5", 
            "text": "We are now ready to pick our OTUs. We do that by running the following\ncommand (all on one line and no space after gg_otus-12-10):  1 pick_closed_reference_otus.py -i A7A-paired.prok.16s.trim.fasta -o ./cr_uc -r /mnt/workshop/tools/qiime_software/gg_otus-12_10-release/rep_set/97_otus.fasta -t /mnt/workshop/tools/qiime_software/gg_otus-12_10-release/taxonomy/97_otu_taxonomy.txt   We need to specify the following options. The command will take several\nminutes to run. When finished open Megan as described before, import the\notu_table.biom file and explore the results.  1\n2\n3\n4 -i input_file.fasta\n-o output_directory\n-r /path/to/reference_sequences\n-t /path/to/reference_taxonomy", 
            "title": "Closed-reference OTU picking workflow and visualization of results in Megan 5"
        }, 
        {
            "location": "/modules/metagenomics-module-tax/tax/#bonus", 
            "text": "If there is time left you could go back to the Polar bear study. The aim\nof this study was to understand interrelationship among microbial\ncommunity composition, in hibernating bears and effects on laboratory\nmice. With additional information from the paper, could you come up with\nsome conclusions?  Check the results from the ribosomal database classifier, how do these\ndiffer from the QIIME default GreenGenes (gg_otus_97) database\nclassifier results? What can you conclude?  The QIIME overview tutorial at\n( http://qiime.org/tutorials/tutorial.html ) has a number of additional\nsteps that you may find interesting; so feel free to try some of them\nout. Note hat we have not installed Cytoscape, so we cannot visualize\nOTU networks.  We will end this tutorial with a 15-minute summary of what we have done\nand how well our analysis compares with the one in the paper.  Hopefully you will have acquired new skills that allow you to tackle\nyour own taxonomic analyses. There are many more tutorials on the QIIME\nwebsite that can help you pick the best strategy for your project\n( http://qiime.org/tutorials/ ). We picked QIIME for this tutorial as it\nis widely used and supported, but there are alternatives that might suit\nyour need better (e.g. VAMPS at  http://vamps.mbl.ed ; mothur at http://www.mothur.org  and others).", 
            "title": "Bonus"
        }, 
        {
            "location": "/modules/metagenomics-module-vis/vis/", 
            "text": "Analysis Visualisation\n\n\nKey Learning Outcomes\n\n\n\n\nAfter completing this module the trainee should be able to:\n\n\n\n\n\n\nVisualise between sample comparisons, using Emperor PCA\n\n\n\n\n\n\nUnderstand the difference between weighted and unweighted analysis\n\n\n\n\n\n\nResources You\u2019ll be Using\n\n\n\n\nTools Used\n\n\nEmperor: \n\n\nhttp://qiime.org/emperor/tutorial_index.html\n\n\nData sets\n\n\n\n\nSutton et al. (2013). Impact of Long-Term Diesel Contamination on Soil Microbial Community Structure: \n\n\nhttp://www.ncbi.nlm.nih.gov/pmc/articles/PMC3553749/pdf/zam619.pdf\n\n\nFierer et al. (2010). Forensic identification using skin bacterial communities: \n\n\nhttp://www.ncbi.nlm.nih.gov/pmc/articles/PMC2852011/\n\n\nIntroduction\n\n\n\n\nGood data quality and sample metadata is important for visualising\nmetagenomics analysis.\n\n\nFor this tutorial we are using Emperor a browser enabled scatter plot\nvisualisation tool. We will be using the Sutton and Fierer data sets for\nviewing the principal components analysis (PCoA) from the QIIME Beta\ndiversity analysis (16S).\n\n\nBeta Diversity computation and plots\n\n\n\n\nBeta diversity represents between sample comparisons based on their\ncomposition. The output of these comparisons is a square matrix where a\n\u201cdistance\u201d or dissimilarity is calculated between every pair of\ncommunity samples, reflecting the dissimilarity between those samples.\nThe data distance matrix can be then visualized with analyses such as\nPCoA and hierarchical clustering. PCoA is a technique that maps the\nsamples in the distance matrix to a set of axes that show the maximum\namount of variation explained. The principal coordinates can be plotted\nin two or three dimensions to provide an intuitive visualization of the\ndata structure to look at differences between the samples, and look for\nsimilarities by sample conditions.\n\n\nThe Beta diversity workflow you ran earlier contained a number of steps:\n\n\n\n\n\n\nRarifying the OTU table to remove sample heterogeneity. A rarified\n    OTU table should be used so that artificial diversity induced due to\n    different sampling effort is removed.\n\n\n\n\n\n\nCalculating the Beta diversity, the unifrac weighted and unweighted\n    generated principal coorodinates are created here by default.\n\n\n\n\n\n\nThe following are the required options and inputs for Beta diversity\nanalysis:\n\n\n1\n2\n3\n4\n5\n1.  Option -i, OTU table (*.biom)\n2.  Option \u2013t, phylogeny tree (*.tre) from pick_de_novo_otus.py\n3.  Option \u2013m, the user-defined mapping file\n4.  Option \u2013o, the output directory\n5.  Option \u2013e, the number of sequences per sample (sequencing depth)\n\n\n\n\n\n\nYou do not need to run the following command, it may overwrite the\npre-computed analysis. This is the command you ran on the Sutton sub\ndataset.\n\n\n1\nbeta_diversity_through_plots.py -i otus/otu_table.biom -m mapping.txt -o wf_bdiv_even122/ -t otus/rep_set.tre -e 122\n\n\n\n\n\n\nPrepare the environment\n\n\nThe data for this practical can be found in the Taxonomy directory on\nyour desktop. Go to the pre-computed Beta diversity analysis in the\nTaxonomy directory.\n\n\nOpen the Terminal and go to where the data is stored. Investigate the\ndirectories available.\n\n\n1\n2\ncd ~/Desktop/Taxonomy/sutton_full_denoised/\nls \u2013lhtr\n\n\n\n\n\n\nOpen the following files: wf_bdiv_even394/unweighted_unifrac_pc.txt\nand wf_bdiv_even394/weighted_unifrac_pc.txt using the \u2018less\u2019\ncommand. The option \u2013S with \u2018less\u2019 allows you to view files with\nunwrapped lines, to escape type q (for quit). To scroll left and right,\nuse the arrow keys.\n\n\n1\n2\nless \u2013S wf_bdiv_even394/unweighted_unifrac_pc.txt\nless \u2013S wf_bdiv_even394/weighted_unifrac_pc.txt\n\n\n\n\n\n\nIn each file every sample is listed in the first column, and the\nsubsequent columns contain the value for the sample against the noted\nprincipal coordinate. At the bottom of each Principal Coordinate column,\nyou will find the eigen value and percent of variation explained by the\ncoordinate. Note the major difference between weighted and unweighted\nanalysis is the inclusion of OTU abundance when calculating distances\nbetween communities. You should use weighted if the biological question\nyou are trying to ask takes OTU abundance of your groups into\nconsideration. If some samples are forming groups with weighted, then\nlikely the larger or smaller abundances of several OTUs are the primary\ndriving force in PCoA space, but when all OTUs are considered at equal\nabundance, these differences are lost (unweighted).\n\n\nNote discrete vs. continuous only has to do with coloring of the points,\nand not the calculation of UniFrac distances.\n\n\nEmperor\n\n\nWe can now run Emperor to prepare the PCoA plots for visualisation.\n\n\n1\n2\n3\nmake_emperor.py -i wf_bdiv_even394/unweighted_unifrac_pc.txt -m sutton_mapping_file.txt \u2013b \nSource,Condition,Source\nCondition\n \u2013o emporer_output_unweighted\n\nls \u2013l emporer_output_unweighted\n\n\n\n\n\n\nRun the last make_emporer.py command with the weighted_unifrac, using\nthe same command. Make sure you rename the output directory to\n\u201cemporer_output_weighted\u201c. Look in the Emporer output directories and\nnote the index.html file outputs.\n\n\nTo view the index.html files created from the \u2018make_emperor.py\u2019 steps\nabove a browser like Firefox or Chrome can be used.\n\n\nImportant - To view the Emporer plots we need to switch from the VM to\nyour local machine. Open the Firefox browser and view the pre-computed\nEmporer directories available at the following URL.\n\nhttp://www.ebi.ac.uk/~sterk/emperor/\n\n\nView the unweighted analysis on the Sutton sub data set:\n\n\nhttp://www.ebi.ac.uk/~sterk/emperor/sutton_subset_emporer_output_unweighted/\n\n\nCheck the \u2018Colors\u2019 options to view your samples. What can you determine\nfrom the PCoA plot?\n\n\nTo change the colouring scheme click the \u2018Colors\u2019 tab. The available\nmetadata categories are sorted in alphabetical order. Extra information\non the features can be found at\n\nhttp://qiime.org/emperor/description_index.html\n Try changing the\nscaling, visibility, label etc. to improve the display. Note that\nEmperor generates publication quality figures in scalable vector\ngraphics (SVG) format.\n\n\nNow look at the weighted unifrac analysis,\n\nhttp://www.ebi.ac.uk/~sterk/emperor/sutton_subset_emporer_output_weighted/\n\n\nYou can compare the sub data analysis against the full-denoised dataset.\n\n\n\nWhat are the main differences between the weighted and unweighted plots?\n\n\nDid you notice any differences between the analysis on the sub data set\nand the full denoised?\n\n\nWhat are your final conclusions on the Sutton dataset?\n\n\nWhat did each of the visualisation methods describe?\n\n\nTaxonomy summary plots:\n\n\nAlpha diversity plots:\n\n\nBeta diversity PCoA plots:\n\n\nNow that you are familiar with Emperor we will next view the forensic\nanalysis of the Fierer et. al. data. This experiment matches the\nbacteria on surfaces to the skin-associated bacteria of the individual\nwho touched the surface. This study shows that skin-associated bacteria\ncan be readily recovered from surfaces (including single computer keys\nand computer mice) and that the structure of these communities can be\nused to differentiate objects handled by different individuals, even if\nthose objects have been left untouched for up to 2 weeks at room\ntemperature.\n\n\nPlease look the information contained in the mapping file and identify\nthose fields that can be plotted.\n\n\n1\n2\ncd ~/Desktop/Taxonomy/fierer/\nless \u2013S fierer_mapping_file.txt\n\n\n\n\n\n\nList any fields that can be plotted for this study. What is the\nremainder of the information for?\n\n\nRun Emperor to prepare the PCoA plots for visualisation\n\n\n1\nmake_emperor.py -i fierer_unweighted_unifrac_pc.txt -m fierer_mapping_file.txt -b \\ \nHOST_SUBJECT_ID,ENV_FEATURE,HOST_SUBJECT_ID\nENV_FEATURE\u201d -o emporer_output_unweighted\n\n\n\n\n\n\nGo to\n\nhttp://www.ebi.ac.uk/~sterk/emperor/fierer_emporer_output_unweighted/\n\n\nWhat did you observe about the clustering?\n\n\nWhat fields have been selected?\n\n\nThe on-line Qiime tutorials are very good and growing in numbers. It is\nhighly recommended checking out in future the many analysis options.", 
            "title": "Metagenomics Visualisation"
        }, 
        {
            "location": "/modules/metagenomics-module-vis/vis/#analysis-visualisation", 
            "text": "", 
            "title": "Analysis Visualisation"
        }, 
        {
            "location": "/modules/metagenomics-module-vis/vis/#key-learning-outcomes", 
            "text": "After completing this module the trainee should be able to:    Visualise between sample comparisons, using Emperor PCA    Understand the difference between weighted and unweighted analysis", 
            "title": "Key Learning Outcomes"
        }, 
        {
            "location": "/modules/metagenomics-module-vis/vis/#resources-youll-be-using", 
            "text": "", 
            "title": "Resources You\u2019ll be Using"
        }, 
        {
            "location": "/modules/metagenomics-module-vis/vis/#tools-used", 
            "text": "Emperor:   http://qiime.org/emperor/tutorial_index.html", 
            "title": "Tools Used"
        }, 
        {
            "location": "/modules/metagenomics-module-vis/vis/#data-sets", 
            "text": "Sutton et al. (2013). Impact of Long-Term Diesel Contamination on Soil Microbial Community Structure:   http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3553749/pdf/zam619.pdf  Fierer et al. (2010). Forensic identification using skin bacterial communities:   http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2852011/", 
            "title": "Data sets"
        }, 
        {
            "location": "/modules/metagenomics-module-vis/vis/#introduction", 
            "text": "Good data quality and sample metadata is important for visualising\nmetagenomics analysis.  For this tutorial we are using Emperor a browser enabled scatter plot\nvisualisation tool. We will be using the Sutton and Fierer data sets for\nviewing the principal components analysis (PCoA) from the QIIME Beta\ndiversity analysis (16S).", 
            "title": "Introduction"
        }, 
        {
            "location": "/modules/metagenomics-module-vis/vis/#beta-diversity-computation-and-plots", 
            "text": "Beta diversity represents between sample comparisons based on their\ncomposition. The output of these comparisons is a square matrix where a\n\u201cdistance\u201d or dissimilarity is calculated between every pair of\ncommunity samples, reflecting the dissimilarity between those samples.\nThe data distance matrix can be then visualized with analyses such as\nPCoA and hierarchical clustering. PCoA is a technique that maps the\nsamples in the distance matrix to a set of axes that show the maximum\namount of variation explained. The principal coordinates can be plotted\nin two or three dimensions to provide an intuitive visualization of the\ndata structure to look at differences between the samples, and look for\nsimilarities by sample conditions.  The Beta diversity workflow you ran earlier contained a number of steps:    Rarifying the OTU table to remove sample heterogeneity. A rarified\n    OTU table should be used so that artificial diversity induced due to\n    different sampling effort is removed.    Calculating the Beta diversity, the unifrac weighted and unweighted\n    generated principal coorodinates are created here by default.    The following are the required options and inputs for Beta diversity\nanalysis:  1\n2\n3\n4\n5 1.  Option -i, OTU table (*.biom)\n2.  Option \u2013t, phylogeny tree (*.tre) from pick_de_novo_otus.py\n3.  Option \u2013m, the user-defined mapping file\n4.  Option \u2013o, the output directory\n5.  Option \u2013e, the number of sequences per sample (sequencing depth)   You do not need to run the following command, it may overwrite the\npre-computed analysis. This is the command you ran on the Sutton sub\ndataset.  1 beta_diversity_through_plots.py -i otus/otu_table.biom -m mapping.txt -o wf_bdiv_even122/ -t otus/rep_set.tre -e 122", 
            "title": "Beta Diversity computation and plots"
        }, 
        {
            "location": "/modules/metagenomics-module-vis/vis/#prepare-the-environment", 
            "text": "The data for this practical can be found in the Taxonomy directory on\nyour desktop. Go to the pre-computed Beta diversity analysis in the\nTaxonomy directory.  Open the Terminal and go to where the data is stored. Investigate the\ndirectories available.  1\n2 cd ~/Desktop/Taxonomy/sutton_full_denoised/\nls \u2013lhtr   Open the following files: wf_bdiv_even394/unweighted_unifrac_pc.txt\nand wf_bdiv_even394/weighted_unifrac_pc.txt using the \u2018less\u2019\ncommand. The option \u2013S with \u2018less\u2019 allows you to view files with\nunwrapped lines, to escape type q (for quit). To scroll left and right,\nuse the arrow keys.  1\n2 less \u2013S wf_bdiv_even394/unweighted_unifrac_pc.txt\nless \u2013S wf_bdiv_even394/weighted_unifrac_pc.txt   In each file every sample is listed in the first column, and the\nsubsequent columns contain the value for the sample against the noted\nprincipal coordinate. At the bottom of each Principal Coordinate column,\nyou will find the eigen value and percent of variation explained by the\ncoordinate. Note the major difference between weighted and unweighted\nanalysis is the inclusion of OTU abundance when calculating distances\nbetween communities. You should use weighted if the biological question\nyou are trying to ask takes OTU abundance of your groups into\nconsideration. If some samples are forming groups with weighted, then\nlikely the larger or smaller abundances of several OTUs are the primary\ndriving force in PCoA space, but when all OTUs are considered at equal\nabundance, these differences are lost (unweighted).  Note discrete vs. continuous only has to do with coloring of the points,\nand not the calculation of UniFrac distances.", 
            "title": "Prepare the environment"
        }, 
        {
            "location": "/modules/metagenomics-module-vis/vis/#emperor", 
            "text": "We can now run Emperor to prepare the PCoA plots for visualisation.  1\n2\n3 make_emperor.py -i wf_bdiv_even394/unweighted_unifrac_pc.txt -m sutton_mapping_file.txt \u2013b  Source,Condition,Source Condition  \u2013o emporer_output_unweighted\n\nls \u2013l emporer_output_unweighted   Run the last make_emporer.py command with the weighted_unifrac, using\nthe same command. Make sure you rename the output directory to\n\u201cemporer_output_weighted\u201c. Look in the Emporer output directories and\nnote the index.html file outputs.  To view the index.html files created from the \u2018make_emperor.py\u2019 steps\nabove a browser like Firefox or Chrome can be used.  Important - To view the Emporer plots we need to switch from the VM to\nyour local machine. Open the Firefox browser and view the pre-computed\nEmporer directories available at the following URL. http://www.ebi.ac.uk/~sterk/emperor/  View the unweighted analysis on the Sutton sub data set:  http://www.ebi.ac.uk/~sterk/emperor/sutton_subset_emporer_output_unweighted/  Check the \u2018Colors\u2019 options to view your samples. What can you determine\nfrom the PCoA plot?  To change the colouring scheme click the \u2018Colors\u2019 tab. The available\nmetadata categories are sorted in alphabetical order. Extra information\non the features can be found at http://qiime.org/emperor/description_index.html  Try changing the\nscaling, visibility, label etc. to improve the display. Note that\nEmperor generates publication quality figures in scalable vector\ngraphics (SVG) format.  Now look at the weighted unifrac analysis, http://www.ebi.ac.uk/~sterk/emperor/sutton_subset_emporer_output_weighted/  You can compare the sub data analysis against the full-denoised dataset.  What are the main differences between the weighted and unweighted plots?  Did you notice any differences between the analysis on the sub data set\nand the full denoised?  What are your final conclusions on the Sutton dataset?  What did each of the visualisation methods describe?  Taxonomy summary plots:  Alpha diversity plots:  Beta diversity PCoA plots:  Now that you are familiar with Emperor we will next view the forensic\nanalysis of the Fierer et. al. data. This experiment matches the\nbacteria on surfaces to the skin-associated bacteria of the individual\nwho touched the surface. This study shows that skin-associated bacteria\ncan be readily recovered from surfaces (including single computer keys\nand computer mice) and that the structure of these communities can be\nused to differentiate objects handled by different individuals, even if\nthose objects have been left untouched for up to 2 weeks at room\ntemperature.  Please look the information contained in the mapping file and identify\nthose fields that can be plotted.  1\n2 cd ~/Desktop/Taxonomy/fierer/\nless \u2013S fierer_mapping_file.txt   List any fields that can be plotted for this study. What is the\nremainder of the information for?  Run Emperor to prepare the PCoA plots for visualisation  1 make_emperor.py -i fierer_unweighted_unifrac_pc.txt -m fierer_mapping_file.txt -b \\  HOST_SUBJECT_ID,ENV_FEATURE,HOST_SUBJECT_ID ENV_FEATURE\u201d -o emporer_output_unweighted   Go to http://www.ebi.ac.uk/~sterk/emperor/fierer_emporer_output_unweighted/  What did you observe about the clustering?  What fields have been selected?  The on-line Qiime tutorials are very good and growing in numbers. It is\nhighly recommended checking out in future the many analysis options.", 
            "title": "Emperor"
        }, 
        {
            "location": "/modules/metagenomics-module-wga/wga/", 
            "text": "Whole Genome Analysis\n\n\nKey Learning Outcomes\n\n\n\n\nAfter completing this module the trainee should be able to:\n\n\n\n\n\n\nUnderstand the main approaches to perform metagenomics assembly\n\n\n\n\n\n\nBe able to perform assembly on your data and assess the quality of your assembly\n\n\n\n\n\n\nResources You\u2019ll be Using\n\n\n\n\nTools Used\n\n\nMeta-Velvet: \n\n\nhttps://github.com/hacchy/MetaVelvet\n\n\nUseful Links\n\n\n\n\nMeta-Velvet: \n\n\nhttp://metavelvet.dna.bio.keio.ac.jp/\n\n\nIntroduction\n\n\n\n\nPerforming genomic assembly aims at generating a genome-length sequence\nusing the sequence information obtained from short reads. In the case of\nmetagenomics sample, the task is complicated by the number of different\ngenomes present in the sample and the fact that their sequences are\nsometimes very similar to each other. There are two main approaches to\nperform de novo assembly (genomic or metagenomic): building a consensus\nand generating De Bruijn k-mer graph.\n\n\nThe k-mers represent the nodes of the de Bruijn graph. Nodes are linked\ntogether if they overlap by k-1 nucleotides. Determining the correct\nk-mer is important. You can use tools such as Velvet Advisor:\n\nhttp://dna.med.monash.edu.au/~torsten/velvet_advisor/\n\n\nBuilding a de Bruijn graph is computationally demanding but navigation\nthrough it to identify path (to generate contigs of continuous\nsequences) is quick and memory efficient. Ideally other information,\nbiological or distance-based, would be used to help build the contigs.\n\n\nAssessing the quality of an assembly\n\n\nFor genomic assembly, the accepted criterion of assembly quality is the\nnumber of contigs obtained: the lower this number, the longer the\ncontigs and therefore the higher genome reconstitution. This number is\noften expressed as N50, which is defined as the weighted median such\nthat 50% of the entire assembly is contained in contigs equal to or\nlarger than this value. It is calculated by ranking the contigs by\ndecreasing length and adding their size sequentially until 50% of the\ntotal number of nucleotides is reached: the N50 is defined by the number\nof contigs included in this sum. The N50 is also generally used for\nmetagenomics\n\n\nPractical\n\n\n\n\nThe purpose of this exercise is to perform an assembly using Meta-Velvet\nand illustrate how k-mer choices influence the output quality. The\nstarting dataset will be a metagenomic dataset. To ensure better\nassembly, rather than using the raw reads, we will only assemble the\nsequences having passed the EMG QC steps. Meta-Velvet is an extension of\nVelvet, a popular genomic assembler. Therefore to perform our assembly,\nwe will first run Velvet and then Meta-Velvet using the Velvet output as\ninput. Both programs are run from the command line.\n\n\nInvestigation of the input sequence file\n\n\nOpen a terminal window (Applications/Accessories/Terminal, a link is\nalso provided on your desktop) and navigate to the \u201cdata\u201d folder in\n\u201cAssemblyTutorial\u201d and look at the first lines of the sequence file. The\nfile is in fasta format and contains sequences of at least 100 nt each\n\n\n1\n2\n3\n4\n5\n6\ncd ~/Desktop/AssemblyTutorial/data\nhead A7A_processed.fasta\n# What is the total number of sequences?\ngrep -c \n A7A_processed.fasta\n# What is the total number of nucleotides?\n~/AssemblyTutorial/stats A7A_processed.fasta\n\n\n\n\n\n\nThe output indicates that the input file contains about 2.2 billion\n(2,164,714,530) nucleotides distributed among \u00a0 21 million (20,975,212)\nsequences. The average sequence length was 103.2 nucleotides. The\nsequence file also contains 12,942 \u201cN\u201d indicating that some sequences\nhave ambiguous bases. In addition, the script displays the N50 to N100\nvalues: N50 = 101, n = 10256045 for example means that a cumulated sum\nof, at least, half of the total nucleotides is reached after adding the\nlength of 10,256,045 sequences and that the last sequence added had a\nlength of 101 nucletotides.\n\n\nPerforming the assembly using Velvet and Meta-Velvet\n\n\nVelvet and Meta-Velvet had already been installed on your computer.\nHowever they need to be configured by indicating the k-mer value and the\nnumber of read categories to use. We already have seen what k-mer is\n(reminder in page 5 above). The number of read categories is the maximum\nnumber of libraries of different insert lengths. As in our case the\nreads are all coming from the same library, we will use the default\nvalue (2). The version of Velvet and MetaVelvet installed on the virtual\nmachine you will be using as already been configured with k-mer = 59.\n\n\nWe will run the applications from the AssemblyTutorial folder. The\nsoftware has been installed in your path so no need to copy/link these\nfiles:\n\n\n1\n2\n# first run velveth to generate the k-mers:\nvelveth A7A-59 59 -fasta data/A7A_processed.fasta\n\n\n\n\n\n\nThen run velvetg to construct the de Bruijn graph. The \u201c- exp_cov auto\u201d\nparameter indicates to the software that the sequence coverage is\nconsidered uniform across the submitted set and that the expected\ncoverage (i.e. number of reads per sequence) should be the median\ncoverage value:\n\n\n1\nvelvetg A7A-59 \u2013exp_cov auto\n\n\n\n\n\n\nFinally run meta-velvetg to generate the assembly output in the A7A-59\ndirectory:\n\n\n1\nmeta-velvetg A7A-59 | tee logfile\n\n\n\n\n\n\nAssessing the quality of the assembly\n\n\nThe main assembly output is a list of contigs provided as a fasta file.\nWe will know look at these in more details. First we need to navigate to\nthe output directory:\n\n\nFinally run meta-velvetg to generate the assembly output in the A7A-59\ndirectory. We can obtain the number of contigs by running the function\ngrep to only count the lines containing the contig names (identified by\n\u201c\n\u201d). Then run the stats script, seen earlier, to also obtain the N50\nvalue:\n\n\n1\n2\n3\ncd A7A-59\ngrep -c \n meta-velvetg.contigs.fa\n~/AssemblyTutorial/stats meta-velvetg.contigs.fa\n\n\n\n\n\n\nIt shows that the sequences had been assembled in 9,182 contigs of an\naverage length equal to about 1,230 nucleotides. The longest contigs\ncontains 95,305 nucleotides. The N50 line indicates that half of the\nnucleotides are comprised in the first 275 longest contigs and that the\n275\nth\n contigs is 9,145 nucleotides long. Comparing these stats to the\none obtained before assembly also reveal that the number of nucleotides\ninvolved in the assembly represents only slightly more than 0.5% of the\nnucleotides submitted. This reflects, of course, the amount of\noverlapping sequences identified by Velvet and MetaVelvet. This also\nexplains the reduction of the number of ambiguous base (N_count).\n\n\nChanging the k-mer value can have a dramatic effect on the quality of\nthe assembly. Reducing the k-mer to 31 for example yields the following\nstatistics:\n\n\n1\n2\n3\n4\n5\n6\n7\n8\nsum = 15527668, n = 42343, ave = 366.71, largest = 93835\nN50 = 1208, n = 2151\nN60 = 649, n = 3903\nN70 = 328, n = 7388\nN80 = 193, n = 13589\nN90 = 115, n = 24202\nN100 = 61, n = 42343\nN_count = 263\n\n\n\n\n\n\nThe number of contigs is almost 5 times larger than with k-mer equal 59.\nHowever, increasing the k-mer alone does not systematically lead to\nbetter stats. A k-mer of 63 produces an assembly with higher number of\ncontigs (Note that the N50 value is also increased):\n\n\n1\n2\n3\n4\n5\n6\n7\n8\nsum = 11579884, n = 10062, ave = 1150.85, largest = 80291\nN50 = 7440, n = 365\nN60 = 4524, n = 563\nN70 = 2251, n = 921\nN80 = 827, n = 1795\nN90 = 326, n = 4155\nN100 = 125, n = 10062\nN_count = 1850\n\n\n\n\n\n\nWithout extra information, it could be challenging, using the N50\nparameter, to judge the quality of a metagenomics assembly. We could use\nblast or other tools to infer taxonomy to different sections of the\ncontigs: obtaining similar affiliation for all fragments originating\nfrom the same contigs would be indicative of a good assembly.", 
            "title": "Metagenomics WGA"
        }, 
        {
            "location": "/modules/metagenomics-module-wga/wga/#whole-genome-analysis", 
            "text": "", 
            "title": "Whole Genome Analysis"
        }, 
        {
            "location": "/modules/metagenomics-module-wga/wga/#key-learning-outcomes", 
            "text": "After completing this module the trainee should be able to:    Understand the main approaches to perform metagenomics assembly    Be able to perform assembly on your data and assess the quality of your assembly", 
            "title": "Key Learning Outcomes"
        }, 
        {
            "location": "/modules/metagenomics-module-wga/wga/#resources-youll-be-using", 
            "text": "", 
            "title": "Resources You\u2019ll be Using"
        }, 
        {
            "location": "/modules/metagenomics-module-wga/wga/#tools-used", 
            "text": "Meta-Velvet:   https://github.com/hacchy/MetaVelvet", 
            "title": "Tools Used"
        }, 
        {
            "location": "/modules/metagenomics-module-wga/wga/#useful-links", 
            "text": "Meta-Velvet:   http://metavelvet.dna.bio.keio.ac.jp/", 
            "title": "Useful Links"
        }, 
        {
            "location": "/modules/metagenomics-module-wga/wga/#introduction", 
            "text": "Performing genomic assembly aims at generating a genome-length sequence\nusing the sequence information obtained from short reads. In the case of\nmetagenomics sample, the task is complicated by the number of different\ngenomes present in the sample and the fact that their sequences are\nsometimes very similar to each other. There are two main approaches to\nperform de novo assembly (genomic or metagenomic): building a consensus\nand generating De Bruijn k-mer graph.  The k-mers represent the nodes of the de Bruijn graph. Nodes are linked\ntogether if they overlap by k-1 nucleotides. Determining the correct\nk-mer is important. You can use tools such as Velvet Advisor: http://dna.med.monash.edu.au/~torsten/velvet_advisor/  Building a de Bruijn graph is computationally demanding but navigation\nthrough it to identify path (to generate contigs of continuous\nsequences) is quick and memory efficient. Ideally other information,\nbiological or distance-based, would be used to help build the contigs.", 
            "title": "Introduction"
        }, 
        {
            "location": "/modules/metagenomics-module-wga/wga/#assessing-the-quality-of-an-assembly", 
            "text": "For genomic assembly, the accepted criterion of assembly quality is the\nnumber of contigs obtained: the lower this number, the longer the\ncontigs and therefore the higher genome reconstitution. This number is\noften expressed as N50, which is defined as the weighted median such\nthat 50% of the entire assembly is contained in contigs equal to or\nlarger than this value. It is calculated by ranking the contigs by\ndecreasing length and adding their size sequentially until 50% of the\ntotal number of nucleotides is reached: the N50 is defined by the number\nof contigs included in this sum. The N50 is also generally used for\nmetagenomics", 
            "title": "Assessing the quality of an assembly"
        }, 
        {
            "location": "/modules/metagenomics-module-wga/wga/#practical", 
            "text": "The purpose of this exercise is to perform an assembly using Meta-Velvet\nand illustrate how k-mer choices influence the output quality. The\nstarting dataset will be a metagenomic dataset. To ensure better\nassembly, rather than using the raw reads, we will only assemble the\nsequences having passed the EMG QC steps. Meta-Velvet is an extension of\nVelvet, a popular genomic assembler. Therefore to perform our assembly,\nwe will first run Velvet and then Meta-Velvet using the Velvet output as\ninput. Both programs are run from the command line.", 
            "title": "Practical"
        }, 
        {
            "location": "/modules/metagenomics-module-wga/wga/#investigation-of-the-input-sequence-file", 
            "text": "Open a terminal window (Applications/Accessories/Terminal, a link is\nalso provided on your desktop) and navigate to the \u201cdata\u201d folder in\n\u201cAssemblyTutorial\u201d and look at the first lines of the sequence file. The\nfile is in fasta format and contains sequences of at least 100 nt each  1\n2\n3\n4\n5\n6 cd ~/Desktop/AssemblyTutorial/data\nhead A7A_processed.fasta\n# What is the total number of sequences?\ngrep -c   A7A_processed.fasta\n# What is the total number of nucleotides?\n~/AssemblyTutorial/stats A7A_processed.fasta   The output indicates that the input file contains about 2.2 billion\n(2,164,714,530) nucleotides distributed among \u00a0 21 million (20,975,212)\nsequences. The average sequence length was 103.2 nucleotides. The\nsequence file also contains 12,942 \u201cN\u201d indicating that some sequences\nhave ambiguous bases. In addition, the script displays the N50 to N100\nvalues: N50 = 101, n = 10256045 for example means that a cumulated sum\nof, at least, half of the total nucleotides is reached after adding the\nlength of 10,256,045 sequences and that the last sequence added had a\nlength of 101 nucletotides.", 
            "title": "Investigation of the input sequence file"
        }, 
        {
            "location": "/modules/metagenomics-module-wga/wga/#performing-the-assembly-using-velvet-and-meta-velvet", 
            "text": "Velvet and Meta-Velvet had already been installed on your computer.\nHowever they need to be configured by indicating the k-mer value and the\nnumber of read categories to use. We already have seen what k-mer is\n(reminder in page 5 above). The number of read categories is the maximum\nnumber of libraries of different insert lengths. As in our case the\nreads are all coming from the same library, we will use the default\nvalue (2). The version of Velvet and MetaVelvet installed on the virtual\nmachine you will be using as already been configured with k-mer = 59.  We will run the applications from the AssemblyTutorial folder. The\nsoftware has been installed in your path so no need to copy/link these\nfiles:  1\n2 # first run velveth to generate the k-mers:\nvelveth A7A-59 59 -fasta data/A7A_processed.fasta   Then run velvetg to construct the de Bruijn graph. The \u201c- exp_cov auto\u201d\nparameter indicates to the software that the sequence coverage is\nconsidered uniform across the submitted set and that the expected\ncoverage (i.e. number of reads per sequence) should be the median\ncoverage value:  1 velvetg A7A-59 \u2013exp_cov auto   Finally run meta-velvetg to generate the assembly output in the A7A-59\ndirectory:  1 meta-velvetg A7A-59 | tee logfile", 
            "title": "Performing the assembly using Velvet and Meta-Velvet"
        }, 
        {
            "location": "/modules/metagenomics-module-wga/wga/#assessing-the-quality-of-the-assembly", 
            "text": "The main assembly output is a list of contigs provided as a fasta file.\nWe will know look at these in more details. First we need to navigate to\nthe output directory:  Finally run meta-velvetg to generate the assembly output in the A7A-59\ndirectory. We can obtain the number of contigs by running the function\ngrep to only count the lines containing the contig names (identified by\n\u201c \u201d). Then run the stats script, seen earlier, to also obtain the N50\nvalue:  1\n2\n3 cd A7A-59\ngrep -c   meta-velvetg.contigs.fa\n~/AssemblyTutorial/stats meta-velvetg.contigs.fa   It shows that the sequences had been assembled in 9,182 contigs of an\naverage length equal to about 1,230 nucleotides. The longest contigs\ncontains 95,305 nucleotides. The N50 line indicates that half of the\nnucleotides are comprised in the first 275 longest contigs and that the\n275 th  contigs is 9,145 nucleotides long. Comparing these stats to the\none obtained before assembly also reveal that the number of nucleotides\ninvolved in the assembly represents only slightly more than 0.5% of the\nnucleotides submitted. This reflects, of course, the amount of\noverlapping sequences identified by Velvet and MetaVelvet. This also\nexplains the reduction of the number of ambiguous base (N_count).  Changing the k-mer value can have a dramatic effect on the quality of\nthe assembly. Reducing the k-mer to 31 for example yields the following\nstatistics:  1\n2\n3\n4\n5\n6\n7\n8 sum = 15527668, n = 42343, ave = 366.71, largest = 93835\nN50 = 1208, n = 2151\nN60 = 649, n = 3903\nN70 = 328, n = 7388\nN80 = 193, n = 13589\nN90 = 115, n = 24202\nN100 = 61, n = 42343\nN_count = 263   The number of contigs is almost 5 times larger than with k-mer equal 59.\nHowever, increasing the k-mer alone does not systematically lead to\nbetter stats. A k-mer of 63 produces an assembly with higher number of\ncontigs (Note that the N50 value is also increased):  1\n2\n3\n4\n5\n6\n7\n8 sum = 11579884, n = 10062, ave = 1150.85, largest = 80291\nN50 = 7440, n = 365\nN60 = 4524, n = 563\nN70 = 2251, n = 921\nN80 = 827, n = 1795\nN90 = 326, n = 4155\nN100 = 125, n = 10062\nN_count = 1850   Without extra information, it could be challenging, using the N50\nparameter, to judge the quality of a metagenomics assembly. We could use\nblast or other tools to infer taxonomy to different sections of the\ncontigs: obtaining similar affiliation for all fragments originating\nfrom the same contigs would be indicative of a good assembly.", 
            "title": "Assessing the quality of the assembly"
        }, 
        {
            "location": "/modules/metagenomics-module-fda/fda/", 
            "text": "Functional Data Analysis\n\n\nKey Learning Outcomes\n\n\n\n\nAfter completing this module the trainee should be able to:\n\n\n\n\n\n\nUnderstand how EMG provides functional analysis of metagenomic data sets\n\n\n\n\n\n\nKnow where to find and how to interpret analysis results for samples on the EMG website\n\n\n\n\n\n\nKnow how to download the raw sample data and analysis results for use with 3\nrd\n party visualisation and statistical analysis packages\n\n\n\n\n\n\nResources You\u2019ll be Using\n\n\n\n\nTools Used\n\n\nSTAMP :\n\n\nhttp://kiwi.cs.dal.ca/Software/STAMP\n\n\nUseful Links\n\n\n\n\nEBI Metagenomics resource (EMG) : \n\n\nwww.ebi.ac.uk/metagenomics/\n\n\nIntroduction\n\n\n\n\nThe EBI Metagenomics resource (EMG) provides functional analysis of\npredicted coding sequences (pCDS) from metagenomic data sets using the\nInterPro database. InterPro is a sequence analysis resource that\npredicts protein family membership, along with the presence of important\ndomains and sites. It does this by combining predictive models known as\nprotein signatures from a number of different databases into a single\nsearchable resource. InterPro curators manually integrate the different\nsignatures, providing names and descriptive abstracts and, whenever\npossible, adding Gene Ontology (GO) terms. Links are also provided to\npathway databases, such as KEGG, MetaCyc and Reactome, and to structural\nresources, such as SCOP, CATH and PDB.\n\n\nWhat are protein signatures?\n\n\nProtein signatures are obtained by modelling the conservation of amino\nacids at specific positions within a group of related proteins (i.e., a\nprotein family), or within the domains/sites shared by a group of\nproteins. InterPro\u2019s different member databases use different\ncomputational methods to produce protein signatures, and they each have\ntheir own particular focus of interest: structural and/or functional\ndomains, protein families, or protein features, such as active sites or\nbinding sites (see Figure 1).\n\n\nFigures\n\n\n\n\nOnly a subset of the InterPro member databases are used by EMG: Gene3D,\nTIGRFAMs, Pfam, PRINTS and PROSITE patterns. These databases were\nselected since, together, they provide both high coverage and offer\ndetailed functional analysis, and have underlying algorithms that can\ncope with the vast amounts of fragmentary sequence data found in\nmetagenomic datasets.\n\n\nAssigning functional information to metagenomic sequences\n\n\nWhilst InterPro matches to metagenomic sequence sets are informative in\ntheir own right, EMG offers an additional type of analysis in the form\nof Gene Ontology (GO) terms. The Gene Ontology is made up of 3\nstructured controlled vocabularies that describe gene products in terms\nof their associated biological processes, cellular components and\nmolecular functions in a species-independent manner. By using GO terms,\nscientists working on different species or using different databases can\ncompare datasets, since they have a precisely defined name and meaning\nfor a particular concept. Terms in the Gene Ontology are ordered into\nhierarchies, with less specific terms towards the top and more specific\nterms towards the bottom (see Figure 2).\n\n\nAn example of GO terms organised into a hierarchy, with terms becoming less specific as the hierarchy is ascended (e.g., alpha-tubulin binding\nis a type of cytoskeletal binding, which is a type of protein binding).\nNote that a GO term can have more than one parent term. The Gene\nOntology also allows for different types of relationships between terms\n(such as \u2018has part of\u2019 or \u2018regulates\u2019). The EMG analysis pipeline only\nuses the straightforward \u2018is a\u2019\nrelationships.\n\n\n\n\nMore information about the GO project can be found\n\nhttp://www.geneontology.org/GO.doc.shtml\n\n\nAs part of the EMG analysis pipeline, GO terms for molecular function,\nbiological process and cellular component are associated to pCDS in a\nsample via the InterPro2GO mapping service. This works as follows:\nInterPro entries are given GO terms by curators if the terms can be\naccurately applied to all of the proteins matching that entry. Sequences\nsearched against InterPro are then associated with GO terms by virtue of\nthe entries they match - a protein that matches one InterPro entry with\nthe GO term \u2018kinase activity\u2019 and another InterPro entry with the GO\nterm \u2018zinc ion binding\u2019 will be annotated with both GO terms.\n\n\nFinding functional information about samples on the EMG website\n\n\nFunctional analysis of samples within projects on the EMG website\n\nwww.ebi.ac.uk/metagenomics/\n can be\naccessed by clicking on the Functional Analysis tab found toward the top\nof any sample page (see Figure 3 below).\n\n\nA Functional analysis tab can be found towards the top of each sample .page\n\n\n\n\nClicking on this tab brings up a page displaying sequence features (the\nnumber of reads with pCDS, the number of pCDS with InterPro matches,\netc), InterPro match information and GO term annotation for the sample,\nas shown in Figure 4 and 5 below.\n\n\nFunctional analysis of metagenomics data, as shown on the EMG website.\n\n\nInterPro match information for the predicted coding sequences in the sample is shown. The number of InterPro matches are displayed graphically, and as a table that has a text search facility.\n\n\n\n\nThe GO terms predicted for the sample are displayed. Different\ngraphical representations are available, and can be selected by clicking\non the \u2018Switch view\u2019\nicons.\n\n\n\n\nThe Gene Ontology terms displayed graphically on the web site have been\n\u2018slimmed\u2019 with a special GO slim developed for metagenomic data sets. GO\nslims are cut-down versions of the Gene Ontology, containing a subset of\nthe terms in the whole GO. They give a broad overview of the ontology\ncontent without the detail of the specific fine-grained terms.\n\n\nThe full data sets used to generate both the InterPro and GO overview\ncharts, along with a host of additional data (processed reads, pCDS,\nreads encoding 16S rRNAs, taxonomic analyses, etc) can be downloaded for\nfurther analysis by clicking the Download tab, found towards the top of\nthe page (see Figure 6).\n\n\nEach sample has a download tab, where the full set of sequences,\nanalyses, summaries and raw data can be\ndownloaded.\n\n\n\n\nPractical\n\n\n\n\nPart 1 \u2013 Browsing analysed data via the EMG website\n\n\nOpen the Metagenomics Portal homepage in a web browser\n\nhttps://www.ebi.ac.uk/metagenomics/\n.\n\n\nFrom the Projects list click the \u2018Projects\u2019 tab, or \u2018View all projects\u2019,\nfind and click on HOT station, Central North Pacific Gyre, ALOH. You\nshould now have a Project overview page describing the project, related\npublications, and links to the samples that the project contains.\n\n\nQuestion 1: What publications are associated with this study?\n\n\nScroll down to Associated Samples and open the HOT Station ALOHA, 25 m\nsample. You should now have a Sample Overview page, describing various\nmeta-data associated with the sample, such as the geographic location\nfrom which it was isolated, its collection date, and so on.\n\n\nQuestion 2: What is the latitude, longitude and depth at which the\nsample was collected?\n\n\nQuestion 3: What geographic location does this correspond to?\n\n\nQuestion 4: What environmental ontology (ENVO) identifer and name has\nthe sample material been annotated with?\n\n\nClick on the \u2018Download\u2019 tab. Right click the file labelled \u2018Predicted\nCDS (FASTA)\u2019 link, and save this file to your desktop. Find the file,\nand either double click on it to open it, or examine it using \u2018less\u2019 by\ntyping the following commands in a terminal window:\n\n\n1\n2\ncd ~/Desktop\nless HOT_Station_ALOHA,_25m_depth_pCDS.faa\n\n\n\n\n\n\nHave a look at one or two of the many sequences it contains. We will\nlook at the analysis results for this entire batch of sequences,\ndisplayed on the EMG website, in a moment. First, we will attempt\nanalyse just one of the predicted coding sequences using InterPro (the\nanalysis results on the EMG website summarise these kind of results for\ntens or hundreds of thousands of sequences).\n\n\nIn a new tab or window, open your web browser and navigate to\n\nhttp://www.ebi.ac.uk/interpro/\n. Copy and paste the following sequence\ninto the text box on the InterPro home page where it says \u2018Analyse your\nsequence\u2019:\n\n\n1\n2\nSRR010898.122503_1_115_+\nENNQEIKIIRNYINEFNLTGFIVGIPLDEKGQMTNQAI\n\n\n\n\n\n\nPress Search and wait for your results. Your sequence will be run\nthrough the InterProScan analysis software, which attempts to match it\nagainst all of the different signatures in the InterProScan database.\n\n\nQuestion 5: What feature does InterProScan predict your sequence to\ncontain?\n\n\nClicking on the InterPro entry name or IPR accession number will take\nyou to the InterPro entry page for your result, where more information\ncan be found.\n\n\nQuestion 6: What GO terms is the feature associated with?\n\n\nReturn to the sample page for HOT station, Central North Pacific Gyre,\nALOH 25 m.\n\n\nNow we are going to look at the functional analysis results for all of\nthe pCDS in the sample. First, we will find the number of sequences that\nmade it through to the functional analysis section of the pipeline.\n\n\nClick on the Quality control tab. This page displays a series of charts,\nshowing how many sequences passed each quality control step, how many\nreads were left after clustering, and so on.\n\n\nQuestion 7: After all of the quality filtering steps are complete, how\nmany reads were submitted for analysis by the pipeline?\n\n\nNext, we will look at the results of the functional predictions for the\npCDS. These can be found under the Functional analysis tab.\n\n\nClick on the Functional analysis tab and examine the InterPro match\nsection. The top part of this page shows a sequence feature summary,\nshowing the number of reads with predicted coding sequences (pCDS), the\nnumber of pCDS with InterPro matches, etc.\n\n\nQuestion 8: How many of the reads that passed the quality control stage\nhad predicted coding sequences (pCDS)?\n\n\nQuestion 9: How many pCDS have InterProScan hits?\n\n\nScroll down the page to the InterPro match summary section\n\n\nQuestion 10: How many different InterPro entries were matched by the\npCDS?\n\n\nQuestion 11: Why is this figure different to the number of pCDS that\nhave InterProScan hits?\n\n\nNext we will examine the GO terms predicted by InterPro for the pCDS in\nthe sample.\n\n\nScroll down to the GO term annotation section of the page and examine\nthe 3 bar charts, which correspond to the 3 different components of the\nGene Ontology.\n\n\nQuestion 12: What are the top 3 biological process terms predicted for\nthe pCDS from the sample?\n\n\nSelecting the pie chart representation of GO terms makes it easier to\nvisualise the data to find the answer.\n\n\nNow we will look at the taxonomic analysis for this sample.\n\n\nClick on the Taxonomy Analysis tab and examine the phylum composition\ngraph and table.\n\n\nQuestion 13: How many different OTUs were found in this sample?\n\n\nQuestion 14: What are the top 3 phyla in the sample?\n\n\nSelect the Krona chart view of the data icon. This brings up an\ninteractive chart that can be used to analyse data at different\ntaxonomic ranks.\n\n\nQuestion 15: What proportion of cyanobacteria in this sample are made up\nof Synechococcaceae?\n\n\nNow we will compare these analyses with those for a sample taken at 500\nm from the same geographical location.\n\n\nIn a new tab or window, find and open the HOT station, Central North\nPacific Gyre, ALOH project page again. Select the sample HOT Station\nALOHA, 500m and examine the information under the Functional analysis\ntabs. Question 16: How many pCDS were in this sample? Question 17: How\nmany of the pCDSs have an InterPro match? Question 18: How many\ndifferent InterPro entries are matched by this sample? Question 19: Are\nthese figures broadly comparable to the ones for the 25 m sample?\n\n\nScroll to the bottom of the page and examine the GO term annotation for\nthe day 500 m sample.\n\n\nQuestion 20: Are there visible differences between the GO terms for this\nsample and the 25 m sample? Could there be any biological explanation\nfor this?\n\n\nSelecting the bar chart representation of GO terms makes it easier to\ncompare different data sets.\n\n\nReturn to the HOT station project page and open the taxonomy analysis\nresults for all 4 of the samples, each in a new window.\n\n\nQuestion 21: How does the taxonomic composition change at different\nocean sampling depths? Are any trends in the data consistent with your\nanswer to question 18?\n\n\nPart 2 - Analysing EMG data using STAMP\n\n\nWhilst EMG does not currently support direct comparison of multiple\nsamples, it is possible to download the underlying data for use with\nother visualisation and/or statistical analysis tools, such as STAMP\n(Statistical Analysis of Metagenomic Profiles:\n\nhttp://kiwi.cs.dal.ca/Software/STAMP\n. The Downloads tab for a given\nsample lists the files with the necessary data, although a slight amount\nof data wrangling is usually required to reformat files, etc.\n\n\nWe are going perform a detailed comparison of the GO slimmed biological\nprocess results for the samples at 25 m and 500 m using STAMP. To do\nthis, open the HOT station, Central North Pacific Gyre, ALOH project\npage again. Select the sample HOT Station ALOHA, 25 m study and download\nthe GO slim annotation (CSV) file from the download tab. Repeat this for\nthe HOT Station ALOHA, 500 m study.\n\n\nYou should now have the following 2 files in your downloads folder:\nHOT_Station_ALOHA,_25m_depth_GO_slim.csv\nHOT_Station_ALOHA,_500m_depth,_MG_GO_slim.csv.txt (don\u2019t worry\nabout the fact the file names are constructed slightly differently \u2013\nthis is an inconsistency on our web site that needs to be corrected!)\n\n\nDrag or copy these files to your desktop, and then launch a terminal\nwindow, so that you can manipulate the files on the command line.\n\n\nNavigate to the Desktop directory by typing \u2018cd ~/Desktop/\u2019 Next, type\n\u2018ls\u2019 to make sure you can see the files, and take a look at their\ncontents using the \u2018less\u2019 command.\n\n\nFirst we are going to wrangle data from the 25 m sample, using a mixture\nof \u2018grep\u2019 to pull out the lines containing the phrase\n\u2018biological_process\u2019 and \u2018awk\u2019 to split up and reorder the fields.\n\n\nawk is a text-processing language that is useful for extracting or\ntransforming documents and reformatting text. You can find out more\nabout it here: \nhttp://www.gnu.org/software/gawk/manual/gawk.html\n\n\nEnter the following command (all on one line):\n\n\n1\ngrep \nbiological_process\n  HOT_Station_ALOHA,_25m_depth_GO_slim.csv  | awk -F\n \n{print \\$6 \n\\t\n \\$4 \n\\t\n \\$8 }\n  \n  biological_process_25m.txt\n\n\n\n\n\n\nThis should produce a new file called \u2018biological_process_26m.txt\u2019.\nUse \u2018less\u2019 to examine this file.\n\n\nQuestion 22: What are the differences between this file and the\nHOT_Station_ALOHA,_25m_depth_GO_slim.csv file?\n\n\nNext, we are going to add the biological process data from the 500 m\nsample to this file. Once again, will use \u2018grep\u2019 to pull out the\nrelevant lines in the source file, \u2018awk\u2019 to process the lines and output\nthe correct fields, and \u2018paste\u2019 to merge the data with the contents of\nthe file we produced above.\n\n\nEnter the following command (all on one line):\n\n\n1\ngrep \nbiological_process\n HOT_Station_ALOHA,_500m_depth,_MG_GO_slim.csv | awk -F\n{}\n \n{print \\$8 }\n | paste biological_process_25m.txt - \n biological_process_25mvs500m.txt\n\n\n\n\n\n\nThis should create another new file, this time called\n\u2018biological_process_25mvs500m.txt\u2019. Use \u2018less\u2019 to examine this file.\n\n\nQuestion 23: What is the difference between this file and the\nbiological_process_25m.txt file?\n\n\nFinally, we need to add a simple header to the file, so that STAMP can\nprocess it properly. To do this, we could open the file and add the\nheader manually, but here we will do it on the command line using the\n\u2018printf\u2019 command to write the header, \u2018cat\u2019 to print the contents of the\nfile, and directing the output to a new file with the \u2018\n\u2019 operator.\n\n\nEnter the following command (all on one line):\n\n\n1\nprintf \nClassification\\tSubclassification\\t25m\\t500m\\n\n | cat - biological_process_25mvs500m.txt \n biological_process_25mvs500m.spf\n\n\n\n\n\n\nThis command should have produced a third file called\n\u2018biological_process_25mvs500m.spf\u2019 that STAMP can read.\n\n\nTo launch STAMP, type STAMP in the terminal, hit the enter/return key, a\ngraphical interface will then appear. Load the\nbiological_process_25mvs500m.spf file into STAMP (you will need to set\n\u2018Profile level\u2019 to \u2018Subclassification\u2019 and select \u2018Two samples\u2019 from the\ntabs on the left hand side of the page to view the file once it is\nloaded). Try experimenting with different plot types and statistical\ntests.\n\n\nQuestion 24: Are there any observable differences in biological process\nGO terms between the 2 samples?\n\n\nFor your convenience, three files containing the HOT Station GO term\nresults for all 3 branches of the Gene Ontology (molecular function,\nbiological process and cellular component) at each sample depth, and a\ncorresponding metadata file have been pre-generated and formatted for\nuse with STAMP.\n\n\nThese are located in the /Desktop/InterproTutorial/\nfolder on the desktop (filenames: HOT_Station_ALOHA_bp.spf,\nHOT_Station_ALOHA_mf.spf, HOT_Station_ALOHA_cc.spf and\nHOT_Station_ALOHA_metadata.tsv).\n\n\nTry loading these files into STAMP, as you did with the previous file,\nand exploring the results. You can select different samples to compare\n(25, 75, 125 and 500 m) using the sample 1 and sample 2 drop down menus.\nYou can now also use the Multiple Groups tab and explore some of its\noptions.\n\n\nQuestion 25: What trends can you identify in the molecular function,\nbiological process and cellular component GO terms across the different\nsampling depths?", 
            "title": "Metagenomics Functional Analysis"
        }, 
        {
            "location": "/modules/metagenomics-module-fda/fda/#functional-data-analysis", 
            "text": "", 
            "title": "Functional Data Analysis"
        }, 
        {
            "location": "/modules/metagenomics-module-fda/fda/#key-learning-outcomes", 
            "text": "After completing this module the trainee should be able to:    Understand how EMG provides functional analysis of metagenomic data sets    Know where to find and how to interpret analysis results for samples on the EMG website    Know how to download the raw sample data and analysis results for use with 3 rd  party visualisation and statistical analysis packages", 
            "title": "Key Learning Outcomes"
        }, 
        {
            "location": "/modules/metagenomics-module-fda/fda/#resources-youll-be-using", 
            "text": "", 
            "title": "Resources You\u2019ll be Using"
        }, 
        {
            "location": "/modules/metagenomics-module-fda/fda/#tools-used", 
            "text": "STAMP :  http://kiwi.cs.dal.ca/Software/STAMP", 
            "title": "Tools Used"
        }, 
        {
            "location": "/modules/metagenomics-module-fda/fda/#useful-links", 
            "text": "EBI Metagenomics resource (EMG) :   www.ebi.ac.uk/metagenomics/", 
            "title": "Useful Links"
        }, 
        {
            "location": "/modules/metagenomics-module-fda/fda/#introduction", 
            "text": "The EBI Metagenomics resource (EMG) provides functional analysis of\npredicted coding sequences (pCDS) from metagenomic data sets using the\nInterPro database. InterPro is a sequence analysis resource that\npredicts protein family membership, along with the presence of important\ndomains and sites. It does this by combining predictive models known as\nprotein signatures from a number of different databases into a single\nsearchable resource. InterPro curators manually integrate the different\nsignatures, providing names and descriptive abstracts and, whenever\npossible, adding Gene Ontology (GO) terms. Links are also provided to\npathway databases, such as KEGG, MetaCyc and Reactome, and to structural\nresources, such as SCOP, CATH and PDB.", 
            "title": "Introduction"
        }, 
        {
            "location": "/modules/metagenomics-module-fda/fda/#what-are-protein-signatures", 
            "text": "Protein signatures are obtained by modelling the conservation of amino\nacids at specific positions within a group of related proteins (i.e., a\nprotein family), or within the domains/sites shared by a group of\nproteins. InterPro\u2019s different member databases use different\ncomputational methods to produce protein signatures, and they each have\ntheir own particular focus of interest: structural and/or functional\ndomains, protein families, or protein features, such as active sites or\nbinding sites (see Figure 1).", 
            "title": "What are protein signatures?"
        }, 
        {
            "location": "/modules/metagenomics-module-fda/fda/#figures", 
            "text": "Only a subset of the InterPro member databases are used by EMG: Gene3D,\nTIGRFAMs, Pfam, PRINTS and PROSITE patterns. These databases were\nselected since, together, they provide both high coverage and offer\ndetailed functional analysis, and have underlying algorithms that can\ncope with the vast amounts of fragmentary sequence data found in\nmetagenomic datasets.", 
            "title": "Figures"
        }, 
        {
            "location": "/modules/metagenomics-module-fda/fda/#assigning-functional-information-to-metagenomic-sequences", 
            "text": "Whilst InterPro matches to metagenomic sequence sets are informative in\ntheir own right, EMG offers an additional type of analysis in the form\nof Gene Ontology (GO) terms. The Gene Ontology is made up of 3\nstructured controlled vocabularies that describe gene products in terms\nof their associated biological processes, cellular components and\nmolecular functions in a species-independent manner. By using GO terms,\nscientists working on different species or using different databases can\ncompare datasets, since they have a precisely defined name and meaning\nfor a particular concept. Terms in the Gene Ontology are ordered into\nhierarchies, with less specific terms towards the top and more specific\nterms towards the bottom (see Figure 2).  An example of GO terms organised into a hierarchy, with terms becoming less specific as the hierarchy is ascended (e.g., alpha-tubulin binding\nis a type of cytoskeletal binding, which is a type of protein binding).\nNote that a GO term can have more than one parent term. The Gene\nOntology also allows for different types of relationships between terms\n(such as \u2018has part of\u2019 or \u2018regulates\u2019). The EMG analysis pipeline only\nuses the straightforward \u2018is a\u2019\nrelationships.   More information about the GO project can be found http://www.geneontology.org/GO.doc.shtml  As part of the EMG analysis pipeline, GO terms for molecular function,\nbiological process and cellular component are associated to pCDS in a\nsample via the InterPro2GO mapping service. This works as follows:\nInterPro entries are given GO terms by curators if the terms can be\naccurately applied to all of the proteins matching that entry. Sequences\nsearched against InterPro are then associated with GO terms by virtue of\nthe entries they match - a protein that matches one InterPro entry with\nthe GO term \u2018kinase activity\u2019 and another InterPro entry with the GO\nterm \u2018zinc ion binding\u2019 will be annotated with both GO terms.", 
            "title": "Assigning functional information to metagenomic sequences"
        }, 
        {
            "location": "/modules/metagenomics-module-fda/fda/#finding-functional-information-about-samples-on-the-emg-website", 
            "text": "Functional analysis of samples within projects on the EMG website www.ebi.ac.uk/metagenomics/  can be\naccessed by clicking on the Functional Analysis tab found toward the top\nof any sample page (see Figure 3 below).  A Functional analysis tab can be found towards the top of each sample .page   Clicking on this tab brings up a page displaying sequence features (the\nnumber of reads with pCDS, the number of pCDS with InterPro matches,\netc), InterPro match information and GO term annotation for the sample,\nas shown in Figure 4 and 5 below.  Functional analysis of metagenomics data, as shown on the EMG website.  InterPro match information for the predicted coding sequences in the sample is shown. The number of InterPro matches are displayed graphically, and as a table that has a text search facility.   The GO terms predicted for the sample are displayed. Different\ngraphical representations are available, and can be selected by clicking\non the \u2018Switch view\u2019\nicons.   The Gene Ontology terms displayed graphically on the web site have been\n\u2018slimmed\u2019 with a special GO slim developed for metagenomic data sets. GO\nslims are cut-down versions of the Gene Ontology, containing a subset of\nthe terms in the whole GO. They give a broad overview of the ontology\ncontent without the detail of the specific fine-grained terms.  The full data sets used to generate both the InterPro and GO overview\ncharts, along with a host of additional data (processed reads, pCDS,\nreads encoding 16S rRNAs, taxonomic analyses, etc) can be downloaded for\nfurther analysis by clicking the Download tab, found towards the top of\nthe page (see Figure 6).  Each sample has a download tab, where the full set of sequences,\nanalyses, summaries and raw data can be\ndownloaded.", 
            "title": "Finding functional information about samples on the EMG website"
        }, 
        {
            "location": "/modules/metagenomics-module-fda/fda/#practical", 
            "text": "", 
            "title": "Practical"
        }, 
        {
            "location": "/modules/metagenomics-module-fda/fda/#part-1-browsing-analysed-data-via-the-emg-website", 
            "text": "Open the Metagenomics Portal homepage in a web browser https://www.ebi.ac.uk/metagenomics/ .  From the Projects list click the \u2018Projects\u2019 tab, or \u2018View all projects\u2019,\nfind and click on HOT station, Central North Pacific Gyre, ALOH. You\nshould now have a Project overview page describing the project, related\npublications, and links to the samples that the project contains.  Question 1: What publications are associated with this study?  Scroll down to Associated Samples and open the HOT Station ALOHA, 25 m\nsample. You should now have a Sample Overview page, describing various\nmeta-data associated with the sample, such as the geographic location\nfrom which it was isolated, its collection date, and so on.  Question 2: What is the latitude, longitude and depth at which the\nsample was collected?  Question 3: What geographic location does this correspond to?  Question 4: What environmental ontology (ENVO) identifer and name has\nthe sample material been annotated with?  Click on the \u2018Download\u2019 tab. Right click the file labelled \u2018Predicted\nCDS (FASTA)\u2019 link, and save this file to your desktop. Find the file,\nand either double click on it to open it, or examine it using \u2018less\u2019 by\ntyping the following commands in a terminal window:  1\n2 cd ~/Desktop\nless HOT_Station_ALOHA,_25m_depth_pCDS.faa   Have a look at one or two of the many sequences it contains. We will\nlook at the analysis results for this entire batch of sequences,\ndisplayed on the EMG website, in a moment. First, we will attempt\nanalyse just one of the predicted coding sequences using InterPro (the\nanalysis results on the EMG website summarise these kind of results for\ntens or hundreds of thousands of sequences).  In a new tab or window, open your web browser and navigate to http://www.ebi.ac.uk/interpro/ . Copy and paste the following sequence\ninto the text box on the InterPro home page where it says \u2018Analyse your\nsequence\u2019:  1\n2 SRR010898.122503_1_115_+\nENNQEIKIIRNYINEFNLTGFIVGIPLDEKGQMTNQAI   Press Search and wait for your results. Your sequence will be run\nthrough the InterProScan analysis software, which attempts to match it\nagainst all of the different signatures in the InterProScan database.  Question 5: What feature does InterProScan predict your sequence to\ncontain?  Clicking on the InterPro entry name or IPR accession number will take\nyou to the InterPro entry page for your result, where more information\ncan be found.  Question 6: What GO terms is the feature associated with?  Return to the sample page for HOT station, Central North Pacific Gyre,\nALOH 25 m.  Now we are going to look at the functional analysis results for all of\nthe pCDS in the sample. First, we will find the number of sequences that\nmade it through to the functional analysis section of the pipeline.  Click on the Quality control tab. This page displays a series of charts,\nshowing how many sequences passed each quality control step, how many\nreads were left after clustering, and so on.  Question 7: After all of the quality filtering steps are complete, how\nmany reads were submitted for analysis by the pipeline?  Next, we will look at the results of the functional predictions for the\npCDS. These can be found under the Functional analysis tab.  Click on the Functional analysis tab and examine the InterPro match\nsection. The top part of this page shows a sequence feature summary,\nshowing the number of reads with predicted coding sequences (pCDS), the\nnumber of pCDS with InterPro matches, etc.  Question 8: How many of the reads that passed the quality control stage\nhad predicted coding sequences (pCDS)?  Question 9: How many pCDS have InterProScan hits?  Scroll down the page to the InterPro match summary section  Question 10: How many different InterPro entries were matched by the\npCDS?  Question 11: Why is this figure different to the number of pCDS that\nhave InterProScan hits?  Next we will examine the GO terms predicted by InterPro for the pCDS in\nthe sample.  Scroll down to the GO term annotation section of the page and examine\nthe 3 bar charts, which correspond to the 3 different components of the\nGene Ontology.  Question 12: What are the top 3 biological process terms predicted for\nthe pCDS from the sample?  Selecting the pie chart representation of GO terms makes it easier to\nvisualise the data to find the answer.  Now we will look at the taxonomic analysis for this sample.  Click on the Taxonomy Analysis tab and examine the phylum composition\ngraph and table.  Question 13: How many different OTUs were found in this sample?  Question 14: What are the top 3 phyla in the sample?  Select the Krona chart view of the data icon. This brings up an\ninteractive chart that can be used to analyse data at different\ntaxonomic ranks.  Question 15: What proportion of cyanobacteria in this sample are made up\nof Synechococcaceae?  Now we will compare these analyses with those for a sample taken at 500\nm from the same geographical location.  In a new tab or window, find and open the HOT station, Central North\nPacific Gyre, ALOH project page again. Select the sample HOT Station\nALOHA, 500m and examine the information under the Functional analysis\ntabs. Question 16: How many pCDS were in this sample? Question 17: How\nmany of the pCDSs have an InterPro match? Question 18: How many\ndifferent InterPro entries are matched by this sample? Question 19: Are\nthese figures broadly comparable to the ones for the 25 m sample?  Scroll to the bottom of the page and examine the GO term annotation for\nthe day 500 m sample.  Question 20: Are there visible differences between the GO terms for this\nsample and the 25 m sample? Could there be any biological explanation\nfor this?  Selecting the bar chart representation of GO terms makes it easier to\ncompare different data sets.  Return to the HOT station project page and open the taxonomy analysis\nresults for all 4 of the samples, each in a new window.  Question 21: How does the taxonomic composition change at different\nocean sampling depths? Are any trends in the data consistent with your\nanswer to question 18?", 
            "title": "Part 1 \u2013 Browsing analysed data via the EMG website"
        }, 
        {
            "location": "/modules/metagenomics-module-fda/fda/#part-2-analysing-emg-data-using-stamp", 
            "text": "Whilst EMG does not currently support direct comparison of multiple\nsamples, it is possible to download the underlying data for use with\nother visualisation and/or statistical analysis tools, such as STAMP\n(Statistical Analysis of Metagenomic Profiles: http://kiwi.cs.dal.ca/Software/STAMP . The Downloads tab for a given\nsample lists the files with the necessary data, although a slight amount\nof data wrangling is usually required to reformat files, etc.  We are going perform a detailed comparison of the GO slimmed biological\nprocess results for the samples at 25 m and 500 m using STAMP. To do\nthis, open the HOT station, Central North Pacific Gyre, ALOH project\npage again. Select the sample HOT Station ALOHA, 25 m study and download\nthe GO slim annotation (CSV) file from the download tab. Repeat this for\nthe HOT Station ALOHA, 500 m study.  You should now have the following 2 files in your downloads folder:\nHOT_Station_ALOHA,_25m_depth_GO_slim.csv\nHOT_Station_ALOHA,_500m_depth,_MG_GO_slim.csv.txt (don\u2019t worry\nabout the fact the file names are constructed slightly differently \u2013\nthis is an inconsistency on our web site that needs to be corrected!)  Drag or copy these files to your desktop, and then launch a terminal\nwindow, so that you can manipulate the files on the command line.  Navigate to the Desktop directory by typing \u2018cd ~/Desktop/\u2019 Next, type\n\u2018ls\u2019 to make sure you can see the files, and take a look at their\ncontents using the \u2018less\u2019 command.  First we are going to wrangle data from the 25 m sample, using a mixture\nof \u2018grep\u2019 to pull out the lines containing the phrase\n\u2018biological_process\u2019 and \u2018awk\u2019 to split up and reorder the fields.  awk is a text-processing language that is useful for extracting or\ntransforming documents and reformatting text. You can find out more\nabout it here:  http://www.gnu.org/software/gawk/manual/gawk.html  Enter the following command (all on one line):  1 grep  biological_process   HOT_Station_ALOHA,_25m_depth_GO_slim.csv  | awk -F   {print \\$6  \\t  \\$4  \\t  \\$8 }      biological_process_25m.txt   This should produce a new file called \u2018biological_process_26m.txt\u2019.\nUse \u2018less\u2019 to examine this file.  Question 22: What are the differences between this file and the\nHOT_Station_ALOHA,_25m_depth_GO_slim.csv file?  Next, we are going to add the biological process data from the 500 m\nsample to this file. Once again, will use \u2018grep\u2019 to pull out the\nrelevant lines in the source file, \u2018awk\u2019 to process the lines and output\nthe correct fields, and \u2018paste\u2019 to merge the data with the contents of\nthe file we produced above.  Enter the following command (all on one line):  1 grep  biological_process  HOT_Station_ALOHA,_500m_depth,_MG_GO_slim.csv | awk -F {}   {print \\$8 }  | paste biological_process_25m.txt -   biological_process_25mvs500m.txt   This should create another new file, this time called\n\u2018biological_process_25mvs500m.txt\u2019. Use \u2018less\u2019 to examine this file.  Question 23: What is the difference between this file and the\nbiological_process_25m.txt file?  Finally, we need to add a simple header to the file, so that STAMP can\nprocess it properly. To do this, we could open the file and add the\nheader manually, but here we will do it on the command line using the\n\u2018printf\u2019 command to write the header, \u2018cat\u2019 to print the contents of the\nfile, and directing the output to a new file with the \u2018 \u2019 operator.  Enter the following command (all on one line):  1 printf  Classification\\tSubclassification\\t25m\\t500m\\n  | cat - biological_process_25mvs500m.txt   biological_process_25mvs500m.spf   This command should have produced a third file called\n\u2018biological_process_25mvs500m.spf\u2019 that STAMP can read.  To launch STAMP, type STAMP in the terminal, hit the enter/return key, a\ngraphical interface will then appear. Load the\nbiological_process_25mvs500m.spf file into STAMP (you will need to set\n\u2018Profile level\u2019 to \u2018Subclassification\u2019 and select \u2018Two samples\u2019 from the\ntabs on the left hand side of the page to view the file once it is\nloaded). Try experimenting with different plot types and statistical\ntests.  Question 24: Are there any observable differences in biological process\nGO terms between the 2 samples?  For your convenience, three files containing the HOT Station GO term\nresults for all 3 branches of the Gene Ontology (molecular function,\nbiological process and cellular component) at each sample depth, and a\ncorresponding metadata file have been pre-generated and formatted for\nuse with STAMP.  These are located in the /Desktop/InterproTutorial/\nfolder on the desktop (filenames: HOT_Station_ALOHA_bp.spf,\nHOT_Station_ALOHA_mf.spf, HOT_Station_ALOHA_cc.spf and\nHOT_Station_ALOHA_metadata.tsv).  Try loading these files into STAMP, as you did with the previous file,\nand exploring the results. You can select different samples to compare\n(25, 75, 125 and 500 m) using the sample 1 and sample 2 drop down menus.\nYou can now also use the Multiple Groups tab and explore some of its\noptions.  Question 25: What trends can you identify in the molecular function,\nbiological process and cellular component GO terms across the different\nsampling depths?", 
            "title": "Part 2 - Analysing EMG data using STAMP"
        }
    ]
}