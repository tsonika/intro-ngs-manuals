{
    "docs": [
        {
            "location": "/", 
            "text": "Welcome to Introduction to NGS Data Analysis\n\n\nUniversity of Sydney\n\n\n27\nth\n -29\nth\n June 2017\n\n\nThe Introduction to NGS Data Analysis is a three-day, hands-on workshop that offers attendees a basic understanding of NGS data analysis workflows. The workshop provides hands-on computational experience in analysis of NGS data using common analytical approaches for ChIP-Seq, RNA-Seq data and de novo genome assembly.\n\n\nWorkshop Content\n\n\nTopics covered by this workshop include:\n\n* An introduction to the command line interface and NGS file formats\n\n* Assessment of the quality of NGS sequence reads\n\n* Sequence alignment algorithms\n\n* Basic ChIP-Seq analysis\n\n* Basic RNA-Seq analysis\n\n* Short and long read de novo genome assembly\n\n\nThis workshop will be delivered using a mixture of lectures, hands-on practical sessions, and open discussions.\n\n\nAcknowledgements\n\n\nThis workshop was developed by Bioplatforms Australia, in partnership with CSIRO, and with support from the European Bioinformatics Institute (EBI), a member of the European Molecular Biology Laboratory (EMBL) in the UK. The workshop content has been maintained and updated by a network of dedicated [bioinformatics trainers] \nhttp://www.bioplatforms.com/bioinformatics-training/\n from around Australia.\n\n\nThe Bioinformatics Training Platform was developed in collaboration with the Monash Bioinformatics Platform and this workshop has been hosted on Interset infrastructure.\n\n\nLicensing\n\n\nThis work is licensed under a Creative Commons Attribution 3.0 Unported License and the below text is a summary of the main terms of the full Legal Code (the full licence) available at \nhttp://creativecommons.org/licenses/by/3.0/legalcode\n.\n\n\nYou are free:\n\n* to copy, distribute, display, and perform the work to make derivative works\n\n* to make commercial use of the work\n\n\nUnder the following conditions:\n\n* \nAttribution\n - You must give the original author credit.\n\n\nWith the understanding that:\n\n\n\n\nWaiver\n - Any of the above conditions can be waived if you get permission from the copyright holder. \n\n\nPublic Domain\n- Where the work or any of its elements is in the public domain under applicable law, that status is in no way affected by the license. \n\n\nOther Rights\n - In no way are any of the following rights affected by the license: \u2022 Your fair dealing or fair use rights, or other applicable copyright exceptions and limitations;\n\n* The author\u2019s moral rights;\n\n* Rights other persons may have either in the work itself or in how the work is used, such as publicity or privacy rights.\n\n\nNotice\n - For any reuse or distribution, you must make clear to others the licence terms of this work.", 
            "title": "Home"
        }, 
        {
            "location": "/#welcome-to-introduction-to-ngs-data-analysis", 
            "text": "", 
            "title": "Welcome to Introduction to NGS Data Analysis"
        }, 
        {
            "location": "/#university-of-sydney", 
            "text": "27 th  -29 th  June 2017  The Introduction to NGS Data Analysis is a three-day, hands-on workshop that offers attendees a basic understanding of NGS data analysis workflows. The workshop provides hands-on computational experience in analysis of NGS data using common analytical approaches for ChIP-Seq, RNA-Seq data and de novo genome assembly.", 
            "title": "University of Sydney"
        }, 
        {
            "location": "/#workshop-content", 
            "text": "Topics covered by this workshop include: \n* An introduction to the command line interface and NGS file formats \n* Assessment of the quality of NGS sequence reads \n* Sequence alignment algorithms \n* Basic ChIP-Seq analysis \n* Basic RNA-Seq analysis \n* Short and long read de novo genome assembly  This workshop will be delivered using a mixture of lectures, hands-on practical sessions, and open discussions.", 
            "title": "Workshop Content"
        }, 
        {
            "location": "/#acknowledgements", 
            "text": "This workshop was developed by Bioplatforms Australia, in partnership with CSIRO, and with support from the European Bioinformatics Institute (EBI), a member of the European Molecular Biology Laboratory (EMBL) in the UK. The workshop content has been maintained and updated by a network of dedicated [bioinformatics trainers]  http://www.bioplatforms.com/bioinformatics-training/  from around Australia.  The Bioinformatics Training Platform was developed in collaboration with the Monash Bioinformatics Platform and this workshop has been hosted on Interset infrastructure.", 
            "title": "Acknowledgements"
        }, 
        {
            "location": "/#licensing", 
            "text": "This work is licensed under a Creative Commons Attribution 3.0 Unported License and the below text is a summary of the main terms of the full Legal Code (the full licence) available at  http://creativecommons.org/licenses/by/3.0/legalcode .  You are free: \n* to copy, distribute, display, and perform the work to make derivative works \n* to make commercial use of the work  Under the following conditions: \n*  Attribution  - You must give the original author credit.  With the understanding that:   Waiver  - Any of the above conditions can be waived if you get permission from the copyright holder.   Public Domain - Where the work or any of its elements is in the public domain under applicable law, that status is in no way affected by the license.   Other Rights  - In no way are any of the following rights affected by the license: \u2022 Your fair dealing or fair use rights, or other applicable copyright exceptions and limitations; \n* The author\u2019s moral rights; \n* Rights other persons may have either in the work itself or in how the work is used, such as publicity or privacy rights.  Notice  - For any reuse or distribution, you must make clear to others the licence terms of this work.", 
            "title": "Licensing"
        }, 
        {
            "location": "/trainers/trainers/", 
            "text": "Workshop Trainers\n\n\n\n\n\n\n\n\n\n\n\nDr. Susan Corley\n \n\nPostdoctoral Research Associate\n\nThe University of New South Wales (UNSW), Sydney  \n\n\n\n\nDr. Nandan Deshpande\n \n\nPostdoctoral Research Associate\n\nThe University of New South Wales (UNSW), Sydney  \n\n\n\n\n\n \n\n\nDr. Matthew Field\n \n\nSenior Research Fellow\n\nAustralian National University/James Cook University, Cairns\n\n\n\n\n\n\n\n\n \n\n\nDr. Xi (Sean) Li\n\nGenomics Bioinformatician\n\nThe Australian National University, Canberra  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\nDr. Sonika Tyagi\n \n\nBioinformatics Supervisor\n\nAustralian Genome Research Facility Ltd, Melbourne  \n\n\nBioinformatics Training Development\n\n\n \n\n\nMr. Jerico Revote\n \n\nSoftware Developer \n\nMonash Bioinformatics Platform Monash University, Clayton Melbourne  \n\n\n \n\n\nDr. Nathan S. Watson-Haigh\n \n\nResearch Fellow in Bioinformatics\n\nThe Australian Centre for Plant Functional Genomics (ACPFG), Adelaide  \n\n\n \n\n\nDr. Annette McGrath\n\n\nPrincipal Research Scientist, Team Leader\n\nLife Science Informatics DATA61, CSIRO, Canberra  \n\n\nWorkshop Coordinator\n\n\n \n\n\nKatherine Champ\n \n\nWorkshop Coordinator, Project Officer \n\nBioplatform Autralia Ltd.", 
            "title": "The Trainers"
        }, 
        {
            "location": "/trainers/trainers/#workshop-trainers", 
            "text": "Dr. Susan Corley   \nPostdoctoral Research Associate The University of New South Wales (UNSW), Sydney     Dr. Nandan Deshpande   \nPostdoctoral Research Associate \nThe University of New South Wales (UNSW), Sydney        Dr. Matthew Field   \nSenior Research Fellow \nAustralian National University/James Cook University, Cairns       Dr. Xi (Sean) Li \nGenomics Bioinformatician \nThe Australian National University, Canberra               Dr. Sonika Tyagi   \nBioinformatics Supervisor \nAustralian Genome Research Facility Ltd, Melbourne", 
            "title": "Workshop Trainers"
        }, 
        {
            "location": "/trainers/trainers/#bioinformatics-training-development", 
            "text": "Mr. Jerico Revote   \nSoftware Developer  \nMonash Bioinformatics Platform Monash University, Clayton Melbourne       Dr. Nathan S. Watson-Haigh   \nResearch Fellow in Bioinformatics \nThe Australian Centre for Plant Functional Genomics (ACPFG), Adelaide       Dr. Annette McGrath  Principal Research Scientist, Team Leader \nLife Science Informatics DATA61, CSIRO, Canberra", 
            "title": "Bioinformatics Training Development"
        }, 
        {
            "location": "/trainers/trainers/#workshop-coordinator", 
            "text": "Katherine Champ   \nWorkshop Coordinator, Project Officer  \nBioplatform Autralia Ltd.", 
            "title": "Workshop Coordinator"
        }, 
        {
            "location": "/timetables/timetable_introNGS/", 
            "text": "Introduction to NGS Data Analysis Workshop\n\n\n\n\nThe University of Sydney, NSW - 27\nth\n - 29\nth\n June 2017\n\n\nInstructors\n\n\n\n\nKatherine Champ (KC) - Bioplatforms Australia, Sydney\n\n\nSonika Tyagi (ST) - AGRF, Melbourne\n\n\nMatthew Field (MF) - Australian National University/James Cook University, Cairns\n\n\nXi (Sean) Li (SL) - Australian National University, Canberra\n\n\nSusan Corley (SC) - UNSW Systems Biology Initiative, Sydney\n\n\n\n\nTimetable\n\n\nDay 1 - Introduction to the command line, data quality \n alignment \n ChIP-Seq\n\n\n27\nth\n June\n - \nComputer Lab 1.4, Charles Perkins Centre, University of Sydney, NSW\n\n\n\n\n\n\n\n\nTime\n\n\nTopic\n\n\nLinks\n\n\nInstructor\n\n\n\n\n\n\n\n\n\n\n09:00\n\n\nIntroductions and course orientation\n\n\n\n\nKC\n\n\n\n\n\n\n09:45\n\n\nPractical: Introduction to the command line\n\n\n\n\nMF\n\n\n\n\n\n\n10:15\n\n\nMorning Tea\n\n\n\n\n\n\n\n\n\n\n10:40\n\n\nPractical: Introduction to the command line course and R course\n\n\n\n\nMF\n\n\n\n\n\n\n11:20\n\n\nIntroduction to NGS- technology, data formats and introduction to quality control\n\n\n\n\nMF\n\n\n\n\n\n\n12:30\n\n\nLunch\n\n\n\n\n\n\n\n\n\n\n13:15\n\n\nQuality control: Intro to practical\n\n\n\n\nST\n\n\n\n\n\n\n13:25\n\n\nPractical: Quality control\n\n\n\n\nST\n\n\n\n\n\n\n14:05\n\n\nIntroduction to sequence alignment\n\n\n\n\nSL\n\n\n\n\n\n\n14:15\n\n\nPractical: Sequence alignment\n\n\n\n\nSL\n\n\n\n\n\n\n15:00\n\n\nAfternoon Tea\n\n\n\n\n\n\n\n\n\n\n15:25\n\n\nIntroduction to ChIP-Seq\n\n\n\n\nSL\n\n\n\n\n\n\n15:55\n\n\nPractical: ChIP-Seq analysis - Peak calling and annotation\n\n\n\n\nSL\n\n\n\n\n\n\n16:30\n\n\nQ\nA and day 1 wrap-up\n\n\n\n\nAll\n\n\n\n\n\n\n\n\nDay 2 - ChIP-Seq motif analysis and RNA-Seq analysis\n\n\n28\nth\n June\n - \nComputer Lab 1.4, Charles Perkins Centre, University of Sydney, NSW\n\n\n\n\n\n\n\n\nTime\n\n\nTopic\n\n\nLinks\n\n\nInstructor\n\n\n\n\n\n\n\n\n\n\n09:00\n\n\nPractical: Motif analysis\n\n\n\n\nSL\n\n\n\n\n\n\n09:40\n\n\nIntroduction to RNA-Seq\n\n\n\n\nSC\n\n\n\n\n\n\n10:30\n\n\nMorning Tea\n\n\n\n\n\n\n\n\n\n\n10:50\n\n\nPractical: Alignment and splice junction identification\n\n\n\n\nSC\n\n\n\n\n\n\n12:30\n\n\nLunch\n\n\n\n\n\n\n\n\n\n\n13:30\n\n\nPractical: Differential gene expression with Bio-conductor package: EdgeR and Voom\n\n\n\n\nSC\n\n\n\n\n\n\n15:00\n\n\nAfternoon Tea\n\n\n\n\n\n\n\n\n\n\n15:30\n\n\nPractical: Biological interpretation\n\n\n\n\nSC\n\n\n\n\n\n\n16:30\n\n\nQ\nA and day 2 wrap-up\n\n\n\n\nAll\n\n\n\n\n\n\n\n\nDay 3 - \nde novo\n Assembly\n\n\n29\nth\n June\n - \nComputer Lab 1.4, Charles Perkins Centre, University of Sydney, NSW\n\n\n\n\n\n\n\n\nTime\n\n\nTopic\n\n\nLinks\n\n\nInstructor\n\n\n\n\n\n\n\n\n\n\n09:00\n\n\nIntroduction to de novo assembly\n\n\n\n\nST\n\n\n\n\n\n\n09:40\n\n\nPractical: de novo assembly using Illumina reads\n\n\n\n\nST\n\n\n\n\n\n\n10:30\n\n\nMorning Tea\n\n\n\n\n\n\n\n\n\n\n10:50\n\n\nPractical: de novo assembly using Illumina reads (cont.)\n\n\n\n\nST\n\n\n\n\n\n\n11:30\n\n\nPractical: de novo assembly using PacBio \u2013 Canu workflow\n\n\n\n\nST\n\n\n\n\n\n\n12:30\n\n\nLunch\n\n\n\n\n\n\n\n\n\n\n13:30\n\n\nPractical: de novo assembly using PacBio \u2013 Canu workflow (cont.)\n\n\n\n\nST\n\n\n\n\n\n\n15:30\n\n\nAfternoon Tea\n\n\n\n\n\n\n\n\n\n\n15:50\n\n\nPractical: Polishing PacBio de novo assembly with Illumina reads\n\n\n\n\nST\n\n\n\n\n\n\n16:30\n\n\nQ\nA and workshop wrap-up\n\n\n\n\nAll\n\n\n\n\n\n\n17:00\n\n\nWorkshop Survey\n\n\n\n\nKC", 
            "title": "Intro to NGS"
        }, 
        {
            "location": "/timetables/timetable_introNGS/#introduction-to-ngs-data-analysis-workshop", 
            "text": "The University of Sydney, NSW - 27 th  - 29 th  June 2017", 
            "title": "Introduction to NGS Data Analysis Workshop"
        }, 
        {
            "location": "/timetables/timetable_introNGS/#instructors", 
            "text": "Katherine Champ (KC) - Bioplatforms Australia, Sydney  Sonika Tyagi (ST) - AGRF, Melbourne  Matthew Field (MF) - Australian National University/James Cook University, Cairns  Xi (Sean) Li (SL) - Australian National University, Canberra  Susan Corley (SC) - UNSW Systems Biology Initiative, Sydney", 
            "title": "Instructors"
        }, 
        {
            "location": "/timetables/timetable_introNGS/#timetable", 
            "text": "", 
            "title": "Timetable"
        }, 
        {
            "location": "/timetables/timetable_introNGS/#day-1-introduction-to-the-command-line-data-quality-alignment-chip-seq", 
            "text": "27 th  June  -  Computer Lab 1.4, Charles Perkins Centre, University of Sydney, NSW     Time  Topic  Links  Instructor      09:00  Introductions and course orientation   KC    09:45  Practical: Introduction to the command line   MF    10:15  Morning Tea      10:40  Practical: Introduction to the command line course and R course   MF    11:20  Introduction to NGS- technology, data formats and introduction to quality control   MF    12:30  Lunch      13:15  Quality control: Intro to practical   ST    13:25  Practical: Quality control   ST    14:05  Introduction to sequence alignment   SL    14:15  Practical: Sequence alignment   SL    15:00  Afternoon Tea      15:25  Introduction to ChIP-Seq   SL    15:55  Practical: ChIP-Seq analysis - Peak calling and annotation   SL    16:30  Q A and day 1 wrap-up   All", 
            "title": "Day 1 - Introduction to the command line, data quality &amp; alignment &amp; ChIP-Seq"
        }, 
        {
            "location": "/timetables/timetable_introNGS/#day-2-chip-seq-motif-analysis-and-rna-seq-analysis", 
            "text": "28 th  June  -  Computer Lab 1.4, Charles Perkins Centre, University of Sydney, NSW     Time  Topic  Links  Instructor      09:00  Practical: Motif analysis   SL    09:40  Introduction to RNA-Seq   SC    10:30  Morning Tea      10:50  Practical: Alignment and splice junction identification   SC    12:30  Lunch      13:30  Practical: Differential gene expression with Bio-conductor package: EdgeR and Voom   SC    15:00  Afternoon Tea      15:30  Practical: Biological interpretation   SC    16:30  Q A and day 2 wrap-up   All", 
            "title": "Day 2 - ChIP-Seq motif analysis and RNA-Seq analysis"
        }, 
        {
            "location": "/timetables/timetable_introNGS/#day-3-de-novo-assembly", 
            "text": "29 th  June  -  Computer Lab 1.4, Charles Perkins Centre, University of Sydney, NSW     Time  Topic  Links  Instructor      09:00  Introduction to de novo assembly   ST    09:40  Practical: de novo assembly using Illumina reads   ST    10:30  Morning Tea      10:50  Practical: de novo assembly using Illumina reads (cont.)   ST    11:30  Practical: de novo assembly using PacBio \u2013 Canu workflow   ST    12:30  Lunch      13:30  Practical: de novo assembly using PacBio \u2013 Canu workflow (cont.)   ST    15:30  Afternoon Tea      15:50  Practical: Polishing PacBio de novo assembly with Illumina reads   ST    16:30  Q A and workshop wrap-up   All    17:00  Workshop Survey   KC", 
            "title": "Day 3 - de novo Assembly"
        }, 
        {
            "location": "/preamble/", 
            "text": "Providing Feedback\n\n\nWhile we endeavour to deliver a workshop with quality content and\ndocumentation in a venue conducive to an exciting, well run hands-on\nworkshop with a bunch of knowledgeable and likable trainers, we know\nthere are things we could do better.\n\n\nWhilst we want to know what didn\u2019t quite hit the mark for you, what\nwould be most helpful and least depressing, would be for you to provide\nways to improve the workshop. i.e. constructive feedback. After all, if\nwe knew something wasn\u2019t going to work, we wouldn\u2019t have done it or put\nit into the workshop in the first place! Remember, we\u2019re experts in the\nfield of bioinformatics not experts in the field of biology!\n\n\nClearly, we also want to know what we did well! This gives us that \u201cfeel\ngood\u201d factor which will see us through those long days and nights in the\nlead up to such hands-on workshops!\n\n\nWith that in mind, we\u2019ll provide three really high tech mechanism\nthrough which you can provide anonymous feedback during the workshop:\n\n\n\n\n\n\nA sheet of paper, from a flip-chart, sporting a \u201chappy\u201d face and a\n\u201cnot so happy\u201d face. Armed with a stack of colourful post-it notes, your\nmission is to see how many comments you can stick on the \u201chappy\u201d side!\n\n\n\n\n\n\nSome empty ruled pages at the back of this handout. Use them for\nyour own personal notes or for write specific comments/feedback about\nthe workshop as it progresses.\n\n\n\n\n\n\nAn online post-workshop evaluation survey. We\u2019ll ask you to complete\nthis before you leave. If you\u2019ve used the blank pages at the back of\nthis handout to make feedback notes, you\u2019ll be able to provide more\nspecific and helpful feedback with the least amount of brain-drain!\n\n\n\n\n\n\nDocument Structure\n\n\nWe have provided you with an electronic copy of the workshop\u2019s hands-on\ntutorial documents. We have done this for two reasons: 1) you will have\nsomething to take away with you at the end of the workshop, and 2) you\ncan save time (mis)typing commands on the command line by using\ncopy-and-paste.\n\n\n\n\nWhile you could fly through the hands-on sessions doing copy-and-paste\nyou will learn more if you take the time, saved from not having to type\nall those commands, to understand what each command is doing!\n\n\n\n\nThe commands to enter at a terminal look something like this:\n\n\n1\ntophat --solexa-quals -g 2 --library-type fr-unstranded -j annotation/Danio_rerio.Zv9.66.spliceSites -o tophat/ZV9_2cells genome/ZV9 data/2cells_1.fastq data/2cells_2.fastq\n\n\n\n\n\n\nThe following styled code is not to be entered at a terminal, it is\nsimply to show you the syntax of the command. You must use your own\njudgement to substitute in the correct arguments, options, filenames etc\n\n\n1\ntophat [options]* \nindex_base\n \nreads_1\n \nreads_2\n\n\n\n\n\n\n\nThe following is an example how of R commands are styled:\n\n\n1\n2\n3\n4\n5\nR \n--\nno\n-\nsave\n\n\nlibrary\n(\nplotrix\n)\n\ndata \n-\n read.table\n(\nrun_25/stats.txt\n,\n header\n=\nTRUE\n)\n\nweighted.hist\n(\ndata\n$\nshort1_cov\n+\ndata\n$\nshort2_cov\n,\n data\n$\nlgth\n,\n breaks\n=\n0\n:\n70\n)\n\n\nq\n()\n\n\n\n\n\n\n\nThe following icons are used throughout the documentation\nto help you navigate around the document more easily:\n\n\n\n\nQuestion\n\n\nQuestions to answer.\n\n\n\n\n\n\nAnswer\n\n\nAnswers will be provided at the end of the workskop.\n\n\n\n\n\n\nImportant\n\n\nThis is important. \n\n\n\n\n\n\nSTOP\n\n\nWarning - STOP and read.\n\n\n\n\n\n\nBonus exercise\n\n\nBonus exercise for fast learners.\n\n\n\n\n\n\nAdvanced exercise\n\n\nAdvanced exercise for super-fast learners\n\n\n\n\nResources Used\n\n\nWe have provided you with an environment which contains all the tools\nand data you need for the duration of this workshop. However, we also\nprovide details about the tools and data used by each module at the\nstart of the respective module documentation.", 
            "title": "Workshop Information"
        }, 
        {
            "location": "/preamble/#providing-feedback", 
            "text": "While we endeavour to deliver a workshop with quality content and\ndocumentation in a venue conducive to an exciting, well run hands-on\nworkshop with a bunch of knowledgeable and likable trainers, we know\nthere are things we could do better.  Whilst we want to know what didn\u2019t quite hit the mark for you, what\nwould be most helpful and least depressing, would be for you to provide\nways to improve the workshop. i.e. constructive feedback. After all, if\nwe knew something wasn\u2019t going to work, we wouldn\u2019t have done it or put\nit into the workshop in the first place! Remember, we\u2019re experts in the\nfield of bioinformatics not experts in the field of biology!  Clearly, we also want to know what we did well! This gives us that \u201cfeel\ngood\u201d factor which will see us through those long days and nights in the\nlead up to such hands-on workshops!  With that in mind, we\u2019ll provide three really high tech mechanism\nthrough which you can provide anonymous feedback during the workshop:    A sheet of paper, from a flip-chart, sporting a \u201chappy\u201d face and a\n\u201cnot so happy\u201d face. Armed with a stack of colourful post-it notes, your\nmission is to see how many comments you can stick on the \u201chappy\u201d side!    Some empty ruled pages at the back of this handout. Use them for\nyour own personal notes or for write specific comments/feedback about\nthe workshop as it progresses.    An online post-workshop evaluation survey. We\u2019ll ask you to complete\nthis before you leave. If you\u2019ve used the blank pages at the back of\nthis handout to make feedback notes, you\u2019ll be able to provide more\nspecific and helpful feedback with the least amount of brain-drain!", 
            "title": "Providing Feedback"
        }, 
        {
            "location": "/preamble/#document-structure", 
            "text": "We have provided you with an electronic copy of the workshop\u2019s hands-on\ntutorial documents. We have done this for two reasons: 1) you will have\nsomething to take away with you at the end of the workshop, and 2) you\ncan save time (mis)typing commands on the command line by using\ncopy-and-paste.   While you could fly through the hands-on sessions doing copy-and-paste\nyou will learn more if you take the time, saved from not having to type\nall those commands, to understand what each command is doing!   The commands to enter at a terminal look something like this:  1 tophat --solexa-quals -g 2 --library-type fr-unstranded -j annotation/Danio_rerio.Zv9.66.spliceSites -o tophat/ZV9_2cells genome/ZV9 data/2cells_1.fastq data/2cells_2.fastq   The following styled code is not to be entered at a terminal, it is\nsimply to show you the syntax of the command. You must use your own\njudgement to substitute in the correct arguments, options, filenames etc  1 tophat [options]*  index_base   reads_1   reads_2    The following is an example how of R commands are styled:  1\n2\n3\n4\n5 R  -- no - save  library ( plotrix ) \ndata  -  read.table ( run_25/stats.txt ,  header = TRUE ) \nweighted.hist ( data $ short1_cov + data $ short2_cov ,  data $ lgth ,  breaks = 0 : 70 )  q ()    The following icons are used throughout the documentation\nto help you navigate around the document more easily:   Question  Questions to answer.    Answer  Answers will be provided at the end of the workskop.    Important  This is important.     STOP  Warning - STOP and read.    Bonus exercise  Bonus exercise for fast learners.    Advanced exercise  Advanced exercise for super-fast learners", 
            "title": "Document Structure"
        }, 
        {
            "location": "/preamble/#resources-used", 
            "text": "We have provided you with an environment which contains all the tools\nand data you need for the duration of this workshop. However, we also\nprovide details about the tools and data used by each module at the\nstart of the respective module documentation.", 
            "title": "Resources Used"
        }, 
        {
            "location": "/modules/btp-module-ngs-cli/commandline/", 
            "text": "Key Learning Outcomes\n\n\nAfter completing this practical the trainee should be able to:\n\n\n\n\n\n\nFamiliarise yourself with the command line environment on a Linux\n    operating system.\n\n\n\n\n\n\nRun some basic linux system and file operation commands\n\n\n\n\n\n\nNavigration of biological data files structure and manipulation\n\n\n\n\n\n\nResources\n\n\nTools\n\n\n\n\n\n\nBasic Linux system commands on an Ubuntu OS.\n\n\n\n\n\n\nBasic file operation commands\n\n\n\n\n\n\nLinks\n\n\n\n\n\n\nSoftware Carpentry\n\n\n\n\n\n\nExample 1000Genome Project data\n\n\n\n\n\n\nAuthor Information\n\n\nPrimary Author(s):\n\n    Matt Field \n     \n\n\nShell Exercise\n\n\nLet\u2019s try out your new shell skills on some real data.\n\n\nThe file \n1000gp.vcf\n is a small sample (1%) of a very large text file\ncontaining human genetics data. Specifically, it describes genetic\nvariation in three African individuals sequenced as part of the 1000\nGenomes Project (\nhttp://www.1000genomes.org\n). The \u2019vcf\u2019 extension lets\nus know that it\u2019s in a specific text format, namely \u2019Variant Call\nFormat\u2019. The file starts with a bunch of comment lines (they start with\n\u2019#\u2019 or \u2019##\u2019), and then a large number of data lines. This VCF file\nlists the differences between the three African individuals and a\nstandard \u2019individual\u2019 called the reference (actually based upon a few\ndifferent people). Each line in the file corresponds to a difference.\nThe line tells us the position of the difference (chromosome and\nposition), the genetic sequence in the reference, and the corresponding\nsequence in each of the three Africans. Before we start processing the\nfile, let\u2019s get a high-level view of the file that we\u2019re about to work\nwith.\n\n\nOpen the Terminal and go to the directory where the data are stored:\n\n1\n2\n3\n4\n5\ncd /home/trainee/cli\nls\npwd\nls -lh 1000gp.vcf\nwc -l 1000gp.vcf\n\n\n\n\n\n\n\nQuestion\n\n\nWhat is the file size (in kilo/mega-bytes), and how many lines are in the file?.\n\n\n\n\nHint\nHint: \nman ls\n, \nman wc\n\n\nAnswer\n3.6M\n45034 lines\n\n\nBecause this file is so large, you\u2019re going to almost always want to\npipe (\n|\n) the result of any command to less (a simple text viewer, type\n\u2018\nq\n\u2019 to exit) or head (to print the first 10 lines) so that you don\u2019t\naccidentally print 45,000 lines to the screen.\n\n\nLet\u2019s start by printing the first 5 lines to see what it looks like.\n\n1\nhead -5 1000gp.vcf\n\n\n\n\n\nThat isn\u2019t very interesting; it\u2019s just a bunch of the comments at the\nbeginning of the file (they all start with \u2019#\u2019)!\n\n\nPrint the first 20 lines to see more of the file.\n\n1\nhead -20 1000gp.vcf\n\n\n\n\n\nOkay, so now we can see the basic structure of the file. A few comment\nlines that start with \u2019#\u2019 or \u2019##\u2019 and then a bunch of lines of data\nthat contain all the data and are pretty hard to understand. Each line\nof data contains the same number of fields, and all fields are separated\nwith TABs. These fields are:\n\n\n\n\n\n\nthe chromosome (which volume the difference is in)\n\n\n\n\n\n\nthe position (which character in the volume the difference starts\n    at)\n\n\n\n\n\n\nthe ID of the difference\n\n\n\n\n\n\nthe sequence in the reference human(s)\n\n\n\n\n\n\nThe rest of the columns tell us, in a rather complex way, a bunch of\nadditional information about that position, including: the predicted\nsequence for each of the three Africans and how confident the scientists\nare that these sequences are correct.\n\n\nTo start analyzing the actual data, we have to remove the header.\n\n\n\n\nQuestion\n\n\nHow can we print the first 10 non-header lines (those that don\u2019t start\nwith a \u2019#\u2019)?\n\n\n\n\nHint\nHint: \nman grep\n (remember to use pipes \n|\n)\n\n\nAnswer\ngrep -v \n^#\n 1000gp.vcf | head  \n\n\n\n\nQuestion\n\n\nHow many lines of data are in the file (rather than counting the number\nof header lines and subtracting, try just counting the number of data\nlines)?\n\n\n\n\nAnswer\ngrep -v \n^#\n 1000gp.vcf | wc -l (should print 45024)\n\n\nWhere these differences are located can be important. If all the\ndifferences between two encyclopedias were in just the first volume,\nthat would be interesting. The first field of each data line is the name\nof the chromosome that the difference occurs on (which volume we\u2019re on).\n\n\n\n\nQuestion\n\n\nPrint the first 10 chromosomes, one per line.\n\n\n\n\nHint\nHint: \nman cut\n (remember to remove header lines first)\n\n\nAnswer\ngrep -v \n^#\n 1000gp.vcf | cut -f 1 | head\n\n\nAs you should have observed, the first 10 lines are on numbered\nchromosomes. Every normal cell in your body has 23 pairs of chromosomes,\n22 pairs of \u2018autosomal\u2019 chromosomes (these are numbered 1-22) and a pair\nof sex chromosomes (two Xs if you\u2019re female, an X and a Y if you\u2019re\nmale).\n\n\nLet\u2019s look at which chromosomes these variations are on.\n\n\n\n\nQuestion\n\n\nPrint a list of the chromosomes that are in the file (each chromosome\nname should only be printed once, so you should only print 23 lines).\n\n\n\n\nHint\nHint: remove all duplicates from your previous answer (\nman sort\n)\n\n\nAnswer\ngrep -v \n^#\n 1000gp.vcf | cut -f 1 | sort -u\n\n\nRather than using \nsort\n to print unique results, a common pipeline is\nto first sort and then pipe to another UNIX command, \nuniq\n. The \nuniq\n\ncommand takes sorted input and prints only unique lines, but it provides\nmore flexibility than just using sort by itself. Keep in mind, if the\ninput isn\u2019t sorted, \nuniq\n won\u2019t work properly.\n\n\n\n\nQuestion\n\n\nUsing \nsort\n and \nuniq\n, print the number of times each chromosome\noccurs in the file.\n\n\n\n\nHint\nHint: \nman uniq\n\n\nAnswer\ngrep -v \n^#\n 1000gp.vcf | cut -f 1 | sort | uniq -c\n\n\n\n\nQuestion\n\n\nAdd to your previous solution to list the chromosomes from most\nfrequently observed to least frequently observed.\n\n\n\n\nHint\nHint: Make sure you\u2019re sorting in descending order. By default, sort\nsorts in ascending order.\n\n\nAnswer\ngrep -v \n^#\n 1000gp.vcf | cut -f 1 | sort | uniq -c | sort -n -r\n\n\nThis is great, but biologists might also like to see the chromosomes\nordered by their number (not dictionary order), since different\nchromosomes have different attributes and this ordering allows them to\nfind a specific chromosome more easily.\n\n\n\n\nQuestion\n\n\nSort the previous output by chromosome number\n\n\n\n\nHint\nHint: A lot of the power of sort comes from the fact that you can\nspecify which fields to sort on, and the order in which to sort them. In\nthis case you only need to sort on one field.\n\n\nAnswer\ngrep -v \n^#\n 1000gp.vcf | cut -f 1 | sort | uniq -c | sort -k 2n", 
            "title": "Introduction to Command Line"
        }, 
        {
            "location": "/modules/btp-module-ngs-cli/commandline/#key-learning-outcomes", 
            "text": "After completing this practical the trainee should be able to:    Familiarise yourself with the command line environment on a Linux\n    operating system.    Run some basic linux system and file operation commands    Navigration of biological data files structure and manipulation", 
            "title": "Key Learning Outcomes"
        }, 
        {
            "location": "/modules/btp-module-ngs-cli/commandline/#resources", 
            "text": "", 
            "title": "Resources"
        }, 
        {
            "location": "/modules/btp-module-ngs-cli/commandline/#tools", 
            "text": "Basic Linux system commands on an Ubuntu OS.    Basic file operation commands", 
            "title": "Tools"
        }, 
        {
            "location": "/modules/btp-module-ngs-cli/commandline/#links", 
            "text": "Software Carpentry    Example 1000Genome Project data", 
            "title": "Links"
        }, 
        {
            "location": "/modules/btp-module-ngs-cli/commandline/#author-information", 
            "text": "Primary Author(s): \n    Matt Field", 
            "title": "Author Information"
        }, 
        {
            "location": "/modules/btp-module-ngs-cli/commandline/#shell-exercise", 
            "text": "Let\u2019s try out your new shell skills on some real data.  The file  1000gp.vcf  is a small sample (1%) of a very large text file\ncontaining human genetics data. Specifically, it describes genetic\nvariation in three African individuals sequenced as part of the 1000\nGenomes Project ( http://www.1000genomes.org ). The \u2019vcf\u2019 extension lets\nus know that it\u2019s in a specific text format, namely \u2019Variant Call\nFormat\u2019. The file starts with a bunch of comment lines (they start with\n\u2019#\u2019 or \u2019##\u2019), and then a large number of data lines. This VCF file\nlists the differences between the three African individuals and a\nstandard \u2019individual\u2019 called the reference (actually based upon a few\ndifferent people). Each line in the file corresponds to a difference.\nThe line tells us the position of the difference (chromosome and\nposition), the genetic sequence in the reference, and the corresponding\nsequence in each of the three Africans. Before we start processing the\nfile, let\u2019s get a high-level view of the file that we\u2019re about to work\nwith.  Open the Terminal and go to the directory where the data are stored: 1\n2\n3\n4\n5 cd /home/trainee/cli\nls\npwd\nls -lh 1000gp.vcf\nwc -l 1000gp.vcf    Question  What is the file size (in kilo/mega-bytes), and how many lines are in the file?.   Hint Hint:  man ls ,  man wc  Answer 3.6M 45034 lines  Because this file is so large, you\u2019re going to almost always want to\npipe ( | ) the result of any command to less (a simple text viewer, type\n\u2018 q \u2019 to exit) or head (to print the first 10 lines) so that you don\u2019t\naccidentally print 45,000 lines to the screen.  Let\u2019s start by printing the first 5 lines to see what it looks like. 1 head -5 1000gp.vcf   That isn\u2019t very interesting; it\u2019s just a bunch of the comments at the\nbeginning of the file (they all start with \u2019#\u2019)!  Print the first 20 lines to see more of the file. 1 head -20 1000gp.vcf   Okay, so now we can see the basic structure of the file. A few comment\nlines that start with \u2019#\u2019 or \u2019##\u2019 and then a bunch of lines of data\nthat contain all the data and are pretty hard to understand. Each line\nof data contains the same number of fields, and all fields are separated\nwith TABs. These fields are:    the chromosome (which volume the difference is in)    the position (which character in the volume the difference starts\n    at)    the ID of the difference    the sequence in the reference human(s)    The rest of the columns tell us, in a rather complex way, a bunch of\nadditional information about that position, including: the predicted\nsequence for each of the three Africans and how confident the scientists\nare that these sequences are correct.  To start analyzing the actual data, we have to remove the header.   Question  How can we print the first 10 non-header lines (those that don\u2019t start\nwith a \u2019#\u2019)?   Hint Hint:  man grep  (remember to use pipes  | )  Answer grep -v  ^#  1000gp.vcf | head     Question  How many lines of data are in the file (rather than counting the number\nof header lines and subtracting, try just counting the number of data\nlines)?   Answer grep -v  ^#  1000gp.vcf | wc -l (should print 45024)  Where these differences are located can be important. If all the\ndifferences between two encyclopedias were in just the first volume,\nthat would be interesting. The first field of each data line is the name\nof the chromosome that the difference occurs on (which volume we\u2019re on).   Question  Print the first 10 chromosomes, one per line.   Hint Hint:  man cut  (remember to remove header lines first)  Answer grep -v  ^#  1000gp.vcf | cut -f 1 | head  As you should have observed, the first 10 lines are on numbered\nchromosomes. Every normal cell in your body has 23 pairs of chromosomes,\n22 pairs of \u2018autosomal\u2019 chromosomes (these are numbered 1-22) and a pair\nof sex chromosomes (two Xs if you\u2019re female, an X and a Y if you\u2019re\nmale).  Let\u2019s look at which chromosomes these variations are on.   Question  Print a list of the chromosomes that are in the file (each chromosome\nname should only be printed once, so you should only print 23 lines).   Hint Hint: remove all duplicates from your previous answer ( man sort )  Answer grep -v  ^#  1000gp.vcf | cut -f 1 | sort -u  Rather than using  sort  to print unique results, a common pipeline is\nto first sort and then pipe to another UNIX command,  uniq . The  uniq \ncommand takes sorted input and prints only unique lines, but it provides\nmore flexibility than just using sort by itself. Keep in mind, if the\ninput isn\u2019t sorted,  uniq  won\u2019t work properly.   Question  Using  sort  and  uniq , print the number of times each chromosome\noccurs in the file.   Hint Hint:  man uniq  Answer grep -v  ^#  1000gp.vcf | cut -f 1 | sort | uniq -c   Question  Add to your previous solution to list the chromosomes from most\nfrequently observed to least frequently observed.   Hint Hint: Make sure you\u2019re sorting in descending order. By default, sort\nsorts in ascending order.  Answer grep -v  ^#  1000gp.vcf | cut -f 1 | sort | uniq -c | sort -n -r  This is great, but biologists might also like to see the chromosomes\nordered by their number (not dictionary order), since different\nchromosomes have different attributes and this ordering allows them to\nfind a specific chromosome more easily.   Question  Sort the previous output by chromosome number   Hint Hint: A lot of the power of sort comes from the fact that you can\nspecify which fields to sort on, and the order in which to sort them. In\nthis case you only need to sort on one field.  Answer grep -v  ^#  1000gp.vcf | cut -f 1 | sort | uniq -c | sort -k 2n", 
            "title": "Shell Exercise"
        }, 
        {
            "location": "/modules/btp-module-ngs-qc/ngs-qc/", 
            "text": "Key Learning Outcomes\n\n\nAfter completing this practical the trainee should be able to:\n\n\n\n\n\n\nAssess the overall quality of NGS (FastQ format) sequence reads\n\n\n\n\n\n\nVisualise the quality, and other associated matrices, of reads to\n    decide on filters and cutoffs for cleaning up data ready for\n    downstream analysis\n\n\n\n\n\n\nClean up adaptors and pre-process the sequence data for further\n    analysis\n\n\n\n\n\n\nResources You\u2019ll be Using\n\n\nTools Used\n\n\nFastQC:\n\n\nhttp://www.bioinformatics.babraham.ac.uk/projects/fastqc/\n\n\nSkewer:\n\n\nhttp://sourceforge.net/projects/skewer/\n\n\nFASTX-Toolkit:\n\n\nhttp://hannonlab.cshl.edu/fastx_toolkit/\n\n\nUseful Links\n\n\nFASTQ Encoding\n\n\n\n\nAuthor Information\n\n\nPrimary Author(s):\n  \n\nSonika Tyagi \n \n\n\nContributor(s):\n  \n\nNandan Deshpande \n\n\n\n\nIntroduction\n\n\nGoing on a blind date with your read set? For a better understanding of\nthe consequences please check the data quality!\n\n\nFor the purpose of this tutorial we are focusing only on Illumina\nsequencing which uses \u2019sequence by synthesis\u2019 technology in a highly\nparallel fashion. Although Illumina high throughput sequencing provides\nhighly accurate sequence data, several sequence artifacts, including\nbase calling errors and small insertions/deletions, poor quality reads\nand primer/adapter contamination are quite common in the high throughput\nsequencing data. The primary errors are substitution errors. The error\nrates can vary from 0.5-2.0% with errors mainly rising in frequency at\nthe 3\u2019 ends of reads.\n\n\nOne way to investigate sequence data quality is to visualize the quality\nscores and other metrics in a compact manner to get an idea about the\nquality of a read data set. Read data sets can be improved by pre\nprocessing in different ways like trimming off low quality bases,\ncleaning up any sequencing adapters, removing PCR duplicates and\nscreening for contamination. We can also look at other statistics such\nas, sequence length distribution, base composition, sequence complexity,\npresence of ambiguous bases etc. to assess the overall quality of the\ndata set.\n\n\nHighly redundant coverage (\n15X) of the genome can be used to correct\nsequencing errors in the reads before assembly. Various k-mer based\nerror correction methods exist but are beyond the scope of this\ntutorial.\n\n\nQuality Value Encoding Schema\n\n\nQuality scoring calculates a set of predictors for each base call, and then uses the predictor values to look up the Q-score in a quality table. Quality tables are created to provide optimally accurate quality predictions for runs generated by a specific configuration of sequencing platform and version of chemistry [\nwww.illumina.com\n].\nIn order to use a single character to encode Phred qualities, ASCII\ncharacters are used\n(\nhttp://shop.alterlinks.com/ascii-table/ascii-table-us.php\n). All ASCII\ncharacters have a decimal number associated with them but the first 32\ncharacters are non-printable (e.g. backspace, shift, return, escape).\nTherefore, the first printable ASCII character is number 33, the\nexclamation mark (!). In Phred+33 encoded quality values the exclamation\nmark takes the Phred quality score of zero.\n\n\nEarly Solexa (now Illumina) sequencing needed to encode negative quality\nvalues. Because ASCII characters \n 33 are non-printable, using the\nPhred+33 encoding was not possible. Therefore, they simply moved the\noffset from 33 to 64 thus inventing the Phred+64 encoded quality values.\nIn this encoding a Phred quality of zero is denoted by the ASCII number\n64 (the @ character). Since Illumina 1.8, quality values are now encoded\nusing Phred+33.\n\n\nFASTQ does not provide a way to describe what quality encoding is used\nfor the quality values. Therefore, you should find this out from your\nsequencing provider. Alternatively, you may be able to figure this out\nby determining what ASCII characters are present in the FASTQ file. E.g\nthe presence of numbers in the quality strings, can only mean the\nquality values are Phred+33 encoded. However, due to the overlapping\nnature of the Phred+33 and Phred+64 encoding schema it is not always\npossible to identify what encoding is in use. For example, if the only\ncharacters seen in the quality string are (\n@ABCDEFGHI\n), then it is\nimpossible to know if you have really good Phred+33 encoded qualities or\nreally bad Phred+64 encoded qualities.\n\n\nFor a graphical representation of the different ASCII characters used in\nthe two encoding schema see:\n\nhttp://en.wikipedia.org/wiki/FASTQ_format#Encoding\n.\n\n\nQ-score encoding implemented with the Novaseq platform\n\n\nIn order to reduce the data footrpints Illumina has come up with a new\nmethod to reduce quality score resolution and optimise data storae. The new Q-score \nencoding now follows an 8 level mapping of individual quality scores (0-40 or \n40) [See Table 1].\nWith the new scoring scheme the original scores 20-24 may form one bin and the quality scores in that \nbin mapped to a new value of 22. This can be thought of as simply replacing all the \noccurrences of scores 20, 21, 23, 24 with a new score of 22 in the output sequence.\nIllumina claims that with the new Q-scoring system the reduction in the Illumina raw sequence format (.bcl) is typically \n 50% and the resulting sorted BAM  les are reduced by ~30%.\n\n\n\n\n\n\n\n\nQuality Score Bins\n\n\nMapped quality scores\n\n\n\n\n\n\n\n\n\n\nN (no call)\n\n\nN (no call)\n\n\n\n\n\n\n2-9\n\n\n6\n\n\n\n\n\n\n10-19\n\n\n15\n\n\n\n\n\n\n20-24\n\n\n22\n\n\n\n\n\n\n25-29\n\n\n27\n\n\n\n\n\n\n30-34\n\n\n33\n\n\n\n\n\n\n35-39\n\n\n37\n\n\n\n\n\n\n=40\n\n\n40\n\n\n\n\n\n\nTable 1: Novaseq Q-score bins mapping\n\n\n\n\n\n\n\n\n\n\nPrepare the Environment\n\n\nTo investigate sequence data quality we will demonstrate tools called\nFastQC and Skewer. FastQC will process and present the reports in a\nvisual manner. Based on the results, the sequence data can be processed\nusing the Skewer. We will use one data set in this practical, which can\nbe found in the QC directory on your desktop.\n\n\nOpen the Terminal and go to the directory where the data are stored:\n\n\n1\n2\n3\n4\ncd\nls\ncd qc\npwd\n\n\n\n\n\n\nAt any time, help can be displayed for FastQC using the following\ncommand:\n\n\n1\nfastqc -h\n\n\n\n\n\n\nLook at SYNOPSIS (Usage) and options after typing fastqc -h\n\n\nQuality Visualisation\n\n\nWe have a file for a good quality and bad quality statistics. FastQC\ngenerates results in the form of a zipped and unzipped directory for\neach input file.\n\n\nExecute the following command on the two files:\n\n\n1\n2\nfastqc -f fastq qcdemo_R1.fastq.gz\nfastqc -f fastq qcdemo_R2.fastq.gz\n\n\n\n\n\n\nView the FastQC report file of the bad data using a web browser such as\nfirefox. The \u2019\n\u2019 sign puts the job in the background.\n\n\n1\nfirefox qcdemo_R2_fastqc.html \n\n\n\n\n\n\n\nThe report file will have a Basic Statistics table and various graphs\nand tables for different quality statistics. E.g.:\n\n\n\n\n\n\n\n\nProperty\n\n\nValue\n\n\n\n\n\n\n\n\n\n\nFilename\n\n\nqcdemo_R2.fastq.gz\n\n\n\n\n\n\nFile type\n\n\nConventional base calls\n\n\n\n\n\n\nEncoding\n\n\nSanger / Illumina 1.9\n\n\n\n\n\n\nTotal Sequences\n\n\n1000000\n\n\n\n\n\n\nFiltered Sequences\n\n\n0\n\n\n\n\n\n\nSequence length\n\n\n150\n\n\n\n\n\n\n%GC\n\n\n37\n\n\n\n\n\n\n\n\nTable 2: Summary statistics for bad_example_untrimmed\n\n\n\n\nFigure 1:bad_example_untrimmed_QC_plot\n\n\nA Phred quality score (or Q-score) expresses an error probability. In\nparticular, it serves as a convenient and compact way to communicate\nvery small error probabilities. The probability that base A is wrong\n(P(A)) is expressed by a quality score, Q(A), according to the\nrelationship:\n\n\n1\nQ(A) =-10 log10(P(A))\n\n\n\n\n\n\nThe relationship between the quality score and error probability is\ndemonstrated with the following table:\n\n\n\n\n\n\n\n\nQuality score, Q(A)\n\n\nError probability, P(A)\n\n\nAccuracy of base call\n\n\n\n\n\n\n\n\n\n\n10\n\n\n0.1\n\n\n90%\n\n\n\n\n\n\n20\n\n\n0.01\n\n\n99%\n\n\n\n\n\n\n30\n\n\n0.001\n\n\n99.9%\n\n\n\n\n\n\n40\n\n\n0.0001\n\n\n99.99%\n\n\n\n\n\n\n50\n\n\n0.00001\n\n\n99.999%\n\n\n\n\n\n\n\n\nTable 3: Quality Error Probabilities\n\n\n\n\nQuestion\n\n\nHow many sequences were there in your file? What is the read length?\n\n\n\n\nAnswer\n1,000,000. read length=150bp\n\n\n\n\nQuestion\n\n\nDoes the quality score values vary throughout the read length? \n\n\n\n\nHint\nlook at the \u2019per base sequence quality plot\u2019\n\n\nAnswer\nYes. Quality scores are dropping towards the end of the reads.\n\n\n\n\nQuestion\n\n\nWhat is the quality score range you see?\n\n\n\n\nAnswer\n2-40\n\n\n\n\nQuestion\n\n\nAt around which position do the scores start falling below Q20 for the 25% quartile range (25%of reads below Q20)?\n\n\n\n\nAnswer\nAround 30 bp position\n\n\n\n\nQuestion\n\n\nHow can we trim the reads to filter out the low quality data?\n\n\n\n\nAnswer\nBy trimming off the bases after a fixed position of the read or by trimming off bases based on the quality score.\n\n\nGood Quality Data\n\n\nView the FastQC report files \nfastqc_report.html\n to see examples of a\ngood quality data and compare the quality plot with that of the\n\nbad_example_fastqc\n.\n\n\n1\nfirefox qcdemo_R1_fastqc.html \n\n\n\n\n\n\n\nSequencing errors can complicate the downstream analysis, which normally\nrequires that reads be aligned to each other (for genome assembly) or to\na reference genome (for detection of mutations). Sequence reads\ncontaining errors may lead to ambiguous paths in the assembly or\nimproper gaps. In variant analysis projects sequence reads are aligned\nagainst the reference genome. The errors in the reads may lead to more\nmismatches than expected from mutations alone. But if these errors can\nbe removed or corrected, the read alignments and hence the variant\ndetection will improve. The assemblies will also improve after\npre-processing the reads to remove errors.\n\n\nRead Trimming\n\n\nRead trimming can be done in a variety of different ways. Choose a\nmethod which best suits your data. Here we are giving examples of\nfixed-length trimming and quality-based trimming.\n\n\nQuality Based Trimming\n\n\nBase call quality scores can be used to dynamically determine the trim\npoints for each read. A quality score threshold and minimum read length\nfollowing trimming can be used to remove low quality data.\n\n\nThe previous FastQC results show R1 is fine but R2 has low quality at\nthe end. There is no adaptor contamination though. We will be using\nSkewer to perform the quality trimming.\n\n\nRun the following command to quality trim a set of paired end data.\n\n\n1\n2\ncd /home/trainee/qc\nskewer -t 4 -l 50  -q 30 -Q 25 -m pe -o qcdemo qcdemo_R1.fastq.gz qcdemo_R2.fastq.gz\n\n\n\n\n\n\n-t\n:   number of threads to use\n\n\n-l\n:   min length to keep after trimming\n\n\n-q\n:   Quality threshold used for trimming at 3\u2019 end\n\n\n-Q\n:   mean quality threshold for a read\n\n\n-m\n:   pair-end mode\n\n\nRun FastQC on the quality trimmed file and visualise the quality scores.\n\n\nLook at the last files generated, are the file names same as the input ?\n\n\n1\nls -ltr\n\n\n\n\n\n\nRun Fastqc on the quality trimmed files:\n\n\n1\n2\nfastqc -f fastq qcdemo-trimmed-pair1.fastq\nfastqc -f fastq qcdemo-trimmed-pair2.fastq\n\n\n\n\n\n\nVisualise the fastqc results:\n\n\n1\n2\nfirefox qcdemo-trimmed-pair1_fastqc.html \n\nfirefox qcdemo-trimmed-pair2_fastqc.html \n\n\n\n\n\n\n\nLet\u2019s look at the quality from the second reads. The output should look\nlike:\n\n\n\n\n\n\n\n\nProperty\n\n\nValue\n\n\n\n\n\n\n\n\n\n\nFilename\n\n\nqcdemo-trimmed-pair2.fastq\n\n\n\n\n\n\nFile type\n\n\nConventional base calls\n\n\n\n\n\n\nEncoding\n\n\nSanger / Illumina 1.9\n\n\n\n\n\n\nTotal Sequences\n\n\n742262\n\n\n\n\n\n\nFiltered Sequences\n\n\n0\n\n\n\n\n\n\nSequence length\n\n\n50-150\n\n\n\n\n\n\n%GC\n\n\n37\n\n\n\n\n\n\n\n\nTable 4:Summary Statistics of QC_demo_R1_trimmed\n\n\n\n\nFigure 2:bad_example_quality_trimmed_plot\n\n\nDid the number of total reads in R1 and R2 change after trimming?\n\n\nQuality trimming discarded \n25000 reads. However, We retain a lot of\nmaximal length reads which have good quality all the way to the ends.\n\n\nWhat reads lengths were obtained after quality based trimming?\n\n\n50-150\n\n\nReads \n50 bp, following quality trimming, were discarded.\n\n\n\n\nQuestion\n\n\nDid you observe adapter sequences in the data?\n\n\nAnswer\nNo. (Hint: look at the overrepresented sequences)    \n\n\n\n\nQuestion\n\n\nHow can you use -a option with fastqc? (Hint: try fastqc -h).\n\n\nAnswer\nAdaptors can be supplied in a file for screening.\n\n\nAdapter Clipping\n\n\nSometimes sequence reads may end up getting the leftover of adapters and\nprimers used in the sequencing process. It\u2019s good practice to screen\nyour data for these possible contamination for more sensitive alignment\nand assembly based analysis.\n\n\nThis is particularly important when read lengths can be longer than the\nmolecules being sequenced. For example when sequencing miRNAs.\n\n\nVarious QC tools are available to screen and/or clip these\nadapter/primer sequences from your data. Apart from skewer which will be\nusing today the following two tools are also useful for trimming and\nremoving adapter sequence.\n\n\nCutadapt \nhttp://code.google.com/p/cutadapt/\n\n\nTrimmomatic \nhttp://www.usadellab.org/cms/?page=trimmomatic\n\n\nHere we are demonstrating \nSkewer\n to trim a given adapter sequence.\n\n\n1\n2\n3\n4\ncd /home/trainee/qc\nfastqc -f fastq  adaptorQC.fastq.gz\nfirefox adaptorQC_fastqc.html\nskewer -x TGGAATTCTCGGGTGCCAAGGT -t 20 -l 10 -L 35 -q 30 adaptorQC.fastq.gz\n\n\n\n\n\n\n-x\n:   adaptor sequence used\n\n\n-t\n:   number of threads to use\n\n\n-l\n:   min length to keep after trimming\n\n\n-L\n:   Max length to keep after trimming, in this experiment we were\n    expecting only small RNA fragments\n\n\n-Q\n:   Quality threshold used for trimming at 3\u2019 end. Use -m option to\n    control the end you want to trim\n\n\nRun FastQC on the adapter trimmed file and visualise the quality scores.\nFastqc now shows adaptor free results.\n\n\n1\n2\nfastqc adaptorQC.fastq-trimmed.fastq\nfirefox adaptorQC.fastq-trimmed_fastqc.html \n\n\n\n\n\n\n\nFixed Length Trimming\n\n\nWe will not cover Fixed Length Trimming but provide the following for\nyour information.\n Low quality read ends can be trimmed using a\nfixed-length trimming. We will use the \nfastx_trimmer\n from the\nFASTX-Toolkit. Usage message to find out various options you can use\nwith this tool. Type \nfastx_trimmer -h\n at anytime to display help.\n\n\nWe will now do fixed-length trimming of the \nbad_example.fastq\n file\nusing the following command. You should still be in the qc directory, if\nnot cd back in.\n\n\n1\n2\n3\n4\ncd /home/trainee/qc\nfastqc -f fastq bad_example.fastq\nfastx_trimmer -h\nfastx_trimmer -Q 33 -f 1 -l 80 -i bad_example.fastq -o bad_example_trimmed01.fastq\n\n\n\n\n\n\nWe used the following options in the command above:\n\n\n-Q 33\n:   Indicates the input quality scores are Phred+33 encoded\n\n\n-f\n:   First base to be retained in the output\n\n\n-l\n:   Last base to be retained in the output\n\n\n-i\n:   Input FASTQ file name\n\n\n-o\n:   Output file name\n\n\nRun FastQC on the trimmed file and visualise the quality scores of the\ntrimmed file.\n\n\n1\n2\nfastqc -f fastq bad_example_trimmed01.fastq\nfirefox bad_example_trimmed01_fastqc.html \n\n\n\n\n\n\n\nThe output should look like:\n\n\n\n\n\n\n\n\nProperty\n\n\nValue\n\n\n\n\n\n\n\n\n\n\nFilename\n\n\nbad_example_trimmed01.fastq\n\n\n\n\n\n\nFile type\n\n\nConventional base calls\n\n\n\n\n\n\nEncoding\n\n\nSanger / Illumina 1.9\n\n\n\n\n\n\nTotal Sequences\n\n\n40000\n\n\n\n\n\n\nFiltered Sequences\n\n\n0\n\n\n\n\n\n\nSequence length\n\n\n80\n\n\n\n\n\n\n%GC\n\n\n48\n\n\n\n\n\n\n\n\nTable 5:Summary Statistics of bad_example_trimmed summary \n\n\n\n\nFigure 3: bad_example_trimmed_plot\n\n\nWhat values would you use for \n-f\n if you wanted to trim off 10 bases at\nthe 5\u2019 end of the reads?\n\n\n-f 11", 
            "title": "Data Quality"
        }, 
        {
            "location": "/modules/btp-module-ngs-qc/ngs-qc/#key-learning-outcomes", 
            "text": "After completing this practical the trainee should be able to:    Assess the overall quality of NGS (FastQ format) sequence reads    Visualise the quality, and other associated matrices, of reads to\n    decide on filters and cutoffs for cleaning up data ready for\n    downstream analysis    Clean up adaptors and pre-process the sequence data for further\n    analysis", 
            "title": "Key Learning Outcomes"
        }, 
        {
            "location": "/modules/btp-module-ngs-qc/ngs-qc/#resources-youll-be-using", 
            "text": "", 
            "title": "Resources You\u2019ll be Using"
        }, 
        {
            "location": "/modules/btp-module-ngs-qc/ngs-qc/#tools-used", 
            "text": "FastQC:  http://www.bioinformatics.babraham.ac.uk/projects/fastqc/  Skewer:  http://sourceforge.net/projects/skewer/  FASTX-Toolkit:  http://hannonlab.cshl.edu/fastx_toolkit/", 
            "title": "Tools Used"
        }, 
        {
            "location": "/modules/btp-module-ngs-qc/ngs-qc/#useful-links", 
            "text": "FASTQ Encoding", 
            "title": "Useful Links"
        }, 
        {
            "location": "/modules/btp-module-ngs-qc/ngs-qc/#author-information", 
            "text": "Primary Author(s):    \nSonika Tyagi     Contributor(s):    \nNandan Deshpande", 
            "title": "Author Information"
        }, 
        {
            "location": "/modules/btp-module-ngs-qc/ngs-qc/#introduction", 
            "text": "Going on a blind date with your read set? For a better understanding of\nthe consequences please check the data quality!  For the purpose of this tutorial we are focusing only on Illumina\nsequencing which uses \u2019sequence by synthesis\u2019 technology in a highly\nparallel fashion. Although Illumina high throughput sequencing provides\nhighly accurate sequence data, several sequence artifacts, including\nbase calling errors and small insertions/deletions, poor quality reads\nand primer/adapter contamination are quite common in the high throughput\nsequencing data. The primary errors are substitution errors. The error\nrates can vary from 0.5-2.0% with errors mainly rising in frequency at\nthe 3\u2019 ends of reads.  One way to investigate sequence data quality is to visualize the quality\nscores and other metrics in a compact manner to get an idea about the\nquality of a read data set. Read data sets can be improved by pre\nprocessing in different ways like trimming off low quality bases,\ncleaning up any sequencing adapters, removing PCR duplicates and\nscreening for contamination. We can also look at other statistics such\nas, sequence length distribution, base composition, sequence complexity,\npresence of ambiguous bases etc. to assess the overall quality of the\ndata set.  Highly redundant coverage ( 15X) of the genome can be used to correct\nsequencing errors in the reads before assembly. Various k-mer based\nerror correction methods exist but are beyond the scope of this\ntutorial.", 
            "title": "Introduction"
        }, 
        {
            "location": "/modules/btp-module-ngs-qc/ngs-qc/#quality-value-encoding-schema", 
            "text": "Quality scoring calculates a set of predictors for each base call, and then uses the predictor values to look up the Q-score in a quality table. Quality tables are created to provide optimally accurate quality predictions for runs generated by a specific configuration of sequencing platform and version of chemistry [ www.illumina.com ].\nIn order to use a single character to encode Phred qualities, ASCII\ncharacters are used\n( http://shop.alterlinks.com/ascii-table/ascii-table-us.php ). All ASCII\ncharacters have a decimal number associated with them but the first 32\ncharacters are non-printable (e.g. backspace, shift, return, escape).\nTherefore, the first printable ASCII character is number 33, the\nexclamation mark (!). In Phred+33 encoded quality values the exclamation\nmark takes the Phred quality score of zero.  Early Solexa (now Illumina) sequencing needed to encode negative quality\nvalues. Because ASCII characters   33 are non-printable, using the\nPhred+33 encoding was not possible. Therefore, they simply moved the\noffset from 33 to 64 thus inventing the Phred+64 encoded quality values.\nIn this encoding a Phred quality of zero is denoted by the ASCII number\n64 (the @ character). Since Illumina 1.8, quality values are now encoded\nusing Phred+33.  FASTQ does not provide a way to describe what quality encoding is used\nfor the quality values. Therefore, you should find this out from your\nsequencing provider. Alternatively, you may be able to figure this out\nby determining what ASCII characters are present in the FASTQ file. E.g\nthe presence of numbers in the quality strings, can only mean the\nquality values are Phred+33 encoded. However, due to the overlapping\nnature of the Phred+33 and Phred+64 encoding schema it is not always\npossible to identify what encoding is in use. For example, if the only\ncharacters seen in the quality string are ( @ABCDEFGHI ), then it is\nimpossible to know if you have really good Phred+33 encoded qualities or\nreally bad Phred+64 encoded qualities.  For a graphical representation of the different ASCII characters used in\nthe two encoding schema see: http://en.wikipedia.org/wiki/FASTQ_format#Encoding .", 
            "title": "Quality Value Encoding Schema"
        }, 
        {
            "location": "/modules/btp-module-ngs-qc/ngs-qc/#q-score-encoding-implemented-with-the-novaseq-platform", 
            "text": "In order to reduce the data footrpints Illumina has come up with a new\nmethod to reduce quality score resolution and optimise data storae. The new Q-score \nencoding now follows an 8 level mapping of individual quality scores (0-40 or  40) [See Table 1].\nWith the new scoring scheme the original scores 20-24 may form one bin and the quality scores in that \nbin mapped to a new value of 22. This can be thought of as simply replacing all the \noccurrences of scores 20, 21, 23, 24 with a new score of 22 in the output sequence.\nIllumina claims that with the new Q-scoring system the reduction in the Illumina raw sequence format (.bcl) is typically   50% and the resulting sorted BAM  les are reduced by ~30%.     Quality Score Bins  Mapped quality scores      N (no call)  N (no call)    2-9  6    10-19  15    20-24  22    25-29  27    30-34  33    35-39  37    =40  40    Table 1: Novaseq Q-score bins mapping", 
            "title": "Q-score encoding implemented with the Novaseq platform"
        }, 
        {
            "location": "/modules/btp-module-ngs-qc/ngs-qc/#prepare-the-environment", 
            "text": "To investigate sequence data quality we will demonstrate tools called\nFastQC and Skewer. FastQC will process and present the reports in a\nvisual manner. Based on the results, the sequence data can be processed\nusing the Skewer. We will use one data set in this practical, which can\nbe found in the QC directory on your desktop.  Open the Terminal and go to the directory where the data are stored:  1\n2\n3\n4 cd\nls\ncd qc\npwd   At any time, help can be displayed for FastQC using the following\ncommand:  1 fastqc -h   Look at SYNOPSIS (Usage) and options after typing fastqc -h", 
            "title": "Prepare the Environment"
        }, 
        {
            "location": "/modules/btp-module-ngs-qc/ngs-qc/#quality-visualisation", 
            "text": "We have a file for a good quality and bad quality statistics. FastQC\ngenerates results in the form of a zipped and unzipped directory for\neach input file.  Execute the following command on the two files:  1\n2 fastqc -f fastq qcdemo_R1.fastq.gz\nfastqc -f fastq qcdemo_R2.fastq.gz   View the FastQC report file of the bad data using a web browser such as\nfirefox. The \u2019 \u2019 sign puts the job in the background.  1 firefox qcdemo_R2_fastqc.html     The report file will have a Basic Statistics table and various graphs\nand tables for different quality statistics. E.g.:     Property  Value      Filename  qcdemo_R2.fastq.gz    File type  Conventional base calls    Encoding  Sanger / Illumina 1.9    Total Sequences  1000000    Filtered Sequences  0    Sequence length  150    %GC  37     Table 2: Summary statistics for bad_example_untrimmed   Figure 1:bad_example_untrimmed_QC_plot  A Phred quality score (or Q-score) expresses an error probability. In\nparticular, it serves as a convenient and compact way to communicate\nvery small error probabilities. The probability that base A is wrong\n(P(A)) is expressed by a quality score, Q(A), according to the\nrelationship:  1 Q(A) =-10 log10(P(A))   The relationship between the quality score and error probability is\ndemonstrated with the following table:     Quality score, Q(A)  Error probability, P(A)  Accuracy of base call      10  0.1  90%    20  0.01  99%    30  0.001  99.9%    40  0.0001  99.99%    50  0.00001  99.999%     Table 3: Quality Error Probabilities   Question  How many sequences were there in your file? What is the read length?   Answer 1,000,000. read length=150bp   Question  Does the quality score values vary throughout the read length?    Hint look at the \u2019per base sequence quality plot\u2019  Answer Yes. Quality scores are dropping towards the end of the reads.   Question  What is the quality score range you see?   Answer 2-40   Question  At around which position do the scores start falling below Q20 for the 25% quartile range (25%of reads below Q20)?   Answer Around 30 bp position   Question  How can we trim the reads to filter out the low quality data?   Answer By trimming off the bases after a fixed position of the read or by trimming off bases based on the quality score.", 
            "title": "Quality Visualisation"
        }, 
        {
            "location": "/modules/btp-module-ngs-qc/ngs-qc/#good-quality-data", 
            "text": "View the FastQC report files  fastqc_report.html  to see examples of a\ngood quality data and compare the quality plot with that of the bad_example_fastqc .  1 firefox qcdemo_R1_fastqc.html     Sequencing errors can complicate the downstream analysis, which normally\nrequires that reads be aligned to each other (for genome assembly) or to\na reference genome (for detection of mutations). Sequence reads\ncontaining errors may lead to ambiguous paths in the assembly or\nimproper gaps. In variant analysis projects sequence reads are aligned\nagainst the reference genome. The errors in the reads may lead to more\nmismatches than expected from mutations alone. But if these errors can\nbe removed or corrected, the read alignments and hence the variant\ndetection will improve. The assemblies will also improve after\npre-processing the reads to remove errors.", 
            "title": "Good Quality Data"
        }, 
        {
            "location": "/modules/btp-module-ngs-qc/ngs-qc/#read-trimming", 
            "text": "Read trimming can be done in a variety of different ways. Choose a\nmethod which best suits your data. Here we are giving examples of\nfixed-length trimming and quality-based trimming.", 
            "title": "Read Trimming"
        }, 
        {
            "location": "/modules/btp-module-ngs-qc/ngs-qc/#quality-based-trimming", 
            "text": "Base call quality scores can be used to dynamically determine the trim\npoints for each read. A quality score threshold and minimum read length\nfollowing trimming can be used to remove low quality data.  The previous FastQC results show R1 is fine but R2 has low quality at\nthe end. There is no adaptor contamination though. We will be using\nSkewer to perform the quality trimming.  Run the following command to quality trim a set of paired end data.  1\n2 cd /home/trainee/qc\nskewer -t 4 -l 50  -q 30 -Q 25 -m pe -o qcdemo qcdemo_R1.fastq.gz qcdemo_R2.fastq.gz   -t\n:   number of threads to use  -l\n:   min length to keep after trimming  -q\n:   Quality threshold used for trimming at 3\u2019 end  -Q\n:   mean quality threshold for a read  -m\n:   pair-end mode  Run FastQC on the quality trimmed file and visualise the quality scores.  Look at the last files generated, are the file names same as the input ?  1 ls -ltr   Run Fastqc on the quality trimmed files:  1\n2 fastqc -f fastq qcdemo-trimmed-pair1.fastq\nfastqc -f fastq qcdemo-trimmed-pair2.fastq   Visualise the fastqc results:  1\n2 firefox qcdemo-trimmed-pair1_fastqc.html  \nfirefox qcdemo-trimmed-pair2_fastqc.html     Let\u2019s look at the quality from the second reads. The output should look\nlike:     Property  Value      Filename  qcdemo-trimmed-pair2.fastq    File type  Conventional base calls    Encoding  Sanger / Illumina 1.9    Total Sequences  742262    Filtered Sequences  0    Sequence length  50-150    %GC  37     Table 4:Summary Statistics of QC_demo_R1_trimmed   Figure 2:bad_example_quality_trimmed_plot  Did the number of total reads in R1 and R2 change after trimming?  Quality trimming discarded  25000 reads. However, We retain a lot of\nmaximal length reads which have good quality all the way to the ends.  What reads lengths were obtained after quality based trimming?  50-150  Reads  50 bp, following quality trimming, were discarded.   Question  Did you observe adapter sequences in the data?  Answer No. (Hint: look at the overrepresented sequences)       Question  How can you use -a option with fastqc? (Hint: try fastqc -h).  Answer Adaptors can be supplied in a file for screening.", 
            "title": "Quality Based Trimming"
        }, 
        {
            "location": "/modules/btp-module-ngs-qc/ngs-qc/#adapter-clipping", 
            "text": "Sometimes sequence reads may end up getting the leftover of adapters and\nprimers used in the sequencing process. It\u2019s good practice to screen\nyour data for these possible contamination for more sensitive alignment\nand assembly based analysis.  This is particularly important when read lengths can be longer than the\nmolecules being sequenced. For example when sequencing miRNAs.  Various QC tools are available to screen and/or clip these\nadapter/primer sequences from your data. Apart from skewer which will be\nusing today the following two tools are also useful for trimming and\nremoving adapter sequence.  Cutadapt  http://code.google.com/p/cutadapt/  Trimmomatic  http://www.usadellab.org/cms/?page=trimmomatic  Here we are demonstrating  Skewer  to trim a given adapter sequence.  1\n2\n3\n4 cd /home/trainee/qc\nfastqc -f fastq  adaptorQC.fastq.gz\nfirefox adaptorQC_fastqc.html\nskewer -x TGGAATTCTCGGGTGCCAAGGT -t 20 -l 10 -L 35 -q 30 adaptorQC.fastq.gz   -x\n:   adaptor sequence used  -t\n:   number of threads to use  -l\n:   min length to keep after trimming  -L\n:   Max length to keep after trimming, in this experiment we were\n    expecting only small RNA fragments  -Q\n:   Quality threshold used for trimming at 3\u2019 end. Use -m option to\n    control the end you want to trim  Run FastQC on the adapter trimmed file and visualise the quality scores.\nFastqc now shows adaptor free results.  1\n2 fastqc adaptorQC.fastq-trimmed.fastq\nfirefox adaptorQC.fastq-trimmed_fastqc.html", 
            "title": "Adapter Clipping"
        }, 
        {
            "location": "/modules/btp-module-ngs-qc/ngs-qc/#fixed-length-trimming", 
            "text": "We will not cover Fixed Length Trimming but provide the following for\nyour information.  Low quality read ends can be trimmed using a\nfixed-length trimming. We will use the  fastx_trimmer  from the\nFASTX-Toolkit. Usage message to find out various options you can use\nwith this tool. Type  fastx_trimmer -h  at anytime to display help.  We will now do fixed-length trimming of the  bad_example.fastq  file\nusing the following command. You should still be in the qc directory, if\nnot cd back in.  1\n2\n3\n4 cd /home/trainee/qc\nfastqc -f fastq bad_example.fastq\nfastx_trimmer -h\nfastx_trimmer -Q 33 -f 1 -l 80 -i bad_example.fastq -o bad_example_trimmed01.fastq   We used the following options in the command above:  -Q 33\n:   Indicates the input quality scores are Phred+33 encoded  -f\n:   First base to be retained in the output  -l\n:   Last base to be retained in the output  -i\n:   Input FASTQ file name  -o\n:   Output file name  Run FastQC on the trimmed file and visualise the quality scores of the\ntrimmed file.  1\n2 fastqc -f fastq bad_example_trimmed01.fastq\nfirefox bad_example_trimmed01_fastqc.html     The output should look like:     Property  Value      Filename  bad_example_trimmed01.fastq    File type  Conventional base calls    Encoding  Sanger / Illumina 1.9    Total Sequences  40000    Filtered Sequences  0    Sequence length  80    %GC  48     Table 5:Summary Statistics of bad_example_trimmed summary    Figure 3: bad_example_trimmed_plot  What values would you use for  -f  if you wanted to trim off 10 bases at\nthe 5\u2019 end of the reads?  -f 11", 
            "title": "Fixed Length Trimming"
        }, 
        {
            "location": "/modules/btp-module-ngs-mapping/alignment/", 
            "text": "Key Learning Outcomes\n\n\nAfter completing this practical the trainee should be able to:\n\n\n\n\n\n\nPerform a simple NGS data alignment task, with Bowtie2, against one\n    interested reference data\n\n\n\n\n\n\nInterpret and manipulate the mapping output using SAMtools\n\n\n\n\n\n\nVisualise the alignment via a standard genome browser, e.g. IGV\n    browser\n\n\n\n\n\n\nResources You\u2019ll be Using\n\n\nTools Used\n\n\nBowtie2:\n\n\nhttp://bowtie-bio.sourceforge.net/bowtie2/index.shtml\n\n\nSamtools:\n\n\nhttp://broadinstitute.github.io/picard\n\n\nBEDTools:\n\n\nhttp://code.google.com/p/bedtools/\n\n\nUCSC tools:\n\n\nhttp://hgdownload.cse.ucsc.edu/admin/exe/\n\n\nIGV genome browser:\n\n\nhttp://www.broadinstitute.org/igv/\n\n\nUseful Links\n\n\nSAM Specification:\n\n\nhttp://samtools.sourceforge.net/SAM1.pdf\n\n\nExplain SAM Flags:\n\n\nhttps://broadinstitute.github.io/picard/explain-flags.html\n\n\nSources of Data\n\n\nhttp://www.ebi.ac.uk/arrayexpress/experiments/E-GEOD-11431\n\n\nAuthor Information\n\n\nPrimary Author(s):\n\n    Myrto Kostadima \n\n\nContributor(s):\n\n    Xi (Sean) Li \n\n\nIntroduction\n\n\nThe goal of this hands-on session is to perform an unspliced alignment\nfor a small subset of raw reads. We will align raw sequencing data to\nthe mouse genome using Bowtie2 and then we will manipulate the SAM\noutput in order to visualize the alignment on the IGV browser.\n\n\nPrepare the Environment\n\n\nWe will use one data set in this practical, which can be found in the\n\nChIP-seq\n directory on your desktop.\n\n\nOpen the Terminal.\n\n\nFirst, go to the right folder, where the data are stored.\n\n\n1\ncd /home/trainee/chipseq\n\n\n\n\n\n\nThe \n.fastq\n file that we will align is called \nOct4.fastq\n. This file\nis based on Oct4 ChIP-seq data published by Chen \net al.\n (2008). For\nthe sake of time, we will align these reads to a single mouse\nchromosome.\n\n\nAlignment\n\n\nYou already know that there are a number of competing tools for short\nread alignment, each with its own set of strengths, weaknesses, and\ncaveats. Here we will try Bowtie2, a widely used ultrafast, memory\nefficient short read aligner.\n\n\nBowtie2 has a number of parameters in order to perform the alignment. To\nview them all type\n\n\n1\nbowtie2 --help\n\n\n\n\n\n\nBowtie2 uses indexed genome for the alignment in order to keep its\nmemory footprint small. Because of time constraints we will build the\nindex only for one chromosome of the mouse genome. For this we need the\nchromosome sequence in FASTA format. This is stored in a file named\n\nmm10\n, under the subdirectory \nbowtie_index\n.\n\n\n\n\nSTOP\n\n\nDO NOT run this command. This has already been run for you.\n\n\n** bowtie2-build bowtie_index/mm10.fa bowtie_index/mm10 **\n\n\n\n\nThis command will output 6 files that constitute the index. These files\nthat have the prefix \nmm10\n are stored in the \nbowtie_index\n\nsubdirectory. To view if they files have been successfully created type:\n\n\n1\nls -l bowtie_index\n\n\n\n\n\n\nNow that the genome is indexed we can move on to the actual alignment.\nThe first argument for \nbowtie2\n is the basename of the index for the\ngenome to be searched; in our case this is \nmm10\n. We also want to make\nsure that the output is in SAM format using the \n-S\n parameter. The last\nargument is the name of the FASTQ file.\n\n\nAlign the Oct4 reads using Bowtie2:\n\n\n1\nbowtie2 -x bowtie_index/mm10 -q Oct4.fastq \n Oct4.sam\n\n\n\n\n\n\nThe above command outputs the alignment in SAM format and stores them in\nthe file \nOct4.sam\n.\n\n\nIn general before you run Bowtie2, you have to know what quality\nencoding your FASTQ files are in. The available FASTQ encodings for\nbowtie are:\n\n\n\u2013phred33-quals\n:   Input qualities are Phred+33 (default).\n\n\n\u2013phred64-quals\n:   Input qualities are Phred+64 (same as \n\u2013solexa1.3-quals\n).\n\n\n\u2013solexa-quals\n:   Input qualities are from GA Pipeline ver. \n 1.3.\n\n\n\u2013solexa1.3-quals\n:   Input qualities are from GA Pipeline ver. \n= 1.3.\n\n\n\u2013integer-quals\n:   Qualities are given as space-separated integers (not ASCII).\n\n\nThe FASTQ files we are working with are Sanger encoded (Phred+33), which\nis the default for Bowtie2.\n\n\nBowtie2 will take 2-3 minutes to align the file. This is fast compared\nto other aligners which sacrifice some speed to obtain higher\nsensitivity.\n\n\nLook at the top 10 lines of the SAM file using head (record lines are\nwrapped). Then try the second command, note use arrow navigation and to\nexit type \u2019q\u2019.\n\n\n1\n2\nhead  Oct4.sam\nless -S Oct4.sam\n\n\n\n\n\n\n\n\nQuestion\n\n\nCan you distinguish between the header of the SAM format and the actual alignments?\n\n\n\n\nAnswer\nAnswer\nThe header line starts with the letter \u2018@\u2019, i.e.:\n@HD   VN:1.0       SO:unsorted             \n\n@SQ   SN:chr1      LN:195471971            \n\n@PG   ID:Bowtie2   PN:bowtie2     VN:2.2.4   CL:\u201c/tools/bowtie2/bowtie2-default/bowtie2-align-s \u2013wrapper basic-0 -x bowtie_index/mm10 -q Oct4.fastq\u201d\nWhile, the actual alignments start with read id, i.e.:\n\n\n\n\nSRR002012.45   0    etc  \n\n  SRR002012.48   16   chr1   etc\n\n\n\n\n\n\nQuestion\n\n\nWhat kind of information does the header provide?\n\n\n\n\nAnswer\nAnswer\n@HD: Header line; VN: Format version; SO: the sort order of alignments.\n@SQ: Reference sequence information; SN: reference sequence name; LN: reference sequence length.\n@PG: Read group information; ID: Read group identifier; VN: Program version; CL: the command line that produces the alignment.\n\n\nQuestion\n\n\nTo which chromosome are the reads mapped?\n\n\n\n\nAnswer\nAnswer\nChromosome 1.\nManipulate SAM output\n\n\nSAM files are rather big and when dealing with a high volume of NGS\ndata, storage space can become an issue. As we have already seen, we can\nconvert SAM to BAM files (their binary equivalent that are not human\nreadable) that occupy much less space.\n\n\nConvert SAM to BAM using \nsamtools view\n and store the output in the\nfile \nOct4.bam\n. You have to instruct \nsamtools view\n that the input is\nin SAM format (\n-S\n), the output should be in BAM format (\n-b\n) and that\nyou want the output to be stored in the file specified by the \n-o\n\noption:\n\n\n1\nsamtools view -bSo Oct4.bam Oct4.sam\n\n\n\n\n\n\nCompute summary stats for the Flag values associated with the alignments\nusing:\n\n\n1\nsamtools flagstat Oct4.bam\n\n\n\n\n\n\nVisualize alignments in IGV\n\n\nIGV is a stand-alone genome browser. Please check their website\n(\nhttp://www.broadinstitute.org/igv/\n) for all the formats that IGV can\ndisplay. For our visualization purposes we will use the BAM and bigWig\nformats.\n\n\nWhen uploading a BAM file into the genome browser, the browser will look\nfor the index of the BAM file in the same folder where the BAM files is.\nThe index file should have the same name as the BAM file and the suffix\n\n.bai\n. Finally, to create the index of a BAM file you need to make sure\nthat the file is sorted according to chromosomal coordinates.\n\n\nSort alignments according to chromosomal position and store the result\nin the file with the prefix \nOct4.sorted\n:\n\n\n1\nsamtools sort Oct4.bam -o Oct4.sorted.bam\n\n\n\n\n\n\nIndex the sorted file.\n\n\n1\nsamtools index Oct4.sorted.bam\n\n\n\n\n\n\nThe indexing will create a file called \nOct4.sorted.bam.bai\n. Note that\nyou don\u2019t have to specify the name of the index file when running\n\nsamtools index\n, it simply appends a \n.bai\n suffix to the input BAM\nfile.\n\n\nAnother way to visualize the alignments is to convert the BAM file into\na bigWig file. The bigWig format is for display of dense, continuous\ndata and the data will be displayed as a graph. The resulting bigWig\nfiles are in an indexed binary format.\n\n\nThe BAM to bigWig conversion takes place in two steps. Firstly, we\nconvert the BAM file into a bedgraph, called \nOct4.bedgraph\n, using the\ntool \ngenomeCoverageBed\n from BEDTools. Then we convert the bedgraph\ninto a bigWig binary file called \nOct4.bw\n, using \nbedGraphToBigWig\n\nfrom the UCSC tools:\n\n\n1\n2\ngenomeCoverageBed -bg -ibam Oct4.sorted.bam -g bowtie_index/mouse.mm10.genome \n Oct4.bedgraph\nbedGraphToBigWig Oct4.bedgraph bowtie_index/mouse.mm10.genome Oct4.bw\n\n\n\n\n\n\nBoth of the commands above take as input a file called\n\nmouse.mm10.genome\n that is stored under the subdirectory\n\nbowtie_index\n. These genome files are tab-delimited and describe the\nsize of the chromosomes for the organism of interest. When using the\nUCSC Genome Browser, Ensembl, or Galaxy, you typically indicate which\nspecies/genome build you are working with. The way you do this for\nBEDTools is to create a \u201cgenome\u201d file, which simply lists the names of\nthe chromosomes (or scaffolds, etc.) and their size (in basepairs).\n\n\nBEDTools includes pre-defined genome files for human and mouse in the\n\ngenomes\n subdirectory included in the BEDTools distribution.\n\n\nNow we will load the data into the IGV browser for visualization. In\norder to launch IGV double click on the \nIGV 2.3\n icon on your Desktop.\nIgnore any warnings and when it opens you have to load the genome of\ninterest.\n\n\nOn the top left of your screen choose from the drop down menu\n\nMouse (mm10)\n. If it doesn\u2019t appear in list, click \nMore ..\n, type\n\nmm10\n in the Filter section, choose the mouse genome and press OK. Then\nin order to load the desire files go to:\n\n\n1\nFile \n Load from File\n\n\n\n\n\n\nOn the pop up window navigate to Desktop -\n chipseq folder and select\nthe file \nOct4.sorted.bam\n.\n\n\nRepeat these steps in order to load \nOct4.bw\n as well.\n\n\nSelect \nchr1\n from the drop down menu on the top left. Right click on\nthe name of \nOct4.bw\n and choose Maximum under the Windowing Function.\nRight click again and select Autoscale.\n\n\nIn order to see the aligned reads of the BAM file, you need to zoom in\nto a specific region. For example, look for gene \nLemd1\n in the search\nbox.\n\n\n\n\nQuestion\n\n\nWhat is the main difference between the visualization of BAM and bigWig files?\n\n\n\n\nAnswer\nAnswer\nThe actual alignment of reads that stack to a particular region can be displayed using the information stored in a BAM format. The bigWig format is for display of dense, continuous data that will be displayed in the Genome Browser as a graph.\nUsing the \n+\n button on the top right, zoom in to see more of the\ndetails of the alignments.\n\n\n\n\nQuestion\n\n\nWhat do you think the different colors mean?\n\n\n\n\nAnswer\nAnswer\nThe different color represents four nucleotides, e.g. blue is Cytidine (C), red is Thymidine (T).\nPractice Makes Perfect!\n\n\nIn the chipseq folder you will find the file \ngfp.fastq\n. Follow the\nabove described analysis, from the bowtie2 alignment step, for this\ndataset as well. You will need these files for the ChIP-Seq module.", 
            "title": "Read Alignment"
        }, 
        {
            "location": "/modules/btp-module-ngs-mapping/alignment/#key-learning-outcomes", 
            "text": "After completing this practical the trainee should be able to:    Perform a simple NGS data alignment task, with Bowtie2, against one\n    interested reference data    Interpret and manipulate the mapping output using SAMtools    Visualise the alignment via a standard genome browser, e.g. IGV\n    browser", 
            "title": "Key Learning Outcomes"
        }, 
        {
            "location": "/modules/btp-module-ngs-mapping/alignment/#resources-youll-be-using", 
            "text": "", 
            "title": "Resources You\u2019ll be Using"
        }, 
        {
            "location": "/modules/btp-module-ngs-mapping/alignment/#tools-used", 
            "text": "Bowtie2:  http://bowtie-bio.sourceforge.net/bowtie2/index.shtml  Samtools:  http://broadinstitute.github.io/picard  BEDTools:  http://code.google.com/p/bedtools/  UCSC tools:  http://hgdownload.cse.ucsc.edu/admin/exe/  IGV genome browser:  http://www.broadinstitute.org/igv/", 
            "title": "Tools Used"
        }, 
        {
            "location": "/modules/btp-module-ngs-mapping/alignment/#useful-links", 
            "text": "SAM Specification:  http://samtools.sourceforge.net/SAM1.pdf  Explain SAM Flags:  https://broadinstitute.github.io/picard/explain-flags.html", 
            "title": "Useful Links"
        }, 
        {
            "location": "/modules/btp-module-ngs-mapping/alignment/#sources-of-data", 
            "text": "http://www.ebi.ac.uk/arrayexpress/experiments/E-GEOD-11431", 
            "title": "Sources of Data"
        }, 
        {
            "location": "/modules/btp-module-ngs-mapping/alignment/#author-information", 
            "text": "Primary Author(s): \n    Myrto Kostadima   Contributor(s): \n    Xi (Sean) Li", 
            "title": "Author Information"
        }, 
        {
            "location": "/modules/btp-module-ngs-mapping/alignment/#introduction", 
            "text": "The goal of this hands-on session is to perform an unspliced alignment\nfor a small subset of raw reads. We will align raw sequencing data to\nthe mouse genome using Bowtie2 and then we will manipulate the SAM\noutput in order to visualize the alignment on the IGV browser.", 
            "title": "Introduction"
        }, 
        {
            "location": "/modules/btp-module-ngs-mapping/alignment/#prepare-the-environment", 
            "text": "We will use one data set in this practical, which can be found in the ChIP-seq  directory on your desktop.  Open the Terminal.  First, go to the right folder, where the data are stored.  1 cd /home/trainee/chipseq   The  .fastq  file that we will align is called  Oct4.fastq . This file\nis based on Oct4 ChIP-seq data published by Chen  et al.  (2008). For\nthe sake of time, we will align these reads to a single mouse\nchromosome.", 
            "title": "Prepare the Environment"
        }, 
        {
            "location": "/modules/btp-module-ngs-mapping/alignment/#alignment", 
            "text": "You already know that there are a number of competing tools for short\nread alignment, each with its own set of strengths, weaknesses, and\ncaveats. Here we will try Bowtie2, a widely used ultrafast, memory\nefficient short read aligner.  Bowtie2 has a number of parameters in order to perform the alignment. To\nview them all type  1 bowtie2 --help   Bowtie2 uses indexed genome for the alignment in order to keep its\nmemory footprint small. Because of time constraints we will build the\nindex only for one chromosome of the mouse genome. For this we need the\nchromosome sequence in FASTA format. This is stored in a file named mm10 , under the subdirectory  bowtie_index .   STOP  DO NOT run this command. This has already been run for you.  ** bowtie2-build bowtie_index/mm10.fa bowtie_index/mm10 **   This command will output 6 files that constitute the index. These files\nthat have the prefix  mm10  are stored in the  bowtie_index \nsubdirectory. To view if they files have been successfully created type:  1 ls -l bowtie_index   Now that the genome is indexed we can move on to the actual alignment.\nThe first argument for  bowtie2  is the basename of the index for the\ngenome to be searched; in our case this is  mm10 . We also want to make\nsure that the output is in SAM format using the  -S  parameter. The last\nargument is the name of the FASTQ file.  Align the Oct4 reads using Bowtie2:  1 bowtie2 -x bowtie_index/mm10 -q Oct4.fastq   Oct4.sam   The above command outputs the alignment in SAM format and stores them in\nthe file  Oct4.sam .  In general before you run Bowtie2, you have to know what quality\nencoding your FASTQ files are in. The available FASTQ encodings for\nbowtie are:  \u2013phred33-quals\n:   Input qualities are Phred+33 (default).  \u2013phred64-quals\n:   Input qualities are Phred+64 (same as  \u2013solexa1.3-quals ).  \u2013solexa-quals\n:   Input qualities are from GA Pipeline ver.   1.3.  \u2013solexa1.3-quals\n:   Input qualities are from GA Pipeline ver.  = 1.3.  \u2013integer-quals\n:   Qualities are given as space-separated integers (not ASCII).  The FASTQ files we are working with are Sanger encoded (Phred+33), which\nis the default for Bowtie2.  Bowtie2 will take 2-3 minutes to align the file. This is fast compared\nto other aligners which sacrifice some speed to obtain higher\nsensitivity.  Look at the top 10 lines of the SAM file using head (record lines are\nwrapped). Then try the second command, note use arrow navigation and to\nexit type \u2019q\u2019.  1\n2 head  Oct4.sam\nless -S Oct4.sam    Question  Can you distinguish between the header of the SAM format and the actual alignments?   Answer Answer The header line starts with the letter \u2018@\u2019, i.e.: @HD   VN:1.0       SO:unsorted              \n@SQ   SN:chr1      LN:195471971             \n@PG   ID:Bowtie2   PN:bowtie2     VN:2.2.4   CL:\u201c/tools/bowtie2/bowtie2-default/bowtie2-align-s \u2013wrapper basic-0 -x bowtie_index/mm10 -q Oct4.fastq\u201d While, the actual alignments start with read id, i.e.:   SRR002012.45   0    etc   \n  SRR002012.48   16   chr1   etc    Question  What kind of information does the header provide?   Answer Answer @HD: Header line; VN: Format version; SO: the sort order of alignments. @SQ: Reference sequence information; SN: reference sequence name; LN: reference sequence length. @PG: Read group information; ID: Read group identifier; VN: Program version; CL: the command line that produces the alignment.  Question  To which chromosome are the reads mapped?   Answer Answer Chromosome 1.", 
            "title": "Alignment"
        }, 
        {
            "location": "/modules/btp-module-ngs-mapping/alignment/#manipulate-sam-output", 
            "text": "SAM files are rather big and when dealing with a high volume of NGS\ndata, storage space can become an issue. As we have already seen, we can\nconvert SAM to BAM files (their binary equivalent that are not human\nreadable) that occupy much less space.  Convert SAM to BAM using  samtools view  and store the output in the\nfile  Oct4.bam . You have to instruct  samtools view  that the input is\nin SAM format ( -S ), the output should be in BAM format ( -b ) and that\nyou want the output to be stored in the file specified by the  -o \noption:  1 samtools view -bSo Oct4.bam Oct4.sam   Compute summary stats for the Flag values associated with the alignments\nusing:  1 samtools flagstat Oct4.bam", 
            "title": "Manipulate SAM output"
        }, 
        {
            "location": "/modules/btp-module-ngs-mapping/alignment/#visualize-alignments-in-igv", 
            "text": "IGV is a stand-alone genome browser. Please check their website\n( http://www.broadinstitute.org/igv/ ) for all the formats that IGV can\ndisplay. For our visualization purposes we will use the BAM and bigWig\nformats.  When uploading a BAM file into the genome browser, the browser will look\nfor the index of the BAM file in the same folder where the BAM files is.\nThe index file should have the same name as the BAM file and the suffix .bai . Finally, to create the index of a BAM file you need to make sure\nthat the file is sorted according to chromosomal coordinates.  Sort alignments according to chromosomal position and store the result\nin the file with the prefix  Oct4.sorted :  1 samtools sort Oct4.bam -o Oct4.sorted.bam   Index the sorted file.  1 samtools index Oct4.sorted.bam   The indexing will create a file called  Oct4.sorted.bam.bai . Note that\nyou don\u2019t have to specify the name of the index file when running samtools index , it simply appends a  .bai  suffix to the input BAM\nfile.  Another way to visualize the alignments is to convert the BAM file into\na bigWig file. The bigWig format is for display of dense, continuous\ndata and the data will be displayed as a graph. The resulting bigWig\nfiles are in an indexed binary format.  The BAM to bigWig conversion takes place in two steps. Firstly, we\nconvert the BAM file into a bedgraph, called  Oct4.bedgraph , using the\ntool  genomeCoverageBed  from BEDTools. Then we convert the bedgraph\ninto a bigWig binary file called  Oct4.bw , using  bedGraphToBigWig \nfrom the UCSC tools:  1\n2 genomeCoverageBed -bg -ibam Oct4.sorted.bam -g bowtie_index/mouse.mm10.genome   Oct4.bedgraph\nbedGraphToBigWig Oct4.bedgraph bowtie_index/mouse.mm10.genome Oct4.bw   Both of the commands above take as input a file called mouse.mm10.genome  that is stored under the subdirectory bowtie_index . These genome files are tab-delimited and describe the\nsize of the chromosomes for the organism of interest. When using the\nUCSC Genome Browser, Ensembl, or Galaxy, you typically indicate which\nspecies/genome build you are working with. The way you do this for\nBEDTools is to create a \u201cgenome\u201d file, which simply lists the names of\nthe chromosomes (or scaffolds, etc.) and their size (in basepairs).  BEDTools includes pre-defined genome files for human and mouse in the genomes  subdirectory included in the BEDTools distribution.  Now we will load the data into the IGV browser for visualization. In\norder to launch IGV double click on the  IGV 2.3  icon on your Desktop.\nIgnore any warnings and when it opens you have to load the genome of\ninterest.  On the top left of your screen choose from the drop down menu Mouse (mm10) . If it doesn\u2019t appear in list, click  More .. , type mm10  in the Filter section, choose the mouse genome and press OK. Then\nin order to load the desire files go to:  1 File   Load from File   On the pop up window navigate to Desktop -  chipseq folder and select\nthe file  Oct4.sorted.bam .  Repeat these steps in order to load  Oct4.bw  as well.  Select  chr1  from the drop down menu on the top left. Right click on\nthe name of  Oct4.bw  and choose Maximum under the Windowing Function.\nRight click again and select Autoscale.  In order to see the aligned reads of the BAM file, you need to zoom in\nto a specific region. For example, look for gene  Lemd1  in the search\nbox.   Question  What is the main difference between the visualization of BAM and bigWig files?   Answer Answer The actual alignment of reads that stack to a particular region can be displayed using the information stored in a BAM format. The bigWig format is for display of dense, continuous data that will be displayed in the Genome Browser as a graph. Using the  +  button on the top right, zoom in to see more of the\ndetails of the alignments.   Question  What do you think the different colors mean?   Answer Answer The different color represents four nucleotides, e.g. blue is Cytidine (C), red is Thymidine (T).", 
            "title": "Visualize alignments in IGV"
        }, 
        {
            "location": "/modules/btp-module-ngs-mapping/alignment/#practice-makes-perfect", 
            "text": "In the chipseq folder you will find the file  gfp.fastq . Follow the\nabove described analysis, from the bowtie2 alignment step, for this\ndataset as well. You will need these files for the ChIP-Seq module.", 
            "title": "Practice Makes Perfect!"
        }, 
        {
            "location": "/modules/btp-module-chip-seq/chip-seq/", 
            "text": "Key Learning Outcomes\n\n\nAfter completing this practical the trainee should be able to:\n\n\n\n\n\n\nPerform ChIP-Seq analysis, e.g. the detection of immuno-enriched\n    areas using the chosen R package: ChIP-seq processing pipeline (SPP)\n\n\n\n\n\n\nVisualize the peak regions through a genome browser, e.g. IGV or\n    Ensembl, and identify the real peak regions\n\n\n\n\n\n\nPerform functional annotation using biomaRt R package and detect\n    potential binding sites (motif) in the predicted binding regions\n    using motif discovery tool, e.g. Trawler or MEME.\n\n\n\n\n\n\nResources You\u2019ll be Using\n\n\nTools Used\n\n\nSPP: \n\n\nhttp://compbio.med.harvard.edu/Supplements/ChIP-seq/\n\n\nIGV:\n\n\nhttp://software.broadinstitute.org/software/igv/\n\n\nEnsembl:\n\n\nhttp://www.ensembl.org\n\n\nTrawler:\n\n\nhttps://trawler.erc.monash.edu.au/index.html\n\n\nRSAT peak-motifs:  \n\n\nhttp://floresta.eead.csic.es/rsat/peak-motifs~f~orm.cgi\n\n\nMEME-ChIP:\n\n\nhttp://meme-suite.org/tools/meme-chip\n\n\nTOMTOM:\n\n\nhttp://meme.ebi.edu/meme/cgi-bin/tomtom.cgi\n\n\nDAVID:\n\n\nhttp://david.abcc.ncifcrf.gov\n\n\nGOstat:\n\n\nhttp://gostat.wehi.edu.au\n\n\nSources of Data\n\n\nhttp://www.ebi.ac.uk/arrayexpress/experiments/E-GEOD-11431\n\n\nAuthor Information\n\n\nPrimary Author(s):\n\n    Sonika Tyagi \n \n\n    Sean Li \n    \n\n\nContributor(s):\n\n    Mirana Ramialison \n  \n\n    Markus Tondl \n    \n\n\nIntroduction\n\n\nThe goal of this hands-on session is to perform some basic tasks in the\nanalysis of ChIP-seq data. In fact, you already performed the first\nstep, alignment of the reads to the genome, in the previous session. We\nstart from the aligned reads and we will find immuno-enriched areas\nusing SPP. We will visualize the identified regions in a genome browser\nand perform functional annotation and motif analysis on the predicted\nbinding regions.\n\n\nPrepare the Environment\n\n\nThe material for this practical can be found in the \nChIPseq\n directory\non your desktop. Please make sure that this directory also contains the\nSAM/BAM files you produced during the alignment practical.\n\n\nIf you didn\u2019t have time to align the control file called \ngfp.fastq\n\nduring the alignment practical, please do it now. Follow the same steps,\nfrom the bowtie alignment step, as for the \nOct4.fastq\n file.\n\n\nIn ChIP-seq analysis (unlike in other applications such as RNA-seq) it\ncan be useful to exclude all reads that map to more than one location in\nthe genome. When using Bowtie, this can be done using the \n-m 1\n option,\nwhich tells it to report only unique matches (See \nbowtie \u2013help\n for\nmore details).\n\n\nOpen the Terminal and go to the \nchipseq\n directory:\n\n\n1\n2\n3\ncd /home/trainee/chipseq\nls\nR\n\n\n\n\n\n\nFinding enriched areas using SPP\n\n\nTerminology used in the tutorial: \n-   \nfragment:\n overlapping fragments\n    obtaining in the IP (immuno precipitation) experiments. \n-   \ntag:\n sequenced part of the fragment which could be from one end (in case of single end\n    sequencing ) or both ends in the paired end data. \n-   \nalignment:\n a process to determine the position of the tags, which typically should be around\n    the binding site. \n-   \npeaks:\n spatial distribution of the tags densities\n    around the binding sites on the genome. You would see two separate peaks\n    of tags on the positive and negative strand around the binding site. The\n    distance between the two peaks should reflect the size of the protected\n    region.\n\n\nSPP is a Chip-seq processing pipeline implemented using R.\n\n\nThe main functions of SPP include locating quality tag alignment by\nscreening overall DNA-binding signals, removing or restricting certain\npositions with extremely high number of tags, estimating significant\nenrichment regions through genome-wide profiling, providing appropriate\noutputs for visualization, and determining statistically significant\nbinding positions with saturation criteria assessment. Moreover, the\nprocessing of ChIP-seq data can require considerable amount of CPU time,\nit is often necessary to make use of parallel processing. SPP supports\nparallel processing if the cluster option is configured. Since our\nexample data is relatively small, we will use single CPU and omit the\ncluster parameters for simplicity. The following steps will work you\nthrough the SPP pipeline.\n\n\nIn your R terminal, load spp and biomaRt packages and make sure to set\nyour working directory correctly:\n\n\n1\n2\n3\nlibrary(spp);\nlibrary(biomaRt);\nsetwd(\n/home/trainee/chipseq\n);\n\n\n\n\n\n\nThe first stage in SPP are:\n1.    load input data; \n2.    choose alignment quality and \n3.    remove anomalies. \n\n\nSPP can read output from the following\naligners and file formats: ELAND, MAQ, bowtie, Arachne, tagAlign format\nand BAM format (Note: because BAM standard doesn\u2019t specify a flag for a\nuniquely-mapped read, the aligner has to generate a BAM file that would\ncontain only unique reads.)\n\n\nSTEP 1. Loading data and quality filter the informative tags\n\n\nFirst load Oct4 and gfp bam files. Here GFP are the control or input\nsamples, these are usually mock IP DNA where you do not expect to see\nany binding peaks.\n\n\n1\n2\noct4.data\n-\nread.bam.tags\n(\nOct4.sorted.bam\n);\n\ngfp.data\n-\nread.bam.tags\n(\ngfp.sorted.bam\n);\n\n\n\n\n\n\n\nThe statistical significance of tags clustering observed for a putative\nprotein binding site depends on the expected background. Therefore, use\nof a input or control DNA is highly recommended in the experiment\ndesign. This provides an experimental assessment of the background tag\ndistribution.\n\n\nThe next step uses cross-correlation profile to calculate binding peak\nseparation distance, and assess whether inclusion of tags with\nnon-perfect alignment quality improves the cross-correlation peak. This\nis done by shifting the strands relative to each other by increasing\ndistance within a given range. cross-correlation of the positive and\nnegative strand tag densities is plotted. The cross-correlation plot\nshould show the predominant size of the protected region.\n\n\n1\nbinding.characteristics \n-\n get.binding.characteristics\n(\noct4.data\n,\nsrange\n=\nc\n(\n50\n,\n500\n),\nbin\n=\n5\n);\n\n\n\n\n\n\n\nThe binding.characteristics provides the estimate of the binding peak\nseparation distance, cross-correlation profile itself and tag quality\nbin acceptance information. The srange parameter defines the possible\nrange for the size of the protected region. It is supposed to be higher\nthan tag length. However, the upper boundary (500) cannot be too high,\nwhich will increase the running time. The bin parameter tags within the\nspecified number of base pairs to speed up calculation. The increase of\nbin size will decrease the accuracy of the determined parameters.\n\n\nThen, print out binding peak separation distance and we can plot\ncross-correlation profile:\n\n\n1\n2\n3\n4\n5\n6\nprint(paste(\nbinding peak separation distance=\n,binding.characteristics$peak$x));\npdf(file=\noct4.crosscorrelation.pdf\n,width=5,height=5);\npar(mar = c(3.5,3.5,1.0,0.5), mgp = c(2,0.65,0), cex = 0.8);\nplot(binding.characteristics$cross.correlation,type=\nl\n,xlab=\nstrand shift\n,ylab=\ncross-correlation\n);\nabline(v=binding.characteristics$peak$x,lty=2,col=2);\ndev.off();\n\n\n\n\n\n\nA set of tags informative about the binding positions should increase\ncross correlation magnitude whereas a randonmly mapped set of tags\nshould decrease it. The following calls will select tags with acceptable\nalignment quality based on the binding characteristics:\n\n\n1\n2\nchip.data \n-\n select.informative.tags\n(\noct4.data\n,\nbinding.characteristics\n);\n\ngfpcontrol.data \n-\n select.informative.tags\n(\ngfp.data\n,\nbinding.characteristics\n);\n\n\n\n\n\n\n\nThe last step below will scan along the chromosomes calculating local\ndensity of region (can be specified using window.size parameter, default\nis 200bp), removing or restricting singular positions with extremely\nhigh tag count relative to the neighborhood:\n\n\n1\n2\nchip.data \n-\n remove.local.tag.anomalies\n(\nchip.data\n);\n\ngfpcontrol.data \n-\n remove.local.tag.anomalies\n(\ngfpcontrol.data\n);\n\n\n\n\n\n\n\nSTEP 2. Calculating genome-wide tag density and tag enrichment/depletion profiles\n\n\nThe following commands will calculate smoothed tag density and output it\ninto a WIG file that can be read with genome browsers, such as IGV\n(Note: the tags are shifted by half of the peak separation distance):\n\n\n1\n2\n3\n4\ntag.shift \n-\n \nround\n(\nbinding.characteristics\n$\npeak\n$\nx\n/\n2\n)\n\nsmoothed.density \n-\n get.smoothed.tag.density\n(\nchip.data\n,\ncontrol.tags\n=\ngfpcontrol.data\n,\nbandwidth\n=\n200\n,\nstep\n=\n100\n,\ntag.shift\n=\ntag.shift\n);\n\nwritewig\n(\nsmoothed.density\n,\noct4.density.wig\n,\nSmoothed, background-subtracted tag density\n);\n\n\nrm\n(\nsmoothed.density\n);\n\n\n\n\n\n\n\nTo provide a rough estimate of the enrichment profile (i.e. ChIP signal\nover input), we can use the get.smoothed.enrichment.mle() method:\n\n\n1\n2\nsmoothed.enrichment.estimate \n-\n get.smoothed.enrichment.mle\n(\nchip.data\n,\ngfpcontrol.data\n,\nbandwidth\n=\n200\n,\nstep\n=\n100\n,\ntag.shift\n=\ntag.shift\n);\n\nwritewig\n(\nsmoothed.enrichment.estimate\n,\noct4.enrichment.wig\n,\nSmoothed maximum likelihood log2 enrichment estimate\n);\n\n\n\n\n\n\n\nNext, we will scan ChIP and signal tag density to estimate lower bounds\nof tag enrichment (and upper bound of tag depletion if it is\nsignificant) along the genome. The resulting profile gives conservative\nstatistical estimates of log2 fold-enrichment ratios along the genome.\nThe example below uses a window of 500bp (and background windows of 1,\n5, 25 and 50 times that size) and a confidence interval corresponding to\n1%.\n\n\n1\n2\n3\nenrichment.estimates \n-\n get.conservative.fold.enrichment.profile\n(\nchip.data\n,\ngfpcontrol.data\n,\nfws\n=\n500\n,\nstep\n=\n100\n,\nalpha\n=\n0.01\n);\n\nwritewig\n(\nenrichment.estimates\n,\noct4.Enrichment.estimates.wig\n,\nConservative fold-enrichment/depletion estimates shown on log2 scale\n);\n\n\nrm\n(\nenrichment.estimates\n);\n\n\n\n\n\n\n\nAlso, broad regions of enrichment for a specified scale can be quickly\nidentified and output in broadPeak format using the following commands:\n\n\n1\n2\nbroad.clusters \n-\n get.broad.enrichment.clusters\n(\nchip.data\n,\ngfpcontrol.data\n,\nwindow.size\n=\n1e3\n,\nz.thr\n=\n3\n,\ntag.shift\n=\nround\n(\nbinding.characteristics\n$\npeak\n$\nx\n/\n2\n));\n\nwrite.broadpeak.info\n(\nbroad.clusters\n,\noct4.broadPeak\n);\n\n\n\n\n\n\n\nwrite out in bed format\n\n\n1\nwrite.table(cbind(rep(\n1\n, length(broad.clusters$chr1$s)), broad.clusters$chr1$s, broad.clusters$chr1$e), file = paste0(\noct4\n,\n_enrich_broad_chr1.bed\n),quote = FALSE, row.names = FALSE, col.names = FALSE, sep = \n\\t\n);\n\n\n\n\n\n\nThe tasks below will use window tag density (WTD) method to call binding\npositions, using FDR of 1% and a window size estimated by the\nbinding.characteristics.\n\n\nWe set the binding detection parameters: FDR (1%) (Note: we can use an\nE-value to the method calls below instead of the fdr), the\nbinding.characteristics contains the optimized half-size for binding\ndetection window:\n\n\n1\n2\nfdr \n-\n \n1e-2\n;\n\ndetection.window.halfsize \n-\n binding.characteristics\n$\nwhs\n;\n\n\n\n\n\n\n\nIdentify binding positions using WTD method and write narrow peaks in\nBED format:\n\n\n1\n2\n3\n4\nbp \n-\n find.binding.positions\n(\nsignal.data\n=\nchip.data\n,\ncontrol.data\n=\ngfpcontrol.data\n,\nfdr\n=\nfdr\n,\nwhs\n=\ndetection.window.halfsize\n);\n\n\nprint\n(\npaste\n(\ndetected\n,\nsum\n(\nunlist\n(\nlapply\n(\nbp\n$\nnpl\n,\nfunction\n(\nd\n)\n \nlength\n(\nd\n$\nx\n)))),\npeaks\n));\n\nbp.short \n-\n add.broad.peak.regions\n(\nchip.data\n,\ngfpcontrol.data\n,\nbp\n,\nwindow.size\n=\n500\n,\nz.thr\n=\n3\n);\n\nwrite.table\n(\nna.omit\n(\ndata.frame\n(\ncbind\n(\nrep\n(\n1\n,\n \nlength\n(\nbp.short\n$\nnpl\n$\nchr1\n$\nrs\n)),\n bp.short\n$\nnpl\n$\nchr1\n$\nrs\n,\n bp.short\n$\nnpl\n$\nchr1\n$\nre\n))),\n file \n=\n \npaste0\n(\noct4\n,\n_enrich_narrow_chr1.bed\n),\nquote \n=\n \nFALSE\n,\n row.names \n=\n \nFALSE\n,\n col.names \n=\n \nFALSE\n,\n sep \n=\n \n\\t\n);\n\n\n\n\n\n\n\nSTEP 3. Comparing Binding Sites to Annotations Using the biomaRt package\n\n\nIn order to biologically interpret the results of ChIP-seq experiments,\nit is usually recommended to look at the genes and other annotated\nelements that are located in proximity to the identified enriched\nregions. This can be easily done using the R biomaRt package, which\nserves as an interface to perform comprehensive data analysis from gene\nannotation to data mining through wealth number of biological databases\nintegrated by the BioMart software suite (\nhttp://www.biomart.org\n). It\nprovides fast access to large amount of data without touching the\nunderlying database or using complex database queries. These major\ndatabases including Ensembl, COSMIC, HGNC, Gramene, Wormbase and dbSNP\nmapped to Emsembl.\n\n\nyou should make sure that ensembl has the same version of reference as\nyou used in bowtie aligner.\n\n\nWe will download the ENSEMBLE mouse genome annotations and generate a\nlist of ENSEMBLE gene information on chromosome 1 including start\nposition, end position, strand and description\n\n\n1\n2\nensembl = useMart(host=\nasia.ensembl.org\n, \nENSEMBL_MART_ENSEMBL\n, dataset = \nmmusculus_gene_ensembl\n);\ngenes.chr1 = getBM(attributes = c(\nchromosome_name\n, \nstart_position\n, \nend_position\n, \nstrand\n, \ndescription\n), filters = \nchromosome_name\n, values= \n1\n, mart = ensembl);\n\n\n\n\n\n\nNext, we\u2019re going to take our binding sites from the bp list and use it\nto determine the set of genes that contain significantly enriched Pol II\nwithin 2kb of their TSS.\n\n\nIn order to compare PolII sites to TSS sites, we need to write an\noverlap function where bs represents a binding site position, ts is the\nannotated TSS and l is the allowed distance of the binding site from the\nTSS.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\noverlap = function(bs, ts, l)\n{\n    if ((bs \n ts - l) \n (bs \n ts + l)) {\n        TRUE;\n    } else {\n        FALSE;\n    }\n}\n\n\n\n\n\n\nNow we\u2019ll write a function that takes a vector of binding site values,\nstart positions, end positions and strands of the genes on chromosome X\nas well as our distance cutoff. l and outputs a logical vector of the\ngenes that contain a Pol II site within l bp (i.e., TRUE value) or do\nnot contain a Pol II site (i.e., FALSE value).\n\n\n 1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15\nfivePrimeGenes = function(bs, ts, te, s, l) {\n    fivePrimeVec = logical();\n    for (i in 1:length(ts)) {\n            fivePrime = FALSE;\n            for (j in 1:length(bs)) {\n                if (s[i] == 1) {\n                    fivePrime = fivePrime || overlap(bs[j], ts[i], l);\n                } else {\n                    fivePrime = fivePrime || overlap(bs[j], te[i], l);\n                }\n             }\n            fivePrimeVec = c(fivePrimeVec, fivePrime);\n    }\n     fivePrimeVec;\n}\n\n\n\n\n\n\nUsing the fivePrimeGenes function, generate a vector of the TSSs and\ngenes that contain Pol II within .2kb of their TSS (i.e., l = 2000).\n\n\n1\nfivePrimeGenesLogical = fivePrimeGenes(bp$npl$chr1$x, genes.chr1$start_position, genes.chr1$end_position, genes.chr1$strand, 2000);\n\n\n\n\n\n\nFind the gene located on the plus strand\n\n\n1\nfivePrimeStartsPlus = genes.chr1$start_position[fivePrimeGenesLogical \n genes.chr1$strand == 1];\n\n\n\n\n\n\nFind the gene located on the minus strand\n\n\n1\nfivePrimeStartsMinus = genes.chr1$end_position[fivePrimeGenesLogical \n genes.chr1$strand == -1];\n\n\n\n\n\n\nCombine the start positions together\n\n\n1\nfivePrimeStarts = sort(c(fivePrimeStartsPlus, fivePrimeStartsMinus))\n\n\n\n\n\n\nGet all the gene names\n\n\n1\nfivePrimeGenes = genes.chr1$description[fivePrimeGenesLogical]\n\n\n\n\n\n\nViewing results with the Genome browser\n\n\nIt is often instructive to look at your data in a genome browser, which\nwill allow you to get a \u2018feel\u2019 for the data, as well as detecting\nabnormalities and problems. Also, exploring the data in such a way may\ngive you ideas for further analyses. Well known web-based genome\nbrowsers, like Ensembl or the UCSC browser do not only allow for more\npolished and flexible visualization, but also provide access to a wealth\nof annotations and external data sources. This makes it straightforward\nto relate your data with information about repeat regions, known genes,\nepigenetic features or areas of cross-species conservation, to name just\na few. As such, they are useful tools for exploratory analysis, even\nthough could be relatively slow. In this section, we will guide you\nthough using IGV, a stand-alone browser, which has the advantage of\nbeing installed locally, easy to use and fast access to visualize your\nin-house data. We alo provide the workflow of how to use Ensembl for\nvisualization. You can practise after the workshop.\n\n\nIGV Visualization\n\n\nDouble click the IGV 2.3 icon on your Desktop. Ignore any warnings and\nwhen it opens you have to load the genome of interest. On the top left\nof your screen choose from the drop down menu Mouse (mm10). If it\ndoesn\u2019t appear in list, click More .., type mm10 in the Filter section,\nchoose the mouse genome and press OK.\n\n\nWe have generated bigWig files in advance for you. Instead of choosing\nthe \u2019Load from File\u2019 option, we are going to use \u2019Load from URL\u2019 to\nupload to IGV. The first file is at the following URL:\n\nhttp://www.ebi.ac.uk/~remco/ChIP-Seq_course/Oct4.bw\n\n\nTo visualise the data:\n\n\n\n\n\n\nSelect chr1 in the chromosome drop-down box next to the \u2019Mouse mm10\u2019\n    box.\n\n\n\n\n\n\nClick \nFile\n then choose \nLoad from URL\n\n\n\n\n\n\nPaste the location above in the field \nFile URL\n.\n\n\n\n\n\n\nClick \nOK\n and close the window to return to the genome browser.\n\n\n\n\n\n\nYou should see Oct4.bw has been loaded in the track region below the\n    genome region.\n\n\n\n\n\n\nMove the mouse to track region over Oct4.bw.\n\n\n\n\n\n\nRight click the mouse, Change the track colour on your own\n    perference.\n\n\n\n\n\n\nRight click again, in the \nWindowing Function\n, choose \nMaxmum\n\n    and set to \nAutoscale\n.\n\n\n\n\n\n\nRepeat the process for the gfp control sample, located at:\n\n\nhttp://www.ebi.ac.uk/~remco/ChIP-Seq_course/gfp.bw\n.\n\n\nGo to a region on chromosome 1 (e.g. \n1\n:\n34823162\n-\n35323161\n), and zoom in\nand out to view the signal and peak regions. Be aware that the y-axis of\neach track is auto-scaled independently of each other, so bigger-looking\npeaks may not actually be bigger! Always look at the values on the left\nhand side axis.\n\n\n\n\nQuestion\n\n\nWhat can you say about the profile of Oct4 peaks in this region?\n\n\n\n\nAnswer\nAnswer\nThere are no significant Oct4 peaks over the selected region.\nCompare it with H3K4me3 histone modification wig file we have generated\nat \nhttp://www.ebi.ac.uk/~remco/ChIP-Seq_course/H3K4me3.bw\n.\n\n\nH3K4me3 has a region that contains relatively high peaks than Oct4.\n\n\n\n\nQuestion\n\n\nJump to \n1\n:\n36066594\n-\n36079728\n for a sample peak. Do you think H3K4me3 peaks regions contain one or more modification sites? What about Oct4?\n\n\n\n\nAnswer\nAnswer\nYes. There are roughly three peaks, which indicate the possibility of having more than one modification sites in this region. For Oct4, no peak can be observed.\n\n\nAdvanced exercise\n\n\nEnsembl Visualization\n\n\nLaunch a web browser and go to the Ensembl website at\n\nhttp://www.ensembl.org/index.html\n. Choose the genome of interest (in\nthis case, mouse) on the left side of the page, browse to any location\nin the genome or click one of the demo links provided on the web page.\nClick on the \nAdd your data\n link on the left, then choose \nAttach\nremote file\n.\n\n\nWig files are large so are inconvenient for uploading directly to the\nEnsemble Genome browser. Instead, we will convert it to an indexed\nbinary format and put this into a web accessible place such as on a\nHTTP, HTTPS, or FTP server. This makes all the browsing process much\nfaster. Detailed instructions for generating a bigWig from a wig type\nfile can be found at:\n\n\nhttp://genome.ucsc.edu/goldenPath/help/bigWig.html\n.\n\n\nWe have generated bigWig files in advance for you to upload to the\nEnsembl browser. They are at the following URL:\n\nhttp://www.ebi.ac.uk/~remco/ChIP-Seq_course/Oct4.bw\n\n\nTo visualise the data:\n\n\n\n\n\n\nPaste the location above in the field File URL.\n\n\n\n\n\n\nChoose data format bigWig.\n\n\n\n\n\n\nChoose some informative name and in the next window choose the\n    colour of your preference.\n\n\n\n\n\n\nClick \nSave\n and close the window to return to the genome browser.\n\n\n\n\n\n\nRepeat the process for the gfp control sample, located at:\n\n\nhttp://www.ebi.ac.uk/~remco/ChIP-Seq_course/gfp.bw\n.\n\n\nIf can not see your tracks: Click on \u2019Configure this page\u2019 in left\npanel. In \u2019Configure region Overview\u2019 tab click on \u2019Ypur data\u2019 in left\npanel. Check the boxes in \u2019Enable/Disable all tracks\u2019 for you *.bw\nfiles by selecting \u2019wiggle plot in the pop up menu.\n\n\nAfter uploading, choose \nConfigure this page\n, and under \nYour data\n\ntick both boxes. Closing the window will save these changes.\n\n\nGo to a region on chromosome 1 (e.g. \n1\n:\n34823162\n-\n35323161\n), and zoom in\nand out to view the signal and peak regions. Be aware that the y-axis of\neach track is auto-scaled independently of each other, so bigger-looking\npeaks may not actually be bigger! Always look at the values on the left\nhand side axis.\n\n\nMACS generates its peak files in a file format called bed file. This is\na simple text format containing genomic locations, specified by\nchromosome, begin and end positions, and some more optional information.\n\n\nSee \nhttp://genome.ucsc.edu/FAQ/FAQformat.html#format1\n for details.\n\n\nBed files can also be uploaded to the Ensembl browser.\n\n\n\n\nMotif analysis\n\n\nIt is often interesting to find out whether we can identify\ntranscription factor binding sites (TFBSs) from the input DNA sequences.\nTFBSs which share a similar sequence pattern (motif) are presumed to\nhave biological functions. Here we introduce three motif discovery tools\nnamed Trawler, RSAT peak-motifs, and MEME-ChIP, which use different\nsearching algorithms. This might lead to varying results. Hence, it is\ngenerally a good idea to use several tools to identify TFBSs. The more\ntools confirm the same result, the better, which is also called an\northogonal approach. Eventually, you probably want to validate your in\nsilico findings in vivo or in vitro.\n\n\nMotif discovery with Trawler\n\n\nTrawler is a fast, yet accurate motif discovery tool that accepts both,\nBED and FASTA files as input file formats. BED files are generated when\nyou process and analyse your NGS data. Thus, it is handy to use them\ndirectly in Trawler. Other tools do not accept BED files as input. With\nTrawler, BED files can be converted into FASTA files that can then be\nused for other motif discovery tools (e.g. RSAT peak-motifs and MEME\nChIP).\n\n\nRunning Trawler\n\n\n\n\n\n\nGo to the website \nhttps://trawler.erc.monash.edu.au\n\n\n\n\n\n\nRun Trawler with BED file as input, and wait for the results\n    \n \n\n\n\n\n\n\nDownload both sample and background files in FASTA format. Right\n    click and choose: \u2019Save the link as\n\u2019 \n\n\n\n\n\n\n\n\nQuestion\n\n\nWhich motif was found to be the most similar to your motif?\n\n\n\n\nAnswer\nAnswer\nSox2\nOptional: Motif discovery with RSAT peak-motif\n\n\nThe motif discovery tool RSAT peak-motifs uses FASTA files as input. An\noptional background can be uploaded in FASTA format. RSAT peak-motifs\nautomatically outputs motifs of 6 and 7 nucleotides length (two separate\nfiles). While still accurate, the running time is longer compared to\nTrawler (up to 20 minutes depending on the size of the files).\n\n\nRunning RSAT peak-motifs\n\n\n\n\n\n\nGo to the website\n    \nhttp://floresta.eead.csic.es/rsat/peak-motifs_form.cgi\n\n\n\n\n\n\nUpload input and background FASTA files just downloaded from Trawler\n    \n\n\n\n\n\n\nWait until the discovery finishes.\n\n\n\n\n\n\nOptional: Motif discovery with MEME-ChIP\n\n\nMEME-ChIP is a popular motif discovery tool and part of MEME Suite.\nMEME-ChIP accepts input files in FASTA format. It is not necessary to\nupload your own background because MEME-ChIP uses its own. Although\nMEME-ChIP is one of the most popular motif discovery tools, the\nidentified motifs are not very accurate and the motif search might take\nup to one hour. MEME-ChIP outputs the three motifs with the lowest\nE-Value.\n\n\nRunning MEME-ChIP\n\n\n\n\n\n\nGo to the website \nhttp://meme-suite.org/tools/meme-chip\n\n\n\n\n\n\nUpload input and background FASTA files just downloaded from Trawler\n    \n\n\n\n\n\n\nWait until the discovery finishes.\n\n\n\n\n\n\nReference\n\n\nChen, X et al.: Integration of external signaling pathways with the core\ntranscriptional network in embryonic stem cells. Cell 133:6, 1106-17\n(2008).", 
            "title": "ChIP-seq"
        }, 
        {
            "location": "/modules/btp-module-chip-seq/chip-seq/#key-learning-outcomes", 
            "text": "After completing this practical the trainee should be able to:    Perform ChIP-Seq analysis, e.g. the detection of immuno-enriched\n    areas using the chosen R package: ChIP-seq processing pipeline (SPP)    Visualize the peak regions through a genome browser, e.g. IGV or\n    Ensembl, and identify the real peak regions    Perform functional annotation using biomaRt R package and detect\n    potential binding sites (motif) in the predicted binding regions\n    using motif discovery tool, e.g. Trawler or MEME.", 
            "title": "Key Learning Outcomes"
        }, 
        {
            "location": "/modules/btp-module-chip-seq/chip-seq/#resources-youll-be-using", 
            "text": "", 
            "title": "Resources You\u2019ll be Using"
        }, 
        {
            "location": "/modules/btp-module-chip-seq/chip-seq/#tools-used", 
            "text": "SPP:   http://compbio.med.harvard.edu/Supplements/ChIP-seq/  IGV:  http://software.broadinstitute.org/software/igv/  Ensembl:  http://www.ensembl.org  Trawler:  https://trawler.erc.monash.edu.au/index.html  RSAT peak-motifs:    http://floresta.eead.csic.es/rsat/peak-motifs~f~orm.cgi  MEME-ChIP:  http://meme-suite.org/tools/meme-chip  TOMTOM:  http://meme.ebi.edu/meme/cgi-bin/tomtom.cgi  DAVID:  http://david.abcc.ncifcrf.gov  GOstat:  http://gostat.wehi.edu.au", 
            "title": "Tools Used"
        }, 
        {
            "location": "/modules/btp-module-chip-seq/chip-seq/#sources-of-data", 
            "text": "http://www.ebi.ac.uk/arrayexpress/experiments/E-GEOD-11431", 
            "title": "Sources of Data"
        }, 
        {
            "location": "/modules/btp-module-chip-seq/chip-seq/#author-information", 
            "text": "Primary Author(s): \n    Sonika Tyagi    \n    Sean Li        Contributor(s): \n    Mirana Ramialison     \n    Markus Tondl", 
            "title": "Author Information"
        }, 
        {
            "location": "/modules/btp-module-chip-seq/chip-seq/#introduction", 
            "text": "The goal of this hands-on session is to perform some basic tasks in the\nanalysis of ChIP-seq data. In fact, you already performed the first\nstep, alignment of the reads to the genome, in the previous session. We\nstart from the aligned reads and we will find immuno-enriched areas\nusing SPP. We will visualize the identified regions in a genome browser\nand perform functional annotation and motif analysis on the predicted\nbinding regions.", 
            "title": "Introduction"
        }, 
        {
            "location": "/modules/btp-module-chip-seq/chip-seq/#prepare-the-environment", 
            "text": "The material for this practical can be found in the  ChIPseq  directory\non your desktop. Please make sure that this directory also contains the\nSAM/BAM files you produced during the alignment practical.  If you didn\u2019t have time to align the control file called  gfp.fastq \nduring the alignment practical, please do it now. Follow the same steps,\nfrom the bowtie alignment step, as for the  Oct4.fastq  file.  In ChIP-seq analysis (unlike in other applications such as RNA-seq) it\ncan be useful to exclude all reads that map to more than one location in\nthe genome. When using Bowtie, this can be done using the  -m 1  option,\nwhich tells it to report only unique matches (See  bowtie \u2013help  for\nmore details).  Open the Terminal and go to the  chipseq  directory:  1\n2\n3 cd /home/trainee/chipseq\nls\nR", 
            "title": "Prepare the Environment"
        }, 
        {
            "location": "/modules/btp-module-chip-seq/chip-seq/#finding-enriched-areas-using-spp", 
            "text": "Terminology used in the tutorial: \n-    fragment:  overlapping fragments\n    obtaining in the IP (immuno precipitation) experiments. \n-    tag:  sequenced part of the fragment which could be from one end (in case of single end\n    sequencing ) or both ends in the paired end data. \n-    alignment:  a process to determine the position of the tags, which typically should be around\n    the binding site. \n-    peaks:  spatial distribution of the tags densities\n    around the binding sites on the genome. You would see two separate peaks\n    of tags on the positive and negative strand around the binding site. The\n    distance between the two peaks should reflect the size of the protected\n    region.  SPP is a Chip-seq processing pipeline implemented using R.  The main functions of SPP include locating quality tag alignment by\nscreening overall DNA-binding signals, removing or restricting certain\npositions with extremely high number of tags, estimating significant\nenrichment regions through genome-wide profiling, providing appropriate\noutputs for visualization, and determining statistically significant\nbinding positions with saturation criteria assessment. Moreover, the\nprocessing of ChIP-seq data can require considerable amount of CPU time,\nit is often necessary to make use of parallel processing. SPP supports\nparallel processing if the cluster option is configured. Since our\nexample data is relatively small, we will use single CPU and omit the\ncluster parameters for simplicity. The following steps will work you\nthrough the SPP pipeline.  In your R terminal, load spp and biomaRt packages and make sure to set\nyour working directory correctly:  1\n2\n3 library(spp);\nlibrary(biomaRt);\nsetwd( /home/trainee/chipseq );   The first stage in SPP are:\n1.    load input data; \n2.    choose alignment quality and \n3.    remove anomalies.   SPP can read output from the following\naligners and file formats: ELAND, MAQ, bowtie, Arachne, tagAlign format\nand BAM format (Note: because BAM standard doesn\u2019t specify a flag for a\nuniquely-mapped read, the aligner has to generate a BAM file that would\ncontain only unique reads.)", 
            "title": "Finding enriched areas using SPP"
        }, 
        {
            "location": "/modules/btp-module-chip-seq/chip-seq/#step-1-loading-data-and-quality-filter-the-informative-tags", 
            "text": "First load Oct4 and gfp bam files. Here GFP are the control or input\nsamples, these are usually mock IP DNA where you do not expect to see\nany binding peaks.  1\n2 oct4.data - read.bam.tags ( Oct4.sorted.bam ); \ngfp.data - read.bam.tags ( gfp.sorted.bam );    The statistical significance of tags clustering observed for a putative\nprotein binding site depends on the expected background. Therefore, use\nof a input or control DNA is highly recommended in the experiment\ndesign. This provides an experimental assessment of the background tag\ndistribution.  The next step uses cross-correlation profile to calculate binding peak\nseparation distance, and assess whether inclusion of tags with\nnon-perfect alignment quality improves the cross-correlation peak. This\nis done by shifting the strands relative to each other by increasing\ndistance within a given range. cross-correlation of the positive and\nnegative strand tag densities is plotted. The cross-correlation plot\nshould show the predominant size of the protected region.  1 binding.characteristics  -  get.binding.characteristics ( oct4.data , srange = c ( 50 , 500 ), bin = 5 );    The binding.characteristics provides the estimate of the binding peak\nseparation distance, cross-correlation profile itself and tag quality\nbin acceptance information. The srange parameter defines the possible\nrange for the size of the protected region. It is supposed to be higher\nthan tag length. However, the upper boundary (500) cannot be too high,\nwhich will increase the running time. The bin parameter tags within the\nspecified number of base pairs to speed up calculation. The increase of\nbin size will decrease the accuracy of the determined parameters.  Then, print out binding peak separation distance and we can plot\ncross-correlation profile:  1\n2\n3\n4\n5\n6 print(paste( binding peak separation distance= ,binding.characteristics$peak$x));\npdf(file= oct4.crosscorrelation.pdf ,width=5,height=5);\npar(mar = c(3.5,3.5,1.0,0.5), mgp = c(2,0.65,0), cex = 0.8);\nplot(binding.characteristics$cross.correlation,type= l ,xlab= strand shift ,ylab= cross-correlation );\nabline(v=binding.characteristics$peak$x,lty=2,col=2);\ndev.off();   A set of tags informative about the binding positions should increase\ncross correlation magnitude whereas a randonmly mapped set of tags\nshould decrease it. The following calls will select tags with acceptable\nalignment quality based on the binding characteristics:  1\n2 chip.data  -  select.informative.tags ( oct4.data , binding.characteristics ); \ngfpcontrol.data  -  select.informative.tags ( gfp.data , binding.characteristics );    The last step below will scan along the chromosomes calculating local\ndensity of region (can be specified using window.size parameter, default\nis 200bp), removing or restricting singular positions with extremely\nhigh tag count relative to the neighborhood:  1\n2 chip.data  -  remove.local.tag.anomalies ( chip.data ); \ngfpcontrol.data  -  remove.local.tag.anomalies ( gfpcontrol.data );", 
            "title": "STEP 1. Loading data and quality filter the informative tags"
        }, 
        {
            "location": "/modules/btp-module-chip-seq/chip-seq/#step-2-calculating-genome-wide-tag-density-and-tag-enrichmentdepletion-profiles", 
            "text": "The following commands will calculate smoothed tag density and output it\ninto a WIG file that can be read with genome browsers, such as IGV\n(Note: the tags are shifted by half of the peak separation distance):  1\n2\n3\n4 tag.shift  -   round ( binding.characteristics $ peak $ x / 2 ) \nsmoothed.density  -  get.smoothed.tag.density ( chip.data , control.tags = gfpcontrol.data , bandwidth = 200 , step = 100 , tag.shift = tag.shift ); \nwritewig ( smoothed.density , oct4.density.wig , Smoothed, background-subtracted tag density );  rm ( smoothed.density );    To provide a rough estimate of the enrichment profile (i.e. ChIP signal\nover input), we can use the get.smoothed.enrichment.mle() method:  1\n2 smoothed.enrichment.estimate  -  get.smoothed.enrichment.mle ( chip.data , gfpcontrol.data , bandwidth = 200 , step = 100 , tag.shift = tag.shift ); \nwritewig ( smoothed.enrichment.estimate , oct4.enrichment.wig , Smoothed maximum likelihood log2 enrichment estimate );    Next, we will scan ChIP and signal tag density to estimate lower bounds\nof tag enrichment (and upper bound of tag depletion if it is\nsignificant) along the genome. The resulting profile gives conservative\nstatistical estimates of log2 fold-enrichment ratios along the genome.\nThe example below uses a window of 500bp (and background windows of 1,\n5, 25 and 50 times that size) and a confidence interval corresponding to\n1%.  1\n2\n3 enrichment.estimates  -  get.conservative.fold.enrichment.profile ( chip.data , gfpcontrol.data , fws = 500 , step = 100 , alpha = 0.01 ); \nwritewig ( enrichment.estimates , oct4.Enrichment.estimates.wig , Conservative fold-enrichment/depletion estimates shown on log2 scale );  rm ( enrichment.estimates );    Also, broad regions of enrichment for a specified scale can be quickly\nidentified and output in broadPeak format using the following commands:  1\n2 broad.clusters  -  get.broad.enrichment.clusters ( chip.data , gfpcontrol.data , window.size = 1e3 , z.thr = 3 , tag.shift = round ( binding.characteristics $ peak $ x / 2 )); \nwrite.broadpeak.info ( broad.clusters , oct4.broadPeak );    write out in bed format  1 write.table(cbind(rep( 1 , length(broad.clusters$chr1$s)), broad.clusters$chr1$s, broad.clusters$chr1$e), file = paste0( oct4 , _enrich_broad_chr1.bed ),quote = FALSE, row.names = FALSE, col.names = FALSE, sep =  \\t );   The tasks below will use window tag density (WTD) method to call binding\npositions, using FDR of 1% and a window size estimated by the\nbinding.characteristics.  We set the binding detection parameters: FDR (1%) (Note: we can use an\nE-value to the method calls below instead of the fdr), the\nbinding.characteristics contains the optimized half-size for binding\ndetection window:  1\n2 fdr  -   1e-2 ; \ndetection.window.halfsize  -  binding.characteristics $ whs ;    Identify binding positions using WTD method and write narrow peaks in\nBED format:  1\n2\n3\n4 bp  -  find.binding.positions ( signal.data = chip.data , control.data = gfpcontrol.data , fdr = fdr , whs = detection.window.halfsize );  print ( paste ( detected , sum ( unlist ( lapply ( bp $ npl , function ( d )   length ( d $ x )))), peaks )); \nbp.short  -  add.broad.peak.regions ( chip.data , gfpcontrol.data , bp , window.size = 500 , z.thr = 3 ); \nwrite.table ( na.omit ( data.frame ( cbind ( rep ( 1 ,   length ( bp.short $ npl $ chr1 $ rs )),  bp.short $ npl $ chr1 $ rs ,  bp.short $ npl $ chr1 $ re ))),  file  =   paste0 ( oct4 , _enrich_narrow_chr1.bed ), quote  =   FALSE ,  row.names  =   FALSE ,  col.names  =   FALSE ,  sep  =   \\t );", 
            "title": "STEP 2. Calculating genome-wide tag density and tag enrichment/depletion profiles"
        }, 
        {
            "location": "/modules/btp-module-chip-seq/chip-seq/#step-3-comparing-binding-sites-to-annotations-using-the-biomart-package", 
            "text": "In order to biologically interpret the results of ChIP-seq experiments,\nit is usually recommended to look at the genes and other annotated\nelements that are located in proximity to the identified enriched\nregions. This can be easily done using the R biomaRt package, which\nserves as an interface to perform comprehensive data analysis from gene\nannotation to data mining through wealth number of biological databases\nintegrated by the BioMart software suite ( http://www.biomart.org ). It\nprovides fast access to large amount of data without touching the\nunderlying database or using complex database queries. These major\ndatabases including Ensembl, COSMIC, HGNC, Gramene, Wormbase and dbSNP\nmapped to Emsembl.  you should make sure that ensembl has the same version of reference as\nyou used in bowtie aligner.  We will download the ENSEMBLE mouse genome annotations and generate a\nlist of ENSEMBLE gene information on chromosome 1 including start\nposition, end position, strand and description  1\n2 ensembl = useMart(host= asia.ensembl.org ,  ENSEMBL_MART_ENSEMBL , dataset =  mmusculus_gene_ensembl );\ngenes.chr1 = getBM(attributes = c( chromosome_name ,  start_position ,  end_position ,  strand ,  description ), filters =  chromosome_name , values=  1 , mart = ensembl);   Next, we\u2019re going to take our binding sites from the bp list and use it\nto determine the set of genes that contain significantly enriched Pol II\nwithin 2kb of their TSS.  In order to compare PolII sites to TSS sites, we need to write an\noverlap function where bs represents a binding site position, ts is the\nannotated TSS and l is the allowed distance of the binding site from the\nTSS.  1\n2\n3\n4\n5\n6\n7\n8 overlap = function(bs, ts, l)\n{\n    if ((bs   ts - l)   (bs   ts + l)) {\n        TRUE;\n    } else {\n        FALSE;\n    }\n}   Now we\u2019ll write a function that takes a vector of binding site values,\nstart positions, end positions and strands of the genes on chromosome X\nas well as our distance cutoff. l and outputs a logical vector of the\ngenes that contain a Pol II site within l bp (i.e., TRUE value) or do\nnot contain a Pol II site (i.e., FALSE value).   1\n 2\n 3\n 4\n 5\n 6\n 7\n 8\n 9\n10\n11\n12\n13\n14\n15 fivePrimeGenes = function(bs, ts, te, s, l) {\n    fivePrimeVec = logical();\n    for (i in 1:length(ts)) {\n            fivePrime = FALSE;\n            for (j in 1:length(bs)) {\n                if (s[i] == 1) {\n                    fivePrime = fivePrime || overlap(bs[j], ts[i], l);\n                } else {\n                    fivePrime = fivePrime || overlap(bs[j], te[i], l);\n                }\n             }\n            fivePrimeVec = c(fivePrimeVec, fivePrime);\n    }\n     fivePrimeVec;\n}   Using the fivePrimeGenes function, generate a vector of the TSSs and\ngenes that contain Pol II within .2kb of their TSS (i.e., l = 2000).  1 fivePrimeGenesLogical = fivePrimeGenes(bp$npl$chr1$x, genes.chr1$start_position, genes.chr1$end_position, genes.chr1$strand, 2000);   Find the gene located on the plus strand  1 fivePrimeStartsPlus = genes.chr1$start_position[fivePrimeGenesLogical   genes.chr1$strand == 1];   Find the gene located on the minus strand  1 fivePrimeStartsMinus = genes.chr1$end_position[fivePrimeGenesLogical   genes.chr1$strand == -1];   Combine the start positions together  1 fivePrimeStarts = sort(c(fivePrimeStartsPlus, fivePrimeStartsMinus))   Get all the gene names  1 fivePrimeGenes = genes.chr1$description[fivePrimeGenesLogical]", 
            "title": "STEP 3. Comparing Binding Sites to Annotations Using the biomaRt package"
        }, 
        {
            "location": "/modules/btp-module-chip-seq/chip-seq/#viewing-results-with-the-genome-browser", 
            "text": "It is often instructive to look at your data in a genome browser, which\nwill allow you to get a \u2018feel\u2019 for the data, as well as detecting\nabnormalities and problems. Also, exploring the data in such a way may\ngive you ideas for further analyses. Well known web-based genome\nbrowsers, like Ensembl or the UCSC browser do not only allow for more\npolished and flexible visualization, but also provide access to a wealth\nof annotations and external data sources. This makes it straightforward\nto relate your data with information about repeat regions, known genes,\nepigenetic features or areas of cross-species conservation, to name just\na few. As such, they are useful tools for exploratory analysis, even\nthough could be relatively slow. In this section, we will guide you\nthough using IGV, a stand-alone browser, which has the advantage of\nbeing installed locally, easy to use and fast access to visualize your\nin-house data. We alo provide the workflow of how to use Ensembl for\nvisualization. You can practise after the workshop.  IGV Visualization  Double click the IGV 2.3 icon on your Desktop. Ignore any warnings and\nwhen it opens you have to load the genome of interest. On the top left\nof your screen choose from the drop down menu Mouse (mm10). If it\ndoesn\u2019t appear in list, click More .., type mm10 in the Filter section,\nchoose the mouse genome and press OK.  We have generated bigWig files in advance for you. Instead of choosing\nthe \u2019Load from File\u2019 option, we are going to use \u2019Load from URL\u2019 to\nupload to IGV. The first file is at the following URL: http://www.ebi.ac.uk/~remco/ChIP-Seq_course/Oct4.bw  To visualise the data:    Select chr1 in the chromosome drop-down box next to the \u2019Mouse mm10\u2019\n    box.    Click  File  then choose  Load from URL    Paste the location above in the field  File URL .    Click  OK  and close the window to return to the genome browser.    You should see Oct4.bw has been loaded in the track region below the\n    genome region.    Move the mouse to track region over Oct4.bw.    Right click the mouse, Change the track colour on your own\n    perference.    Right click again, in the  Windowing Function , choose  Maxmum \n    and set to  Autoscale .    Repeat the process for the gfp control sample, located at:  http://www.ebi.ac.uk/~remco/ChIP-Seq_course/gfp.bw .  Go to a region on chromosome 1 (e.g.  1 : 34823162 - 35323161 ), and zoom in\nand out to view the signal and peak regions. Be aware that the y-axis of\neach track is auto-scaled independently of each other, so bigger-looking\npeaks may not actually be bigger! Always look at the values on the left\nhand side axis.   Question  What can you say about the profile of Oct4 peaks in this region?   Answer Answer There are no significant Oct4 peaks over the selected region. Compare it with H3K4me3 histone modification wig file we have generated\nat  http://www.ebi.ac.uk/~remco/ChIP-Seq_course/H3K4me3.bw .  H3K4me3 has a region that contains relatively high peaks than Oct4.   Question  Jump to  1 : 36066594 - 36079728  for a sample peak. Do you think H3K4me3 peaks regions contain one or more modification sites? What about Oct4?   Answer Answer Yes. There are roughly three peaks, which indicate the possibility of having more than one modification sites in this region. For Oct4, no peak can be observed.  Advanced exercise  Ensembl Visualization  Launch a web browser and go to the Ensembl website at http://www.ensembl.org/index.html . Choose the genome of interest (in\nthis case, mouse) on the left side of the page, browse to any location\nin the genome or click one of the demo links provided on the web page.\nClick on the  Add your data  link on the left, then choose  Attach\nremote file .  Wig files are large so are inconvenient for uploading directly to the\nEnsemble Genome browser. Instead, we will convert it to an indexed\nbinary format and put this into a web accessible place such as on a\nHTTP, HTTPS, or FTP server. This makes all the browsing process much\nfaster. Detailed instructions for generating a bigWig from a wig type\nfile can be found at:  http://genome.ucsc.edu/goldenPath/help/bigWig.html .  We have generated bigWig files in advance for you to upload to the\nEnsembl browser. They are at the following URL: http://www.ebi.ac.uk/~remco/ChIP-Seq_course/Oct4.bw  To visualise the data:    Paste the location above in the field File URL.    Choose data format bigWig.    Choose some informative name and in the next window choose the\n    colour of your preference.    Click  Save  and close the window to return to the genome browser.    Repeat the process for the gfp control sample, located at:  http://www.ebi.ac.uk/~remco/ChIP-Seq_course/gfp.bw .  If can not see your tracks: Click on \u2019Configure this page\u2019 in left\npanel. In \u2019Configure region Overview\u2019 tab click on \u2019Ypur data\u2019 in left\npanel. Check the boxes in \u2019Enable/Disable all tracks\u2019 for you *.bw\nfiles by selecting \u2019wiggle plot in the pop up menu.  After uploading, choose  Configure this page , and under  Your data \ntick both boxes. Closing the window will save these changes.  Go to a region on chromosome 1 (e.g.  1 : 34823162 - 35323161 ), and zoom in\nand out to view the signal and peak regions. Be aware that the y-axis of\neach track is auto-scaled independently of each other, so bigger-looking\npeaks may not actually be bigger! Always look at the values on the left\nhand side axis.  MACS generates its peak files in a file format called bed file. This is\na simple text format containing genomic locations, specified by\nchromosome, begin and end positions, and some more optional information.  See  http://genome.ucsc.edu/FAQ/FAQformat.html#format1  for details.  Bed files can also be uploaded to the Ensembl browser.", 
            "title": "Viewing results with the Genome browser"
        }, 
        {
            "location": "/modules/btp-module-chip-seq/chip-seq/#motif-analysis", 
            "text": "It is often interesting to find out whether we can identify\ntranscription factor binding sites (TFBSs) from the input DNA sequences.\nTFBSs which share a similar sequence pattern (motif) are presumed to\nhave biological functions. Here we introduce three motif discovery tools\nnamed Trawler, RSAT peak-motifs, and MEME-ChIP, which use different\nsearching algorithms. This might lead to varying results. Hence, it is\ngenerally a good idea to use several tools to identify TFBSs. The more\ntools confirm the same result, the better, which is also called an\northogonal approach. Eventually, you probably want to validate your in\nsilico findings in vivo or in vitro.", 
            "title": "Motif analysis"
        }, 
        {
            "location": "/modules/btp-module-chip-seq/chip-seq/#motif-discovery-with-trawler", 
            "text": "Trawler is a fast, yet accurate motif discovery tool that accepts both,\nBED and FASTA files as input file formats. BED files are generated when\nyou process and analyse your NGS data. Thus, it is handy to use them\ndirectly in Trawler. Other tools do not accept BED files as input. With\nTrawler, BED files can be converted into FASTA files that can then be\nused for other motif discovery tools (e.g. RSAT peak-motifs and MEME\nChIP).  Running Trawler    Go to the website  https://trawler.erc.monash.edu.au    Run Trawler with BED file as input, and wait for the results\n          Download both sample and background files in FASTA format. Right\n    click and choose: \u2019Save the link as \u2019      Question  Which motif was found to be the most similar to your motif?   Answer Answer Sox2", 
            "title": "Motif discovery with Trawler"
        }, 
        {
            "location": "/modules/btp-module-chip-seq/chip-seq/#optional-motif-discovery-with-rsat-peak-motif", 
            "text": "The motif discovery tool RSAT peak-motifs uses FASTA files as input. An\noptional background can be uploaded in FASTA format. RSAT peak-motifs\nautomatically outputs motifs of 6 and 7 nucleotides length (two separate\nfiles). While still accurate, the running time is longer compared to\nTrawler (up to 20 minutes depending on the size of the files).  Running RSAT peak-motifs    Go to the website\n     http://floresta.eead.csic.es/rsat/peak-motifs_form.cgi    Upload input and background FASTA files just downloaded from Trawler\n        Wait until the discovery finishes.", 
            "title": "Optional: Motif discovery with RSAT peak-motif"
        }, 
        {
            "location": "/modules/btp-module-chip-seq/chip-seq/#optional-motif-discovery-with-meme-chip", 
            "text": "MEME-ChIP is a popular motif discovery tool and part of MEME Suite.\nMEME-ChIP accepts input files in FASTA format. It is not necessary to\nupload your own background because MEME-ChIP uses its own. Although\nMEME-ChIP is one of the most popular motif discovery tools, the\nidentified motifs are not very accurate and the motif search might take\nup to one hour. MEME-ChIP outputs the three motifs with the lowest\nE-Value.  Running MEME-ChIP    Go to the website  http://meme-suite.org/tools/meme-chip    Upload input and background FASTA files just downloaded from Trawler\n        Wait until the discovery finishes.", 
            "title": "Optional: Motif discovery with MEME-ChIP"
        }, 
        {
            "location": "/modules/btp-module-chip-seq/chip-seq/#reference", 
            "text": "Chen, X et al.: Integration of external signaling pathways with the core\ntranscriptional network in embryonic stem cells. Cell 133:6, 1106-17\n(2008).", 
            "title": "Reference"
        }, 
        {
            "location": "/modules/btp-module-rna-seq/rna-seq/", 
            "text": "Key Learning Outcomes\n\n\nAfter completing this practical the trainee should be able to:\n\n\n\n\n\n\nUnderstand and perform a simple RNA-Seq analysis workflow.\n\n\n\n\n\n\nPerform spliced alignments to an indexed reference genome using\n    TopHat.\n\n\n\n\n\n\nVisualize spliced transcript alignments in a genome browser such as\n    IGV.\n\n\n\n\n\n\nBe able to identify differential gene expression between two\n    experimental conditions.\n\n\n\n\n\n\nBe familiar with R environment and be able to run R based RNA-seq\n    packages.\n\n\n\n\n\n\nWe also have bonus exercises where you can learn to:\n\n\n\n\n\n\nPerform transcript assembly using Cufflinks.\n\n\n\n\n\n\nRun cuffdiff, a Cufflinks utility for differential expression\n    analysis.\n\n\n\n\n\n\nVisualize transcript alignments and annotation in a genome browser\n    such as IGV.\n\n\n\n\n\n\nResources You\u2019ll be Using\n\n\nTools Used\n\n\nTophat:\n\n\nhttps://ccb.jhu.edu/software/tophat/index.shtml\n\n\nCufflinks:\n\n\nhttp://cole-trapnell-lab.github.io/cufflinks/\n\n\nSamtools:\n\n\nhttp://www.htslib.org/\n\n\nBEDTools:\n\n\nhttps://github.com/arq5x/bedtools2\n\n\nUCSC tools:\n\n\nhttp://hgdownload.cse.ucsc.edu/admin/exe/\n\n\nIGV:\n\n\nhttp://www.broadinstitute.org/igv/\n\n\nFeatureCount:\n\n\nhttp://subread.sourceforge.net/\n\n\nedgeR package:\n\n\nhttps://bioconductor.org/packages/release/bioc/html/edgeR.html\n\n\nCummeRbund manual:\n\n\nhttp://www.bioconductor.org/packages/release/bioc/vignettes/cummeRbund/inst/doc/cummeRbund-manual.pdf\n\n\nSources of Data\n\n\nhttp://www.ebi.ac.uk/ena/data/view/ERR022484\n\n\nhttp://www.ebi.ac.uk/ena/data/view/ERR022485\n\n\nhttp://www.pnas.org/content/suppl/2008/12/16/0807121105.DCSupplemental\n\n\nAuthor Information\n\n\nPrimary Author(s):\n\n    Susan M Corley \n\n    Sonika Tyagi \n   \n\n\nContributor(s):\n\n    Nathan S. Watson-Haigh \n \n    Myrto Kostadima, EMBL-EBI \n \n    Remco Loos, EMBL-EBI \n\n\nIntroduction\n\n\nThe goal of this hands-on session is to perform some basic tasks in the\ndownstream analysis of RNA-seq data.\n\n\nFirst we will use RNA-seq data from zebrafish. You will align one set of\nreads to the zebrafish using Tophat2. You will then view the aligned\nreads using the IGV viewer. We will also demonstrate how gene counts can\nbe derived from this data. You will go on to assembly a transcriptome\nfrom the read data using cufflinks. We will show you how this type of\ndata may be analysed for differential expression.\n\n\nThe second part of the tutorial will focus on RNA-seq data from a human\nexperiment (cancer cell line versus normal cells). You will use the\nBioconductor packages edgeR and voom (limma) to determine differential\ngene expression. The results from this analysis will then be used in the\nfinal session which introduces you to some of the tools used to gain\nbiological insight into the results of a differential expression\nanalysis\n\n\nPrepare the Environment\n\n\nWe will use a dataset derived from sequencing of mRNA from \nDanio rerio\n\nembryos in two different developmental stages. Sequencing was performed\non the Illumina platform and generated 76bp paired-end sequence data\nusing polyA selected RNA. Due to the time constraints of the practical\nwe will only use a subset of the reads.\n\n\nThe data files are contained in the subdirectory called \ndata\n and are\nthe following:\n\n\n2cells_1.fastq\n and \n2cells_2.fastq\n\n    These files are based on RNA-seq data of a 2-cell zebrafish embryo\n\n\n6h_1.fastq\n and \n6h_2.fastq\n\n    These files are based on RNA-seq data of zebrafish embryos 6h post\n    fertilization\n\n\nOpen the Terminal and go to the \nrnaseq\n working directory:\n\n\n1\ncd /home/trainee/rnaseq/\n\n\n\n\n\n\n\n\nSTOP\n\n\nAll commands entered into the terminal for this tutorial should be from\nwithin the \n/home/trainee/rnaseq\n directory.\n\n\n\n\nCheck that the \ndata\n directory contains the above-mentioned files by\ntyping:\n\n\n1\nls data\n\n\n\n\n\n\nAlignment\n\n\nThere are numerous tools for performing short read alignment and the\nchoice of aligner should be carefully made according to the analysis\ngoals/requirements. Here we will use Tophat2, a widely used ultrafast\naligner that performs spliced alignments.\n\n\nTophat2 is based on the Bowtie2 aligner and uses an indexed genome for\nthe alignment to speed up the alignment and keep its memory footprint\nsmall. The the index for the \nDanio rerio\n genome has been created for\nyou.\n\n\n\n\nSTOP\n\n\nThe command to create an index is as follows. You DO NOT need to run this command yourself - we have done this for you.\n\n\nbowtie2-build genome/Danio_rerio.Zv9.66.dna.fa genome/ZV9\n\n\n\n\nTophat2 has a number of parameters in order to perform the alignment. To\nview them all type:\n\n\n1\ntophat2 --help\n\n\n\n\n\n\nThe general format of the tophat2 command is:\n\n\n1\ntophat2 [options]* \nindex_base\n \nreads_1\n \nreads_2\n\n\n\n\n\n\n\nWhere the last two arguments are the \n.fastq\n files of the paired end\nreads, and the argument before is the basename of the indexed genome.\n\n\nThe quality values in the FASTQ files used in this hands-on session are\nPhred+33 encoded. We explicitly tell tophat of this fact by using the\ncommand line argument \n\u2013solexa-quals\n.\n\n\nYou can look at the first few reads in the file \ndata/2cells_1.fastq\n\nwith:\n\n\n1\nhead -n 20 data/2cells_1.fastq\n\n\n\n\n\n\nSome other parameters that we are going to use to run Tophat are listed\nbelow:\n\n\n-g\n:   Maximum number of multihits allowed. Short reads are likely to map\n    to more than one location in the genome even though these reads can\n    have originated from only one of these regions. In RNA-seq we allow\n    for a limited number of multihits, and in this case we ask Tophat to\n    report only reads that map at most onto 2 different loci.\n\n\n\u2013library-type\n:   Before performing any type of RNA-seq analysis you need to know a\n    few things about the library preparation. Was it done using a\n    strand-specific protocol or not? If yes, which strand? In our data\n    the protocol was NOT strand specific.\n\n\n-j\n:   Improve spliced alignment by providing Tophat with annotated splice\n    junctions. Pre-existing genome annotation is an advantage when\n    analysing RNA-seq data. This file contains the coordinates of\n    annotated splice junctions from Ensembl. These are stored under the\n    sub-directory \nannotation\n in a file called \nZV9.spliceSites\n.\n\n\n-o\n:   This specifies in which subdirectory Tophat should save the output\n    files. Given that for every run the name of the output files is the\n    same, we specify different directories for each run.\n\n\nIt takes some time (approx. 20 min) to perform tophat spliced\nalignments, on this small subset of reads. Therefore, we have\npre-aligned the \n2cells\n data for you using the following command:\n\n\nYou DO NOT need to run this command yourself - we have done this for\nyou.\n\n\n1\ntophat2 --solexa-quals -g 2 --library-type fr-unstranded -j annotation/Danio_rerio.Zv9.66.spliceSites -o tophat/ZV9_2cells genome/ZV9 data/2cells_1.fastq data/2cells_2.fastq\n\n\n\n\n\n\nAlign the \n6h\n data yourself using the following command:\n\n\n1\n2\n# Takes approx. 20mins\ntophat2 --solexa-quals -g 2 --library-type fr-unstranded -j annotation/Danio_rerio.Zv9.66.spliceSites -o tophat/ZV9_6h genome/ZV9 data/6h_1.fastq data/6h_2.fastq\n\n\n\n\n\n\nThe \n6h\n read alignment will take approx. 20 min to complete. Therefore,\nwe\u2019ll take a look at some of the files, generated by tophat, for the\npre-computed \n2cells\n data.\n\n\nTophat generates several files in the specified output directory. The\nmost important files are listed below.\n\n\naccepted_hits.bam\n:   This file contains the list of read alignments in BAM format.\n\n\nalign_summary.txt\n:   Provides a detailed summary of the read-alignments.\n\n\nunmapped.bam\n:   This file contains the unmapped reads.\n\n\nThe complete documentation can be found at:\n\nhttps://ccb.jhu.edu/software/tophat/manual.shtml\n\n\nAlignment Visualisation in IGV\n\n\nThe Integrative Genomics Viewer (IGV) is able to provide a visualisation\nof read alignments given a reference sequence and a BAM file. We\u2019ll\nvisualise the information contained in the \naccepted_hits.bam\n and\n\njunctions.bed\n files for the pre-computed \n2cells\n data. The former,\ncontains the tophat spliced alignments of the reads to the reference\nwhile the latter stores the coordinates of the splice junctions present\nin the data set.\n\n\nOpen the \nrnaseq\n directory on your Desktop and double-click the\n\ntophat\n subdirectory and then the \nZV9_2cells\n directory.\n\n\n\n\n\n\nLaunch IGV by double-clicking the \u201cIGV 2.3.*\u201d icon on the Desktop\n    (ignore any warnings that you may get as it opens). \nNOTE: IGV may\n    take several minutes to load for the first time, please be patient.\n\n\n\n\n\n\nChoose \u201cZebrafish (Zv9)\u201d from the drop-down box in the top left of\n    the IGV window. Else you can also load the genome fasta file.\n\n\n\n\n\n\nLoad the \naccepted_hits.sorted.bam\n file by clicking the \u201cFile\u201d\n    menu, selecting \u201cLoad from File\u201d and navigating to the\n    \nDesktop/rnaseq/tophat/ZV9_2cells\n directory.\n\n\n\n\n\n\nRename the track by right-clicking on its name and choosing \u201cRename\n    Track\u201d. Give it a meaningful name like \u201c2cells BAM\u201d.\n\n\n\n\n\n\nLoad the \njunctions.bed\n from the same directory and rename the\n    track \u201c2cells Junctions BED\u201d.\n\n\n\n\n\n\nLoad the Ensembl annotations file \nDanio_rerio.Zv9.66.gtf\n stored in\n    the \nrnaseq/annotation\n directory.\n\n\n\n\n\n\nNavigate to a region on chromosome 12 by typing\n    \nchr12\n:\n20\n,\n270\n,\n921\n-\n20\n,\n300\n,\n943\n into the search box at the top of the\n    IGV window.\n\n\n\n\n\n\nKeep zooming to view the bam file alignments\n\n\nSome useful IGV manuals can be found below\n\n\nhttp://www.broadinstitute.org/software/igv/interpreting_insert_size\n\n\nhttp://www.broadinstitute.org/software/igv/alignmentdata\n  \n\n\n\n\nQuestion\n\n\nDoes the file \u2019align_summary.txt\u2019 look interesting? What information does it provide?\n\n\n\n\nAnswer\nAs the name suggests, the file provides a details summary of the alignment statistics.\n\n\nOne other important file is \u2019unmapped.bam\u2019. This file contains the\nunampped reads.\n\n\n\n\nQuestion\n\n\nCan you identify the splice junctions from the BAM file?\n\n\n\n\nAnswer\nSplice junctions can be identified in the alignment BAM files. These are the aligned RNA-Seq reads that have skipped-bases from the reference genome (most likely introns).\n\n\n\n\nQuestion\n\n\nAre the junctions annotated for \nCBY1\n consistent with the annotation?\n\n\n\n\nAnswer\nRead alignment supports an extended length in exon 5 to the gene model(cby1-001)\n\n\nOnce tophat finishes aligning the 6h data you will need to sort the\nalignments found in the BAM file and then index the sorted BAM file.\n\n\n1\n2\nsamtools sort tophat/ZV9_6h/accepted_hits.bam -o tophat/ZV9_6h/accepted_hits.sorted.bam\nsamtools index tophat/ZV9_6h/accepted_hits.sorted.bam\n\n\n\n\n\n\nLoad the sorted BAM file into IGV, as described previously, and rename\nthe track appropriately.\n\n\nGenerating Gene Counts\n\n\nIn RNA-seq experiments the digital gene expression is recorded as the\ngene counts or number of reads aligning to a known gene feature. If you\nhave a well annotated genome, you can use the gene structure file in a\nstandard gene annotation format (GTF or GFF)) along with the spliced\nalignment file to quantify the known genes. We will demonstrate a\nutility called \nFeatureCounts\n that comes with the \nSubread\n package.\n\n\n1\n2\nmkdir gene_counts\nfeatureCounts -a annotation/Danio_rerio.Zv9.66.gtf -t exon -g gene_id -o gene_counts/gene_counts.txt tophat/ZV9_6h/accepted_hits.sorted.bam tophat/ZV9_2cells/accepted_hits.sorted.bam\n\n\n\n\n\n\nIsoform Expression and Transcriptome Assembly\n\n\nFor non-model organisms and genomes with draft assemblies and incomplete\nannotations, it is a common practice to take an assembly based approach\nto generate gene structures followed by the quantification step. There\nare a number of reference based transcript assemblers available that can\nbe used for this purpose such as, cufflinks and stringy. These assemblers\ncan give gene or isoform level assemblies that can be used to perform a\ngene/isoform level quantification. These assemblers require an alignment\nof reads with a reference genome or transcriptome as an input. The\nsecond optional input is a known gene structure in \nGTF\n or \nGFF\n\nformat.\n\n\nThere are a number of tools that perform reconstruction of the\ntranscriptome and for this workshop we are going to use Cufflinks.\nCufflinks can do transcriptome assembly either \nab initio\n or using a\nreference annotation. It also quantifies the isoform expression in\nFragments Per Kilobase of exon per Million fragments mapped (FPKM).\n\n\nCufflinks has a number of parameters in order to perform transcriptome\nassembly and quantification. To view them all type:\n\n\n1\ncufflinks --help\n\n\n\n\n\n\nWe aim to reconstruct the transcriptome for both samples by using the\nEnsembl annotation both strictly and as a guide. In the first case\nCufflinks will only report isoforms that are included in the annotation,\nwhile in the latter case it will report novel isoforms as well.\n\n\nThe Ensembl annotation for \nDanio rerio\n is available in\n\nannotation/Danio_rerio.Zv9.66.gtf\n.\n\n\nThe general format of the \ncufflinks\n command is:\n\n\n1\ncufflinks [options]* \naligned_reads.(sam|bam)\n\n\n\n\n\n\n\nWhere the input is the aligned reads (either in SAM or BAM format).\n\n\nSome of the available parameters for Cufflinks that we are going to use\nto run Cufflinks are listed below:\n\n\n-o\n:   Output directory.\n\n\n-G\n:   Tells Cufflinks to use the supplied GTF annotations strictly in\n    order to estimate isoform annotation.\n\n\n-b\n:   Instructs Cufflinks to run a bias detection and correction algorithm\n    which can significantly improve accuracy of transcript abundance\n    estimates. To do this Cufflinks requires a multi-fasta file with the\n    genomic sequences against which we have aligned the reads.\n\n\n-u\n:   Tells Cufflinks to do an initial estimation procedure to more\n    accurately weight reads mapping to multiple locations in the genome\n    (multi-hits).\n\n\n\u2013library-type\n:   Before performing any type of RNA-seq analysis you need to know a\n    few things about the library preparation. Was it done using a\n    strand-specific protocol or not? If yes, which strand? In our data\n    the protocol was NOT strand specific.\n\n\nPerform transcriptome assembly, strictly using the supplied GTF\nannotations, for the \n2cells\n and \n6h\n data using cufflinks:\n\n\n1\n2\n3\n4\n# 2cells data (takes approx. 5mins):\ncufflinks -o cufflinks/ZV9_2cells_gtf -G annotation/Danio_rerio.Zv9.66.gtf -b genome/Danio_rerio.Zv9.66.dna.fa -u --library-type fr-unstranded tophat/ZV9_2cells/accepted_hits.bam\n# 6h data (takes approx. 5mins):\ncufflinks -o cufflinks/ZV9_6h_gtf -G annotation/Danio_rerio.Zv9.66.gtf -b genome/Danio_rerio.Zv9.66.dna.fa -u --library-type fr-unstranded tophat/ZV9_6h/accepted_hits.bam\n\n\n\n\n\n\nCufflinks generates several files in the specified output directory.\nHere\u2019s a short description of these files:\n\n\ngenes.fpkm_tracking\n:   Contains the estimated gene-level expression values.\n\n\nisoforms.fpkm_tracking\n:   Contains the estimated isoform-level expression values.\n\n\nskipped.gtf\n:   Contains loci skipped as a result of exceeding the maximum number of\n    fragments.\n\n\ntranscripts.gtf\n:   This GTF file contains Cufflinks\u2019 assembled isoforms.\n\n\nThe complete documentation can be found at:\n\n\nhttp://cole-trapnell-lab.github.io/cufflinks/file_formats/#output-formats-used-in-the-cufflinks-suite\n\n\nSo far we have forced cufflinks, by using the \n-G\n option, to strictly\nuse the GTF annotations provided and thus novel transcripts will not be\nreported. We can get cufflinks to perform a GTF-guided transcriptome\nassembly by using the \n-g\n option instead. Thus, novel transcripts will\nbe reported.\n\n\nGTF-guided transcriptome assembly is more computationally intensive than\nstrictly using the GTF annotations. Therefore, we have pre-computed\nthese GTF-guided assemblies for you and have placed the results under\nsubdirectories:\n\n\ncufflinks/ZV9_2cells_gtf_guided\n and \ncufflinks/ZV9_6h_gft_guided\n.\n\n\nYou DO NOT need to run these commands. We provide them so you know how\nwe generated the the GTF-guided transcriptome assemblies:\n\n\n1\n2\n3\n4\n# 2cells guided transcriptome assembly (takes approx. 30mins):\ncufflinks -o cufflinks/ZV9_2cells_gtf_guided -g annotation/Danio_rerio.Zv9.66.gtf -b genome/Danio_rerio.Zv9.66.dna.fa -u --library-type fr-unstranded tophat/ZV9_2cells/accepted_hits.bam\n# 6h guided transcriptome assembly (takes approx. 30mins):\ncufflinks -o cufflinks/ZV9_6h_gtf_guided -g annotation/Danio_rerio.Zv9.66.gtf -b genome/Danio_rerio.Zv9.66.dna.fa -u --library-type fr-unstranded tophat/ZV9_6h/accepted_hits.bam\n\n\n\n\n\n\n\n\n\n\nGo back to IGV and load the pre-computed, GTF-guided transcriptome\n    assembly for the \n2cells\n data\n    (\ncufflinks/ZV9_2cells_gtf_guided/transcripts.gtf\n).\n\n\n\n\n\n\nRename the track as \u201c2cells GTF-Guided Transcripts\u201d.\n\n\n\n\n\n\nIn the search box type \nENSDART00000082297\n in order for the browser\n    to zoom in to the gene of interest.\n\n\n\n\n\n\n\n\nQuestion\n\n\nDo you observe any difference between the Ensembl GTF annotations and the GTF-guided transcripts \nassembled by cufflinks (the \u201c2cells GTF-Guided Transcripts\u201d track)?\n\n\n\n\nAnswer\nYes. It appears that the Ensembl annotations may have truncated the last exon. \nHowever, our data also doesn\u2019t contain reads that span between the last two exons.\n\n\nDifferential Gene Expression Analysis using edgeR\n\n\nExperiment Design\n\n\nThe example we are working through today follows a case Study set out in\nthe edgeR Users Guide (4.3 Androgen-treated prostate cancer cells\n(RNA-Seq, two groups)) which is based on an experiment conducted by Li et\nal. (2008, Proc Natl Acad Sci USA, 105, 20179-84).\n\n\nThe researchers used a prostate cancer cell line (LNCaP cells). These\ncells are sensitive to stimulation by male hormones (androgens). Three\nreplicate RNA samples were collected from LNCaP cells treated with an\nandrogen hormone (DHT). Four replicates were collected from cells\ntreated with an inactive compound. Each of the seven samples was run on\na lane (7 lanes) of an Illumina flow cell to produce 35 bp reads. The\nexperimental design was therefore 4 control samples vs 3 treated samples.\n\n\nThis workflow requires raw gene count files and these can be generated\nusing a utility called featureCounts as demonstrated above. We are using\na pre-computed gene counts data (stored in \npnas_expression.txt\n) for\nthis exercise.\n\n\nPrepare the Environment\n\n\nPrepare the environment and load R:\n\n\n1\n2\ncd /home/trainee/rnaseq/edgeR\nR (press enter)\n\n\n\n\n\n\nOnce on the R prompt. Load libraries:\n\n\n1\n2\n3\n4\n5\n6\nlibrary(edgeR)\nlibrary(biomaRt)\nlibrary(gplots)\nlibrary(limma)\nlibrary(RColorBrewer)\nlibrary(org.Hs.eg.db)\n\n\n\n\n\n\nRead in Data\n\n\nRead in count table and experimental design:\n\n\n1\n2\n3\n4\ndata \n-\n read.delim\n(\npnas_expression.txt\n,\n row.names\n=\n1\n,\n header\n=\nT\n)\n\ntargets \n-\n read.delim\n(\nTargets.txt\n,\n header\n=\nT\n)\n\n\ncolnames\n(\ndata\n)\n \n-\ntargets\n$\nLabel\n\nhead\n(\ndata\n,\n n\n=\n20\n)\n\n\n\n\n\n\n\nAdd Gene annotation\n\n\nThe data set only includes the Ensembl gene id and the counts. It is\nuseful to have other annotations such as the gene symbol and entrez id.\nNext we will add in these annotations. We will use the BiomaRt package\nto do this.\n\n\nWe start by using the useMart function of BiomaRt to access the human\ndata base of ensemble gene ids.\n\n\n1\n2\nhuman\n-\nuseMart\n(\nhost\n=\nwww.ensembl.org\n,\n \nENSEMBL_MART_ENSEMBL\n,\n dataset\n=\nhsapiens_gene_ensembl\n)\n \nattributes\n=\nc\n(\nensembl_gene_id\n,\n \nentrezgene\n,\nhgnc_symbol\n)\n\n\n\n\n\n\n\nWe create a vector of our ensemble gene ids.\n\n\n1\n2\nensembl_names\n-\nrownames\n(\ndata\n)\n\n\nhead\n(\nensembl_names\n)\n\n\n\n\n\n\n\nWe then use the function getBM to get the gene symbol data we want.This\ntakes about a minute.\n\n\n1\ngenemap\n-\ngetBM\n(\nattributes\n,\n filters\n=\nensembl_gene_id\n,\n values\n=\nensembl_names\n,\n mart\n=\nhuman\n)\n\n\n\n\n\n\n\nHave a look at the start of the genemap dataframe.\n\n\n1\nhead(genemap)\n\n\n\n\n\n\nWe then match the data we have retrieved to our dataset.\n\n\n1\n2\n3\n4\n5\n6\nidx \n-\nmatch\n(\nensembl_names\n,\n genemap\n$\nensembl_gene_id\n)\n\ndata\n$\nentrezgene \n-\ngenemap\n$\nentrezgene \n[\n idx \n]\n\ndata\n$\nhgnc_symbol \n-\ngenemap\n$\nhgnc_symbol \n[\n idx \n]\n\nAnn \n-\n \ncbind\n(\nrownames\n(\ndata\n),\n data\n$\nhgnc_symbol\n,\n data\n$\nentrezgene\n)\n\n\ncolnames\n(\nAnn\n)\n-\nc\n(\nEnsembl\n,\n \nSymbol\n,\n \nEntrez\n)\n\nAnn\n-\nas.data.frame\n(\nAnn\n)\n\n\n\n\n\n\n\nLet\u2019s check and see that this additional information is there.\n\n\n1\nhead(data)\n\n\n\n\n\n\nData checks\n\n\nCreate DGEList object:\n\n\n1\n2\ntreatment \n-\nfactor\n(\nc\n(\nrep\n(\nControl\n,\n4\n),\n \nrep\n(\nDHT\n,\n3\n)),\n levels\n=\nc\n(\nControl\n,\n \nDHT\n))\n\ny \n-\nDGEList\n(\ncounts\n=\ndata\n[,\n1\n:\n7\n],\n group\n=\ntreatment\n,\n genes\n=\nAnn\n)\n\n\n\n\n\n\n\nCheck the dimensions of the object:\n\n\n1\ndim(y)\n\n\n\n\n\n\nWe see we have 37435 rows (i.e. genes) and 7 columns (samples).\n\n\nNow we will filter out genes with low counts by only keeping those rows\nwhere the count per million (cpm) is at least 1 in at least three\nsamples:\n\n\n1\n2\nkeep \n-\nrowSums\n(\n cpm\n(\ny\n)\n1\n)\n \n=\n3\n\ny \n-\n y\n[\nkeep\n,\n \n]\n\n\n\n\n\n\n\nCheck how many rows (genes) are retained now.\n\n\n1\ndim(y)\n\n\n\n\n\n\nThis gives you 16494 rows indicating that 20941 (37435-16494) genes were filtered out.\n\n\nAs we have removed the lowly expressed genes the total number of counts\nper sample has not changed greatly. Let us check the total number of\nreads per sample in the original data (data) and now after filtering.\n\n\nBefore:\n\n\n1\ncolSums(data[,1:7])\n\n\n\n\n\n\nAfter filtering:\n\n\n1\ncolSums(y$counts)\n\n\n\n\n\n\nWe will now perform normalization to take account of different library\nsize:\n\n\n1\ny\n-\ncalcNormFactors\n(\ny\n)\n\n\n\n\n\n\n\nWe will check the calculated normalization factors:\n\n\n1\ny$samples\n\n\n\n\n\n\nLets have a look at whether the samples cluster by condition. (You\nshould produce a plot as shown in Figure 4):\n\n\n1\nplotMDS(y, col=as.numeric(y$samples$group))\n\n\n\n\n\n\n[H] \n [fig:MDS plot]\n\n\n\n\nQuestion\n\n\nDoes the MDS plot indicate a difference in gene expression between the Controls and the DHT treated samples?\n\n\n\n\nAnswer\nThe MDS plot shows us that the controls are separated from the DHT treated cells. This indicates that there is a difference in gene expression between the conditions.\n\n\nWe will now estimate the dispersion. We start by estimating the common\ndispersion. The common dispersion estimates the overall Biological\nCoefficient of Variation (BCV) of the dataset averaged over all genes.\n\n\nBy using verbose we get the Disp and BCV values printed on the screen\n\n\n1\ny \n-\n estimateCommonDisp\n(\ny\n,\n verbose\n=\nT\n)\n\n\n\n\n\n\n\nWhat value to you see for BCV?\n\n\nWe now estimate gene-specific dispersion.\n\n\n1\ny \n-\n estimateTagwiseDisp\n(\ny\n)\n\n\n\n\n\n\n\nWe will plot the tagwise dispersion and the common dispersion (You\nshould obtain a plot as shown in the Figure 5):\n\n\n1\nplotBCV(y)\n\n\n\n\n\n\n[H] \n [fig:BCV plot]\n\n\nWe see here that the common dispersion estimates the overall Biological\nCoefficient of Variation (BCV) of the dataset averaged over all genes.\nThe common dispersion is \n0.02\n and the BCV is the square root of the\ncommon dispersion (sqrt[0.02] = 0.14). A BCV of 14% is typical for cell\nline experiment.\n\n\nAs you can see from the plot the BCV of some genes (generally those with\nlow expression) can be much higher than the common dispersion. For\nexample we see genes with a reasonable level of expression with tagwise\ndispersion of 0.4 indicating 40% variation between samples.\n\n\n\n\nQuestion\n\n\nIf we used the common dispersion for these genes instead of the tagwise dispersion what effect would this have?\n\n\n\n\nAnswer\nIf we simply used the common dispersion for these genes we would underestimate biological variability, which in turn affects whether these genes would be identified as being differentially expressed between conditions. \n\n\nIt is recommended to use the tagwise dispersion,which takes account of gene-to-gene variability.\n\n\nNow that we have normalized our data and also calculated the variability\nof gene expression between samples we are in a position to perform\ndifferential expression testing.As this is a simple comparison between\ntwo conditions, androgen treatment and placebo treatment we can use the\nexact test for the negative binomial distribution (Robinson and Smyth,\n2008).\n\n\nTesting for Differential Expression\n\n\nWe now test for differentially expressed BCV genes:\n\n\n1\net \n-\n exactTest\n(\ny\n)\n\n\n\n\n\n\n\nNow we will use the topTags function to adjust for multiple testing. We\nwill use the Benjimini Hochberg (\nBH\n) method and we will produce a\ntable of results:\n\n\n1\nres \n-\n topTags\n(\net\n,\n n\n=\nnrow\n(\ny\n$\ncounts\n),\n adjust.method\n=\nBH\n)\n$\ntable\n\n\n\n\n\n\n\nLet\u2019s have a look at the first rows of the table:\n\n\n1\nhead(res)\n\n\n\n\n\n\nTo get a summary of the number of differentially expressed genes we can\nuse the decideTestsDGE function.\n\n\n1\nsummary\n(\nde \n-\n decideTestsDGE\n(\net\n))\n\n\n\n\n\n\n\nThis tells us that 2086 genes are downregulated and 2345 genes are\nupregulated at 5% FDR.We will now make subsets of the most significant\nupregulated and downregulated genes using a log fold change threshhold of 1.5.\n\n\n1\n2\n3\n4\n5\n6\n7\nalpha\n=\n0.05\n\nlfc\n=\n1.5\n\nedgeR_res_sig\n-\nres\n[\nres\n$\nFDR\nalpha\n,]\n\nedgeR_res_sig_lfc \n-\nedgeR_res_sig\n[\nabs\n(\nedgeR_res_sig\n$\nlogFC\n)\n \n=\n lfc\n,]\n\n\nhead\n(\nedgeR_res_sig\n,\n n\n=\n20\n)\n\n\nnrow\n(\nedgeR_res_sig\n)\n\n\nnrow\n(\nedgeR_res_sig_lfc\n)\n\n\n\n\n\n\n\nWe can write out these results to our current directory.\n\n\n1\n2\nwrite.table(edgeR_res_sig , \nedgeR_res_sig.txt\n, sep=\n\\t\n, col.names=NA, quote=F)\nwrite.table(edgeR_res_sig_lfc , \nedgeR_res_sig_lfc.txt\n, sep=\n\\t\n, col.names=NA, quote=F)\n\n\n\n\n\n\n\n\nQuestion\n\n\nHow many differentially expressed genes are are found by edgeR before and after requiring the log fold change of 1.5?\n\n\n\n\nAnswer\nThere are 4431 DEGs at an FDR = 0.05 which reduces to 1148 if we require a log fold change of 1.5.\n\n\nDifferential expression using the Voom function and the limma package\n\n\nWe will now show an alternative approach to differential expression\nwhich uses the \nlimma\n package.This is based on linear models. The first\nstep is to create a design matrix. In this case we have a simple design\nwhere we have only one condition (treated vs non-treated). However, you\nmay be dealing with more complex experimental designs, for example\nlooking at treatment and other covariates, such as age, gender, batch.\n\n\n1\n2\n3\ndesign \n-\nmodel.matrix\n(\n~\ntreatment\n)\n\ncheck design\n\nprint\n(\ndesign\n)\n\n\n\n\n\n\n\nWe now use voom to transform the data into a form which is appropriate\nfor linear modelling.\n\n\n1\nv \n-\nvoom\n(\ny\n,\n design\n)\n\n\n\n\n\n\n\nNext we will fit linear model to each gene in the dataset using the\nfunction lmFit. Following this we use the function eBayes to test each\ngene to find whether foldchange between the conditions being tested is\nstatistically significant.We filter our results by using the same values\nof alpha (0.05) and log fold change (1.5) used previously.\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\nfit_v \n-\nlmFit\n(\nv\n,\n design\n)\n\n\nfit_v \n-\n eBayes\n(\nfit_v\n)\n\n\nvoom_res\n-\ntopTable\n(\nfit_v\n,\n coef\n=\n2\n,\nadjust.method\n=\nBH\n,\n sort.by\n=\nP\n,\n n\n=\nnrow\n(\ny\n$\ncounts\n))\n\n\nvoom_res_sig \n-\nvoom_res\n[\nvoom_res\n$\nadj.P.Val \nalpha\n,]\n\n\nvoom_res_sig_lfc \n-\nvoom_res_sig\n[\nabs\n(\nvoom_res_sig\n$\nlogFC\n)\n \n=\n lfc\n,]\n\n\n\n\n\n\n\nHow many differentially expressed genes are identified?\n\n\n1\n2\nnrow(voom_res_sig)\nnrow(voom_res_sig_lfc)\n\n\n\n\n\n\n\n\nQuestion\n\n\nHow many differentially expressed genes are found using voom before and after requiring the log fold change of 1.5?\n\n\n\n\nAnswer\nThere are 4103 DEGs at an FDR = 0.05 which reduces to 1109 if we require a log fold change of 1.5.\n\n\nWe will write out these results.\n\n\n1\n2\n3\nwrite.table(voom_res_sig, \nvoom_res_sig.txt\n, sep=\n\\t\n, col.names=NA, quote=F)\nwrite.table(voom_res_sig_lfc, \nvoom_res_sig_lfc.txt\n, sep=\n\\t\n, col.names=NA, quote=F)\nwrite.table(voom_res, \nvoom_res.txt\n, sep=\n\\t\n, col.names=NA, quote=F)\n\n\n\n\n\n\nData Visualisation\n\n\nNow let\u2019s visualize some of this data. First we will make a volcano plot\nusing the \nvolcanoplot\n function available in \nlimma\n. This creates a\nplot which displays fold changes versus a measure of statistical\nsignificance of the change.\n\n\n1\nvolcanoplot(fit_v, coef=2, highlight=5)\n\n\n\n\n\n\n[H] \n [fig:Volcano plot]\n\n\nNext we will create a heatmap of the top differentially expressed genes.\nWe use the heatmap.2 function available in the gplots package.\n\n\n1\n2\n3\nselect_top  \n-\n p.adjust\n(\nfit_v\n$\np.value\n[,\n \n2\n])\n \n1e-2\n\nExp_top \n-\n v\n$\nE \n[\nselect_top\n,\n \n]\n\nheatmap.2\n(\nExp_top\n,\n scale\n=\nrow\n,\n density.info\n=\nnone\n,\n trace\n=\nnone\n,\n main\n=\nTop DEGs\n,\n labRow\n=\n,\n cexRow\n=\n0.4\n,\n cexCol\n=\n0.8\n)\n\n\n\n\n\n\n\n[H] \n [fig:Heatmap]\n\n\nYou can now quit the R prompt\n\n\n1\nq()\n\n\n\n\n\n\nYou can save your workspace by typing \nY\n on prompt.\n\n\nPlease note that the output files you are creating are saved in your\npresent working directory. If you are not sure where you are in the file\nsystem try typing \npwd\n on your command prompt to find out.\n\n\nDifferential Expression using cuffdiff\n\n\nThis is optional exercise and will be run if time permits.\n\n\nNOTE: If this exercise is to be attempted it is necessary to leave the directory we have been working in and go back to the zebra fish data. Commands should be executed from the rnaseq directory.\n\n\nOne of the stand-alone tools that perform differential expression\nanalysis is Cuffdiff. We use this tool to compare between two\nconditions; for example different conditions could be control and\ndisease, or wild-type and mutant, or various developmental stages.\n\n\nIn our case we want to identify genes that are differentially expressed\nbetween two developmental stages; a \n2cells\n embryo and \n6h\n post\nfertilization.\n\n\nThe general format of the cuffdiff command is:\n\n\n1\ncuffdiff [options]* \ntranscripts.gtf\n \nsample1_replicate1.sam[,...,sample1_replicateM]\n \nsample2_replicate1.sam[,...,sample2_replicateM.sam]\n\n\n\n\n\n\n\nWhere the input includes a \ntranscripts.gtf\n file, which is an\nannotation file of the genome of interest or the cufflinks assembled\ntranscripts, and the aligned reads (either in SAM or BAM format) for the\nconditions. Some of the Cufflinks options that we will use to run the\nprogram are:\n\n\n-o\n:   Output directory.\n\n\n-L\n:   Labels for the different conditions\n\n\n-T\n:   Tells Cuffdiff that the reads are from a time series experiment.\n\n\n-b\n:   Instructs Cufflinks to run a bias detection and correction algorithm\n    which can significantly improve accuracy of transcript abundance\n    estimates. To do this Cufflinks requires a multi-fasta file with the\n    genomic sequences against which we have aligned the reads.\n\n\n-u\n:   Tells Cufflinks to do an initial estimation procedure to more\n    accurately weight reads mapping to multiple locations in the genome\n    (multi-hits).\n\n\n\u2013library-type\n:   Before performing any type of RNA-seq analysis you need to know a\n    few things about the library preparation. Was it done using a\n    strand-specific protocol or not? If yes, which strand? In our data\n    the protocol was NOT strand specific.\n\n\n-C\n:   Biological replicates and multiple group contrast can be defined\n    here\n\n\nRun cuffdiff on the tophat generated BAM files for the 2cells vs. 6h\ndata sets:\n\n\n1\ncuffdiff -o cuffdiff/ -L ZV9_2cells,ZV9_6h -T -b genome/Danio_rerio.Zv9.66.dna.fa -u --library-type fr-unstranded annotation/Danio_rerio.Zv9.66.gtf tophat/ZV9_2cells/accepted_hits.bam tophat/ZV9_6h/accepted_hits.bam\n\n\n\n\n\n\nWe are interested in the differential expression at the gene level. The\nresults are reported by Cuffdiff in the file \ncuffdiff/gene_exp.diff\n.\nLook at the first few lines of the file using the following command:\n\n\n1\nhead -n 20 cuffdiff/gene_exp.diff\n\n\n\n\n\n\nWe would like to see which are the most significantly differentially\nexpressed genes. Therefore we will sort the above file according to the\nq value (corrected p value for multiple testing). The result will be\nstored in a different file called \ngene_exp_qval.sorted.diff\n.\n\n\n1\nsort -t$\n\\t\n -g -k 13 cuffdiff/gene_exp.diff \n cuffdiff/gene_exp_qval.sorted.diff\n\n\n\n\n\n\nLook again at the top 20 lines of the sorted file by typing:\n\n\n1\nhead -n 20 cuffdiff/gene_exp_qval.sorted.diff\n\n\n\n\n\n\nCopy an Ensembl transcript identifier from the first two columns for one\nof these genes (e.g. \nENSDARG00000045067\n). Now go back to the IGV\nbrowser and paste it in the search box.\n\n\nWhat are the various outputs generated by cuffdiff? Hint: Please refer\nto the \nCuffdiff output\n section of the cufflinks manual online.\n\n\nDo you see any difference in the read coverage between the \n2cells\n and\n\n6h\n conditions that might have given rise to this transcript being\ncalled as differentially expressed?\n\n\nThe coverage on the Ensembl browser is based on raw reads and no\nnormalisation has taken place contrary to the FPKM values.\n\n\nThe read coverage of this transcript (\nENSDARG00000045067\n) in the 6h\ndata set is much higher than in the 2cells data set.\n\n\nVisualising the CuffDiff expression analysis\n\n\nWe will use an R-Bioconductor package called \ncummeRbund\n to visualise,\nmanipulate and explore Cufflinks RNA-seq output. We will load an R\nenvironment and look at few quick tips to generate simple graphical\noutput of the cufflinks analysis we have just run.\n\n\nCummeRbund\n takes the cuffdiff output and populates a SQLite database\nwith various type of output generated by cuffdiff e.g, genes,\ntranscripts, transcription start site, isoforms and CDS regions. The\ndata from this database can be accessed and processed easily. This\npackage comes with a number of in-built plotting functions that are\ncommonly used for visualising the expression data. We strongly recommend\nreading through the bioconductor manual and user guide of CummeRbund to\nlearn about functionality of the tool. The reference is provided in the\nresource section.\n\n\nPrepare the environment. Go to the \ncuffdiff\n output folder and copy the\ntranscripts file there.\n\n\n1\n2\n3\ncd /home/trainee/rnaseq/cuffdiff\ncp /home/trainee/rnaseq/annotation/Danio_rerio.Zv9.66.gtf /home/trainee/rnaseq/cuffdiff\nls -l\n\n\n\n\n\n\nLoad the R environment\n\n\n1\nR (press enter)\n\n\n\n\n\n\nLoad the require R package.\n\n\n1\nlibrary(cummeRbund)\n\n\n\n\n\n\nRead in the cuffdiff output\n\n\n1\ncuff\n-\nreadCufflinks\n(\ndir\n=\n/home/trainee/rnaseq/cuffdiff\u201d, gtfFile=\nDanio_rerio.Zv9.66.gtf\n,genome=\nZv9\n, rebuild=T)\n\n\n\n\n\n\n\nAssess the distribution of FPKM scores across samples\n\n\n1\n2\n3\n4\npdf\n(\nfile \n=\n \nSCV.pdf\n,\n height \n=\n \n6\n,\n width \n=\n \n6\n)\n\ndens\n-\ncsDensity\n(\ngenes\n(\ncuff\n))\n\ndens\ndev.off\n()\n\n\n\n\n\n\n\nBox plots of the FPKM values for each samples\n\n\n1\n2\n3\n4\npdf\n(\nfile \n=\n \nBoxP.pdf\n,\n height \n=\n \n6\n,\n width \n=\n \n6\n)\n\nb\n-\ncsBoxplot\n(\ngenes\n(\ncuff\n))\n\nb\ndev.off\n()\n\n\n\n\n\n\n\nAccessing the data\n\n\n1\n2\n3\n4\n5\n6\nsigGeneIds\n-\ngetSig\n(\ncuff\n,\nalpha\n=\n0.05\n,\nlevel\n=\ngenes\n)\n\n\nhead\n(\nsigGeneIds\n)\n\nsigGenes\n-\ngetGenes\n(\ncuff\n,\nsigGeneIds\n)\n\nsigGenes\n\nhead\n(\nfpkm\n(\nsigGenes\n))\n\n\nhead\n(\nfpkm\n(\nisoforms\n(\nsigGenes\n)))\n\n\n\n\n\n\n\nPlotting a heatmap of the differentially expressed genes\n\n\n1\n2\n3\n4\npdf\n(\nfile \n=\n \nheatmap.pdf\n,\n height \n=\n \n6\n,\n width \n=\n \n6\n)\n\nh\n-\ncsHeatmap\n(\nsigGenes\n,\ncluster\n=\nboth\n)\n\nh\ndev.off\n()\n\n\n\n\n\n\n\nWhat options would you use to draw a density or boxplot for different\nreplicates if available ? (Hint: look at the manual at Bioconductor\nwebsite)\n\n\n1\n2\ndensRep\n-\ncsDensity\n(\ngenes\n(\ncuff\n),\nreplicates\n=\nT\n)\n\nbrep\n-\ncsBoxplot\n(\ngenes\n(\ncuff\n),\nreplicates\n=\nT\n)\n\n\n\n\n\n\n\nHow many differentially expressed genes did you observe?\n\n\ntype \u2019summary(sigGenes)\u2019 on the R prompt to see.\n\n\nReferences\n\n\n\n\n\n\nTrapnell, C., Pachter, L. \n Salzberg, S. L. TopHat: discovering\n    splice junctions with RNA-Seq. Bioinformatics 25, 1105-1111 (2009).\n\n\n\n\n\n\nTrapnell, C. et al. Transcript assembly and quantification by\n    RNA-Seq reveals unannotated transcripts and isoform switching during\n    cell differentiation. Nat. Biotechnol. 28, 511-515 (2010).\n\n\n\n\n\n\nLangmead, B., Trapnell, C., Pop, M. \n Salzberg, S. L. Ultrafast and\n    memory-efficient alignment of short DNA sequences to the human\n    genome. Genome Biol. 10, R25 (2009).\n\n\n\n\n\n\nRoberts, A., Pimentel, H., Trapnell, C. \n Pachter, L. Identification\n    of novel transcripts in annotated genomes using RNA-Seq.\n    Bioinformatics 27, 2325-2329 (2011).\n\n\n\n\n\n\nRoberts, A., Trapnell, C., Donaghey, J., Rinn, J. L. \n Pachter, L.\n    Improving RNA-Seq expression estimates by correcting for fragment\n    bias. Genome Biol. 12, R22 (2011).\n\n\n\n\n\n\nRobinson MD, McCarthy DJ and Smyth GK. edgeR: a Bioconductor package\n    for differential expression analysis of digital gene expression\n    data. Bioinformatics, 26 (2010).\n\n\n\n\n\n\nRobinson MD and Smyth GK Moderated statistical tests for assessing\n    differences in tag abundance. Bioinformatics, 23, pp. -6.\n\n\n\n\n\n\nRobinson MD and Smyth GK (2008). Small-sample estimation of negative\n    binomial dispersion, with applications to SAGE data.\u201d Biostatistics,\n    9.\n\n\n\n\n\n\nMcCarthy, J. D, Chen, Yunshun, Smyth and K. G (2012). Differential\n    expression analysis of multifactor RNA-Seq experiments with respect\n    to biological variation. Nucleic Acids Research, 40(10), pp. -9.", 
            "title": "RNA-Seq"
        }, 
        {
            "location": "/modules/btp-module-rna-seq/rna-seq/#key-learning-outcomes", 
            "text": "After completing this practical the trainee should be able to:    Understand and perform a simple RNA-Seq analysis workflow.    Perform spliced alignments to an indexed reference genome using\n    TopHat.    Visualize spliced transcript alignments in a genome browser such as\n    IGV.    Be able to identify differential gene expression between two\n    experimental conditions.    Be familiar with R environment and be able to run R based RNA-seq\n    packages.    We also have bonus exercises where you can learn to:    Perform transcript assembly using Cufflinks.    Run cuffdiff, a Cufflinks utility for differential expression\n    analysis.    Visualize transcript alignments and annotation in a genome browser\n    such as IGV.", 
            "title": "Key Learning Outcomes"
        }, 
        {
            "location": "/modules/btp-module-rna-seq/rna-seq/#resources-youll-be-using", 
            "text": "", 
            "title": "Resources You\u2019ll be Using"
        }, 
        {
            "location": "/modules/btp-module-rna-seq/rna-seq/#tools-used", 
            "text": "Tophat:  https://ccb.jhu.edu/software/tophat/index.shtml  Cufflinks:  http://cole-trapnell-lab.github.io/cufflinks/  Samtools:  http://www.htslib.org/  BEDTools:  https://github.com/arq5x/bedtools2  UCSC tools:  http://hgdownload.cse.ucsc.edu/admin/exe/  IGV:  http://www.broadinstitute.org/igv/  FeatureCount:  http://subread.sourceforge.net/  edgeR package:  https://bioconductor.org/packages/release/bioc/html/edgeR.html  CummeRbund manual:  http://www.bioconductor.org/packages/release/bioc/vignettes/cummeRbund/inst/doc/cummeRbund-manual.pdf", 
            "title": "Tools Used"
        }, 
        {
            "location": "/modules/btp-module-rna-seq/rna-seq/#sources-of-data", 
            "text": "http://www.ebi.ac.uk/ena/data/view/ERR022484  http://www.ebi.ac.uk/ena/data/view/ERR022485  http://www.pnas.org/content/suppl/2008/12/16/0807121105.DCSupplemental", 
            "title": "Sources of Data"
        }, 
        {
            "location": "/modules/btp-module-rna-seq/rna-seq/#author-information", 
            "text": "Primary Author(s): \n    Susan M Corley  \n    Sonika Tyagi       Contributor(s): \n    Nathan S. Watson-Haigh   \n    Myrto Kostadima, EMBL-EBI   \n    Remco Loos, EMBL-EBI", 
            "title": "Author Information"
        }, 
        {
            "location": "/modules/btp-module-rna-seq/rna-seq/#introduction", 
            "text": "The goal of this hands-on session is to perform some basic tasks in the\ndownstream analysis of RNA-seq data.  First we will use RNA-seq data from zebrafish. You will align one set of\nreads to the zebrafish using Tophat2. You will then view the aligned\nreads using the IGV viewer. We will also demonstrate how gene counts can\nbe derived from this data. You will go on to assembly a transcriptome\nfrom the read data using cufflinks. We will show you how this type of\ndata may be analysed for differential expression.  The second part of the tutorial will focus on RNA-seq data from a human\nexperiment (cancer cell line versus normal cells). You will use the\nBioconductor packages edgeR and voom (limma) to determine differential\ngene expression. The results from this analysis will then be used in the\nfinal session which introduces you to some of the tools used to gain\nbiological insight into the results of a differential expression\nanalysis", 
            "title": "Introduction"
        }, 
        {
            "location": "/modules/btp-module-rna-seq/rna-seq/#prepare-the-environment", 
            "text": "We will use a dataset derived from sequencing of mRNA from  Danio rerio \nembryos in two different developmental stages. Sequencing was performed\non the Illumina platform and generated 76bp paired-end sequence data\nusing polyA selected RNA. Due to the time constraints of the practical\nwe will only use a subset of the reads.  The data files are contained in the subdirectory called  data  and are\nthe following:  2cells_1.fastq  and  2cells_2.fastq \n    These files are based on RNA-seq data of a 2-cell zebrafish embryo  6h_1.fastq  and  6h_2.fastq \n    These files are based on RNA-seq data of zebrafish embryos 6h post\n    fertilization  Open the Terminal and go to the  rnaseq  working directory:  1 cd /home/trainee/rnaseq/    STOP  All commands entered into the terminal for this tutorial should be from\nwithin the  /home/trainee/rnaseq  directory.   Check that the  data  directory contains the above-mentioned files by\ntyping:  1 ls data", 
            "title": "Prepare the Environment"
        }, 
        {
            "location": "/modules/btp-module-rna-seq/rna-seq/#alignment", 
            "text": "There are numerous tools for performing short read alignment and the\nchoice of aligner should be carefully made according to the analysis\ngoals/requirements. Here we will use Tophat2, a widely used ultrafast\naligner that performs spliced alignments.  Tophat2 is based on the Bowtie2 aligner and uses an indexed genome for\nthe alignment to speed up the alignment and keep its memory footprint\nsmall. The the index for the  Danio rerio  genome has been created for\nyou.   STOP  The command to create an index is as follows. You DO NOT need to run this command yourself - we have done this for you.  bowtie2-build genome/Danio_rerio.Zv9.66.dna.fa genome/ZV9   Tophat2 has a number of parameters in order to perform the alignment. To\nview them all type:  1 tophat2 --help   The general format of the tophat2 command is:  1 tophat2 [options]*  index_base   reads_1   reads_2    Where the last two arguments are the  .fastq  files of the paired end\nreads, and the argument before is the basename of the indexed genome.  The quality values in the FASTQ files used in this hands-on session are\nPhred+33 encoded. We explicitly tell tophat of this fact by using the\ncommand line argument  \u2013solexa-quals .  You can look at the first few reads in the file  data/2cells_1.fastq \nwith:  1 head -n 20 data/2cells_1.fastq   Some other parameters that we are going to use to run Tophat are listed\nbelow:  -g\n:   Maximum number of multihits allowed. Short reads are likely to map\n    to more than one location in the genome even though these reads can\n    have originated from only one of these regions. In RNA-seq we allow\n    for a limited number of multihits, and in this case we ask Tophat to\n    report only reads that map at most onto 2 different loci.  \u2013library-type\n:   Before performing any type of RNA-seq analysis you need to know a\n    few things about the library preparation. Was it done using a\n    strand-specific protocol or not? If yes, which strand? In our data\n    the protocol was NOT strand specific.  -j\n:   Improve spliced alignment by providing Tophat with annotated splice\n    junctions. Pre-existing genome annotation is an advantage when\n    analysing RNA-seq data. This file contains the coordinates of\n    annotated splice junctions from Ensembl. These are stored under the\n    sub-directory  annotation  in a file called  ZV9.spliceSites .  -o\n:   This specifies in which subdirectory Tophat should save the output\n    files. Given that for every run the name of the output files is the\n    same, we specify different directories for each run.  It takes some time (approx. 20 min) to perform tophat spliced\nalignments, on this small subset of reads. Therefore, we have\npre-aligned the  2cells  data for you using the following command:  You DO NOT need to run this command yourself - we have done this for\nyou.  1 tophat2 --solexa-quals -g 2 --library-type fr-unstranded -j annotation/Danio_rerio.Zv9.66.spliceSites -o tophat/ZV9_2cells genome/ZV9 data/2cells_1.fastq data/2cells_2.fastq   Align the  6h  data yourself using the following command:  1\n2 # Takes approx. 20mins\ntophat2 --solexa-quals -g 2 --library-type fr-unstranded -j annotation/Danio_rerio.Zv9.66.spliceSites -o tophat/ZV9_6h genome/ZV9 data/6h_1.fastq data/6h_2.fastq   The  6h  read alignment will take approx. 20 min to complete. Therefore,\nwe\u2019ll take a look at some of the files, generated by tophat, for the\npre-computed  2cells  data.  Tophat generates several files in the specified output directory. The\nmost important files are listed below.  accepted_hits.bam\n:   This file contains the list of read alignments in BAM format.  align_summary.txt\n:   Provides a detailed summary of the read-alignments.  unmapped.bam\n:   This file contains the unmapped reads.  The complete documentation can be found at: https://ccb.jhu.edu/software/tophat/manual.shtml", 
            "title": "Alignment"
        }, 
        {
            "location": "/modules/btp-module-rna-seq/rna-seq/#alignment-visualisation-in-igv", 
            "text": "The Integrative Genomics Viewer (IGV) is able to provide a visualisation\nof read alignments given a reference sequence and a BAM file. We\u2019ll\nvisualise the information contained in the  accepted_hits.bam  and junctions.bed  files for the pre-computed  2cells  data. The former,\ncontains the tophat spliced alignments of the reads to the reference\nwhile the latter stores the coordinates of the splice junctions present\nin the data set.  Open the  rnaseq  directory on your Desktop and double-click the tophat  subdirectory and then the  ZV9_2cells  directory.    Launch IGV by double-clicking the \u201cIGV 2.3.*\u201d icon on the Desktop\n    (ignore any warnings that you may get as it opens).  NOTE: IGV may\n    take several minutes to load for the first time, please be patient.    Choose \u201cZebrafish (Zv9)\u201d from the drop-down box in the top left of\n    the IGV window. Else you can also load the genome fasta file.    Load the  accepted_hits.sorted.bam  file by clicking the \u201cFile\u201d\n    menu, selecting \u201cLoad from File\u201d and navigating to the\n     Desktop/rnaseq/tophat/ZV9_2cells  directory.    Rename the track by right-clicking on its name and choosing \u201cRename\n    Track\u201d. Give it a meaningful name like \u201c2cells BAM\u201d.    Load the  junctions.bed  from the same directory and rename the\n    track \u201c2cells Junctions BED\u201d.    Load the Ensembl annotations file  Danio_rerio.Zv9.66.gtf  stored in\n    the  rnaseq/annotation  directory.    Navigate to a region on chromosome 12 by typing\n     chr12 : 20 , 270 , 921 - 20 , 300 , 943  into the search box at the top of the\n    IGV window.    Keep zooming to view the bam file alignments  Some useful IGV manuals can be found below  http://www.broadinstitute.org/software/igv/interpreting_insert_size  http://www.broadinstitute.org/software/igv/alignmentdata      Question  Does the file \u2019align_summary.txt\u2019 look interesting? What information does it provide?   Answer As the name suggests, the file provides a details summary of the alignment statistics.  One other important file is \u2019unmapped.bam\u2019. This file contains the\nunampped reads.   Question  Can you identify the splice junctions from the BAM file?   Answer Splice junctions can be identified in the alignment BAM files. These are the aligned RNA-Seq reads that have skipped-bases from the reference genome (most likely introns).   Question  Are the junctions annotated for  CBY1  consistent with the annotation?   Answer Read alignment supports an extended length in exon 5 to the gene model(cby1-001)  Once tophat finishes aligning the 6h data you will need to sort the\nalignments found in the BAM file and then index the sorted BAM file.  1\n2 samtools sort tophat/ZV9_6h/accepted_hits.bam -o tophat/ZV9_6h/accepted_hits.sorted.bam\nsamtools index tophat/ZV9_6h/accepted_hits.sorted.bam   Load the sorted BAM file into IGV, as described previously, and rename\nthe track appropriately.", 
            "title": "Alignment Visualisation in IGV"
        }, 
        {
            "location": "/modules/btp-module-rna-seq/rna-seq/#generating-gene-counts", 
            "text": "In RNA-seq experiments the digital gene expression is recorded as the\ngene counts or number of reads aligning to a known gene feature. If you\nhave a well annotated genome, you can use the gene structure file in a\nstandard gene annotation format (GTF or GFF)) along with the spliced\nalignment file to quantify the known genes. We will demonstrate a\nutility called  FeatureCounts  that comes with the  Subread  package.  1\n2 mkdir gene_counts\nfeatureCounts -a annotation/Danio_rerio.Zv9.66.gtf -t exon -g gene_id -o gene_counts/gene_counts.txt tophat/ZV9_6h/accepted_hits.sorted.bam tophat/ZV9_2cells/accepted_hits.sorted.bam", 
            "title": "Generating Gene Counts"
        }, 
        {
            "location": "/modules/btp-module-rna-seq/rna-seq/#isoform-expression-and-transcriptome-assembly", 
            "text": "For non-model organisms and genomes with draft assemblies and incomplete\nannotations, it is a common practice to take an assembly based approach\nto generate gene structures followed by the quantification step. There\nare a number of reference based transcript assemblers available that can\nbe used for this purpose such as, cufflinks and stringy. These assemblers\ncan give gene or isoform level assemblies that can be used to perform a\ngene/isoform level quantification. These assemblers require an alignment\nof reads with a reference genome or transcriptome as an input. The\nsecond optional input is a known gene structure in  GTF  or  GFF \nformat.  There are a number of tools that perform reconstruction of the\ntranscriptome and for this workshop we are going to use Cufflinks.\nCufflinks can do transcriptome assembly either  ab initio  or using a\nreference annotation. It also quantifies the isoform expression in\nFragments Per Kilobase of exon per Million fragments mapped (FPKM).  Cufflinks has a number of parameters in order to perform transcriptome\nassembly and quantification. To view them all type:  1 cufflinks --help   We aim to reconstruct the transcriptome for both samples by using the\nEnsembl annotation both strictly and as a guide. In the first case\nCufflinks will only report isoforms that are included in the annotation,\nwhile in the latter case it will report novel isoforms as well.  The Ensembl annotation for  Danio rerio  is available in annotation/Danio_rerio.Zv9.66.gtf .  The general format of the  cufflinks  command is:  1 cufflinks [options]*  aligned_reads.(sam|bam)    Where the input is the aligned reads (either in SAM or BAM format).  Some of the available parameters for Cufflinks that we are going to use\nto run Cufflinks are listed below:  -o\n:   Output directory.  -G\n:   Tells Cufflinks to use the supplied GTF annotations strictly in\n    order to estimate isoform annotation.  -b\n:   Instructs Cufflinks to run a bias detection and correction algorithm\n    which can significantly improve accuracy of transcript abundance\n    estimates. To do this Cufflinks requires a multi-fasta file with the\n    genomic sequences against which we have aligned the reads.  -u\n:   Tells Cufflinks to do an initial estimation procedure to more\n    accurately weight reads mapping to multiple locations in the genome\n    (multi-hits).  \u2013library-type\n:   Before performing any type of RNA-seq analysis you need to know a\n    few things about the library preparation. Was it done using a\n    strand-specific protocol or not? If yes, which strand? In our data\n    the protocol was NOT strand specific.  Perform transcriptome assembly, strictly using the supplied GTF\nannotations, for the  2cells  and  6h  data using cufflinks:  1\n2\n3\n4 # 2cells data (takes approx. 5mins):\ncufflinks -o cufflinks/ZV9_2cells_gtf -G annotation/Danio_rerio.Zv9.66.gtf -b genome/Danio_rerio.Zv9.66.dna.fa -u --library-type fr-unstranded tophat/ZV9_2cells/accepted_hits.bam\n# 6h data (takes approx. 5mins):\ncufflinks -o cufflinks/ZV9_6h_gtf -G annotation/Danio_rerio.Zv9.66.gtf -b genome/Danio_rerio.Zv9.66.dna.fa -u --library-type fr-unstranded tophat/ZV9_6h/accepted_hits.bam   Cufflinks generates several files in the specified output directory.\nHere\u2019s a short description of these files:  genes.fpkm_tracking\n:   Contains the estimated gene-level expression values.  isoforms.fpkm_tracking\n:   Contains the estimated isoform-level expression values.  skipped.gtf\n:   Contains loci skipped as a result of exceeding the maximum number of\n    fragments.  transcripts.gtf\n:   This GTF file contains Cufflinks\u2019 assembled isoforms.  The complete documentation can be found at:  http://cole-trapnell-lab.github.io/cufflinks/file_formats/#output-formats-used-in-the-cufflinks-suite  So far we have forced cufflinks, by using the  -G  option, to strictly\nuse the GTF annotations provided and thus novel transcripts will not be\nreported. We can get cufflinks to perform a GTF-guided transcriptome\nassembly by using the  -g  option instead. Thus, novel transcripts will\nbe reported.  GTF-guided transcriptome assembly is more computationally intensive than\nstrictly using the GTF annotations. Therefore, we have pre-computed\nthese GTF-guided assemblies for you and have placed the results under\nsubdirectories:  cufflinks/ZV9_2cells_gtf_guided  and  cufflinks/ZV9_6h_gft_guided .  You DO NOT need to run these commands. We provide them so you know how\nwe generated the the GTF-guided transcriptome assemblies:  1\n2\n3\n4 # 2cells guided transcriptome assembly (takes approx. 30mins):\ncufflinks -o cufflinks/ZV9_2cells_gtf_guided -g annotation/Danio_rerio.Zv9.66.gtf -b genome/Danio_rerio.Zv9.66.dna.fa -u --library-type fr-unstranded tophat/ZV9_2cells/accepted_hits.bam\n# 6h guided transcriptome assembly (takes approx. 30mins):\ncufflinks -o cufflinks/ZV9_6h_gtf_guided -g annotation/Danio_rerio.Zv9.66.gtf -b genome/Danio_rerio.Zv9.66.dna.fa -u --library-type fr-unstranded tophat/ZV9_6h/accepted_hits.bam     Go back to IGV and load the pre-computed, GTF-guided transcriptome\n    assembly for the  2cells  data\n    ( cufflinks/ZV9_2cells_gtf_guided/transcripts.gtf ).    Rename the track as \u201c2cells GTF-Guided Transcripts\u201d.    In the search box type  ENSDART00000082297  in order for the browser\n    to zoom in to the gene of interest.     Question  Do you observe any difference between the Ensembl GTF annotations and the GTF-guided transcripts \nassembled by cufflinks (the \u201c2cells GTF-Guided Transcripts\u201d track)?   Answer Yes. It appears that the Ensembl annotations may have truncated the last exon. \nHowever, our data also doesn\u2019t contain reads that span between the last two exons.", 
            "title": "Isoform Expression and Transcriptome Assembly"
        }, 
        {
            "location": "/modules/btp-module-rna-seq/rna-seq/#differential-gene-expression-analysis-using-edger", 
            "text": "", 
            "title": "Differential Gene Expression Analysis using edgeR"
        }, 
        {
            "location": "/modules/btp-module-rna-seq/rna-seq/#experiment-design", 
            "text": "The example we are working through today follows a case Study set out in\nthe edgeR Users Guide (4.3 Androgen-treated prostate cancer cells\n(RNA-Seq, two groups)) which is based on an experiment conducted by Li et\nal. (2008, Proc Natl Acad Sci USA, 105, 20179-84).  The researchers used a prostate cancer cell line (LNCaP cells). These\ncells are sensitive to stimulation by male hormones (androgens). Three\nreplicate RNA samples were collected from LNCaP cells treated with an\nandrogen hormone (DHT). Four replicates were collected from cells\ntreated with an inactive compound. Each of the seven samples was run on\na lane (7 lanes) of an Illumina flow cell to produce 35 bp reads. The\nexperimental design was therefore 4 control samples vs 3 treated samples.  This workflow requires raw gene count files and these can be generated\nusing a utility called featureCounts as demonstrated above. We are using\na pre-computed gene counts data (stored in  pnas_expression.txt ) for\nthis exercise.", 
            "title": "Experiment Design"
        }, 
        {
            "location": "/modules/btp-module-rna-seq/rna-seq/#prepare-the-environment_1", 
            "text": "Prepare the environment and load R:  1\n2 cd /home/trainee/rnaseq/edgeR\nR (press enter)   Once on the R prompt. Load libraries:  1\n2\n3\n4\n5\n6 library(edgeR)\nlibrary(biomaRt)\nlibrary(gplots)\nlibrary(limma)\nlibrary(RColorBrewer)\nlibrary(org.Hs.eg.db)", 
            "title": "Prepare the Environment"
        }, 
        {
            "location": "/modules/btp-module-rna-seq/rna-seq/#read-in-data", 
            "text": "Read in count table and experimental design:  1\n2\n3\n4 data  -  read.delim ( pnas_expression.txt ,  row.names = 1 ,  header = T ) \ntargets  -  read.delim ( Targets.txt ,  header = T )  colnames ( data )   - targets $ Label head ( data ,  n = 20 )", 
            "title": "Read in Data"
        }, 
        {
            "location": "/modules/btp-module-rna-seq/rna-seq/#add-gene-annotation", 
            "text": "The data set only includes the Ensembl gene id and the counts. It is\nuseful to have other annotations such as the gene symbol and entrez id.\nNext we will add in these annotations. We will use the BiomaRt package\nto do this.  We start by using the useMart function of BiomaRt to access the human\ndata base of ensemble gene ids.  1\n2 human - useMart ( host = www.ensembl.org ,   ENSEMBL_MART_ENSEMBL ,  dataset = hsapiens_gene_ensembl )  \nattributes = c ( ensembl_gene_id ,   entrezgene , hgnc_symbol )    We create a vector of our ensemble gene ids.  1\n2 ensembl_names - rownames ( data )  head ( ensembl_names )    We then use the function getBM to get the gene symbol data we want.This\ntakes about a minute.  1 genemap - getBM ( attributes ,  filters = ensembl_gene_id ,  values = ensembl_names ,  mart = human )    Have a look at the start of the genemap dataframe.  1 head(genemap)   We then match the data we have retrieved to our dataset.  1\n2\n3\n4\n5\n6 idx  - match ( ensembl_names ,  genemap $ ensembl_gene_id ) \ndata $ entrezgene  - genemap $ entrezgene  [  idx  ] \ndata $ hgnc_symbol  - genemap $ hgnc_symbol  [  idx  ] \nAnn  -   cbind ( rownames ( data ),  data $ hgnc_symbol ,  data $ entrezgene )  colnames ( Ann ) - c ( Ensembl ,   Symbol ,   Entrez ) \nAnn - as.data.frame ( Ann )    Let\u2019s check and see that this additional information is there.  1 head(data)", 
            "title": "Add Gene annotation"
        }, 
        {
            "location": "/modules/btp-module-rna-seq/rna-seq/#data-checks", 
            "text": "Create DGEList object:  1\n2 treatment  - factor ( c ( rep ( Control , 4 ),   rep ( DHT , 3 )),  levels = c ( Control ,   DHT )) \ny  - DGEList ( counts = data [, 1 : 7 ],  group = treatment ,  genes = Ann )    Check the dimensions of the object:  1 dim(y)   We see we have 37435 rows (i.e. genes) and 7 columns (samples).  Now we will filter out genes with low counts by only keeping those rows\nwhere the count per million (cpm) is at least 1 in at least three\nsamples:  1\n2 keep  - rowSums (  cpm ( y ) 1 )   = 3 \ny  -  y [ keep ,   ]    Check how many rows (genes) are retained now.  1 dim(y)   This gives you 16494 rows indicating that 20941 (37435-16494) genes were filtered out.  As we have removed the lowly expressed genes the total number of counts\nper sample has not changed greatly. Let us check the total number of\nreads per sample in the original data (data) and now after filtering.  Before:  1 colSums(data[,1:7])   After filtering:  1 colSums(y$counts)   We will now perform normalization to take account of different library\nsize:  1 y - calcNormFactors ( y )    We will check the calculated normalization factors:  1 y$samples   Lets have a look at whether the samples cluster by condition. (You\nshould produce a plot as shown in Figure 4):  1 plotMDS(y, col=as.numeric(y$samples$group))   [H]   [fig:MDS plot]   Question  Does the MDS plot indicate a difference in gene expression between the Controls and the DHT treated samples?   Answer The MDS plot shows us that the controls are separated from the DHT treated cells. This indicates that there is a difference in gene expression between the conditions.  We will now estimate the dispersion. We start by estimating the common\ndispersion. The common dispersion estimates the overall Biological\nCoefficient of Variation (BCV) of the dataset averaged over all genes.  By using verbose we get the Disp and BCV values printed on the screen  1 y  -  estimateCommonDisp ( y ,  verbose = T )    What value to you see for BCV?  We now estimate gene-specific dispersion.  1 y  -  estimateTagwiseDisp ( y )    We will plot the tagwise dispersion and the common dispersion (You\nshould obtain a plot as shown in the Figure 5):  1 plotBCV(y)   [H]   [fig:BCV plot]  We see here that the common dispersion estimates the overall Biological\nCoefficient of Variation (BCV) of the dataset averaged over all genes.\nThe common dispersion is  0.02  and the BCV is the square root of the\ncommon dispersion (sqrt[0.02] = 0.14). A BCV of 14% is typical for cell\nline experiment.  As you can see from the plot the BCV of some genes (generally those with\nlow expression) can be much higher than the common dispersion. For\nexample we see genes with a reasonable level of expression with tagwise\ndispersion of 0.4 indicating 40% variation between samples.   Question  If we used the common dispersion for these genes instead of the tagwise dispersion what effect would this have?   Answer If we simply used the common dispersion for these genes we would underestimate biological variability, which in turn affects whether these genes would be identified as being differentially expressed between conditions.   It is recommended to use the tagwise dispersion,which takes account of gene-to-gene variability.  Now that we have normalized our data and also calculated the variability\nof gene expression between samples we are in a position to perform\ndifferential expression testing.As this is a simple comparison between\ntwo conditions, androgen treatment and placebo treatment we can use the\nexact test for the negative binomial distribution (Robinson and Smyth,\n2008).", 
            "title": "Data checks"
        }, 
        {
            "location": "/modules/btp-module-rna-seq/rna-seq/#testing-for-differential-expression", 
            "text": "We now test for differentially expressed BCV genes:  1 et  -  exactTest ( y )    Now we will use the topTags function to adjust for multiple testing. We\nwill use the Benjimini Hochberg ( BH ) method and we will produce a\ntable of results:  1 res  -  topTags ( et ,  n = nrow ( y $ counts ),  adjust.method = BH ) $ table    Let\u2019s have a look at the first rows of the table:  1 head(res)   To get a summary of the number of differentially expressed genes we can\nuse the decideTestsDGE function.  1 summary ( de  -  decideTestsDGE ( et ))    This tells us that 2086 genes are downregulated and 2345 genes are\nupregulated at 5% FDR.We will now make subsets of the most significant\nupregulated and downregulated genes using a log fold change threshhold of 1.5.  1\n2\n3\n4\n5\n6\n7 alpha = 0.05 \nlfc = 1.5 \nedgeR_res_sig - res [ res $ FDR alpha ,] \nedgeR_res_sig_lfc  - edgeR_res_sig [ abs ( edgeR_res_sig $ logFC )   =  lfc ,]  head ( edgeR_res_sig ,  n = 20 )  nrow ( edgeR_res_sig )  nrow ( edgeR_res_sig_lfc )    We can write out these results to our current directory.  1\n2 write.table(edgeR_res_sig ,  edgeR_res_sig.txt , sep= \\t , col.names=NA, quote=F)\nwrite.table(edgeR_res_sig_lfc ,  edgeR_res_sig_lfc.txt , sep= \\t , col.names=NA, quote=F)    Question  How many differentially expressed genes are are found by edgeR before and after requiring the log fold change of 1.5?   Answer There are 4431 DEGs at an FDR = 0.05 which reduces to 1148 if we require a log fold change of 1.5.", 
            "title": "Testing for Differential Expression"
        }, 
        {
            "location": "/modules/btp-module-rna-seq/rna-seq/#differential-expression-using-the-voom-function-and-the-limma-package", 
            "text": "We will now show an alternative approach to differential expression\nwhich uses the  limma  package.This is based on linear models. The first\nstep is to create a design matrix. In this case we have a simple design\nwhere we have only one condition (treated vs non-treated). However, you\nmay be dealing with more complex experimental designs, for example\nlooking at treatment and other covariates, such as age, gender, batch.  1\n2\n3 design  - model.matrix ( ~ treatment ) \ncheck design print ( design )    We now use voom to transform the data into a form which is appropriate\nfor linear modelling.  1 v  - voom ( y ,  design )    Next we will fit linear model to each gene in the dataset using the\nfunction lmFit. Following this we use the function eBayes to test each\ngene to find whether foldchange between the conditions being tested is\nstatistically significant.We filter our results by using the same values\nof alpha (0.05) and log fold change (1.5) used previously.  1\n2\n3\n4\n5\n6\n7\n8\n9 fit_v  - lmFit ( v ,  design ) \n\nfit_v  -  eBayes ( fit_v ) \n\nvoom_res - topTable ( fit_v ,  coef = 2 , adjust.method = BH ,  sort.by = P ,  n = nrow ( y $ counts )) \n\nvoom_res_sig  - voom_res [ voom_res $ adj.P.Val  alpha ,] \n\nvoom_res_sig_lfc  - voom_res_sig [ abs ( voom_res_sig $ logFC )   =  lfc ,]    How many differentially expressed genes are identified?  1\n2 nrow(voom_res_sig)\nnrow(voom_res_sig_lfc)    Question  How many differentially expressed genes are found using voom before and after requiring the log fold change of 1.5?   Answer There are 4103 DEGs at an FDR = 0.05 which reduces to 1109 if we require a log fold change of 1.5.  We will write out these results.  1\n2\n3 write.table(voom_res_sig,  voom_res_sig.txt , sep= \\t , col.names=NA, quote=F)\nwrite.table(voom_res_sig_lfc,  voom_res_sig_lfc.txt , sep= \\t , col.names=NA, quote=F)\nwrite.table(voom_res,  voom_res.txt , sep= \\t , col.names=NA, quote=F)", 
            "title": "Differential expression using the Voom function and the limma package"
        }, 
        {
            "location": "/modules/btp-module-rna-seq/rna-seq/#data-visualisation", 
            "text": "Now let\u2019s visualize some of this data. First we will make a volcano plot\nusing the  volcanoplot  function available in  limma . This creates a\nplot which displays fold changes versus a measure of statistical\nsignificance of the change.  1 volcanoplot(fit_v, coef=2, highlight=5)   [H]   [fig:Volcano plot]  Next we will create a heatmap of the top differentially expressed genes.\nWe use the heatmap.2 function available in the gplots package.  1\n2\n3 select_top   -  p.adjust ( fit_v $ p.value [,   2 ])   1e-2 \nExp_top  -  v $ E  [ select_top ,   ] \nheatmap.2 ( Exp_top ,  scale = row ,  density.info = none ,  trace = none ,  main = Top DEGs ,  labRow = ,  cexRow = 0.4 ,  cexCol = 0.8 )    [H]   [fig:Heatmap]  You can now quit the R prompt  1 q()   You can save your workspace by typing  Y  on prompt.  Please note that the output files you are creating are saved in your\npresent working directory. If you are not sure where you are in the file\nsystem try typing  pwd  on your command prompt to find out.", 
            "title": "Data Visualisation"
        }, 
        {
            "location": "/modules/btp-module-rna-seq/rna-seq/#differential-expression-using-cuffdiff", 
            "text": "This is optional exercise and will be run if time permits.  NOTE: If this exercise is to be attempted it is necessary to leave the directory we have been working in and go back to the zebra fish data. Commands should be executed from the rnaseq directory.  One of the stand-alone tools that perform differential expression\nanalysis is Cuffdiff. We use this tool to compare between two\nconditions; for example different conditions could be control and\ndisease, or wild-type and mutant, or various developmental stages.  In our case we want to identify genes that are differentially expressed\nbetween two developmental stages; a  2cells  embryo and  6h  post\nfertilization.  The general format of the cuffdiff command is:  1 cuffdiff [options]*  transcripts.gtf   sample1_replicate1.sam[,...,sample1_replicateM]   sample2_replicate1.sam[,...,sample2_replicateM.sam]    Where the input includes a  transcripts.gtf  file, which is an\nannotation file of the genome of interest or the cufflinks assembled\ntranscripts, and the aligned reads (either in SAM or BAM format) for the\nconditions. Some of the Cufflinks options that we will use to run the\nprogram are:  -o\n:   Output directory.  -L\n:   Labels for the different conditions  -T\n:   Tells Cuffdiff that the reads are from a time series experiment.  -b\n:   Instructs Cufflinks to run a bias detection and correction algorithm\n    which can significantly improve accuracy of transcript abundance\n    estimates. To do this Cufflinks requires a multi-fasta file with the\n    genomic sequences against which we have aligned the reads.  -u\n:   Tells Cufflinks to do an initial estimation procedure to more\n    accurately weight reads mapping to multiple locations in the genome\n    (multi-hits).  \u2013library-type\n:   Before performing any type of RNA-seq analysis you need to know a\n    few things about the library preparation. Was it done using a\n    strand-specific protocol or not? If yes, which strand? In our data\n    the protocol was NOT strand specific.  -C\n:   Biological replicates and multiple group contrast can be defined\n    here  Run cuffdiff on the tophat generated BAM files for the 2cells vs. 6h\ndata sets:  1 cuffdiff -o cuffdiff/ -L ZV9_2cells,ZV9_6h -T -b genome/Danio_rerio.Zv9.66.dna.fa -u --library-type fr-unstranded annotation/Danio_rerio.Zv9.66.gtf tophat/ZV9_2cells/accepted_hits.bam tophat/ZV9_6h/accepted_hits.bam   We are interested in the differential expression at the gene level. The\nresults are reported by Cuffdiff in the file  cuffdiff/gene_exp.diff .\nLook at the first few lines of the file using the following command:  1 head -n 20 cuffdiff/gene_exp.diff   We would like to see which are the most significantly differentially\nexpressed genes. Therefore we will sort the above file according to the\nq value (corrected p value for multiple testing). The result will be\nstored in a different file called  gene_exp_qval.sorted.diff .  1 sort -t$ \\t  -g -k 13 cuffdiff/gene_exp.diff   cuffdiff/gene_exp_qval.sorted.diff   Look again at the top 20 lines of the sorted file by typing:  1 head -n 20 cuffdiff/gene_exp_qval.sorted.diff   Copy an Ensembl transcript identifier from the first two columns for one\nof these genes (e.g.  ENSDARG00000045067 ). Now go back to the IGV\nbrowser and paste it in the search box.  What are the various outputs generated by cuffdiff? Hint: Please refer\nto the  Cuffdiff output  section of the cufflinks manual online.  Do you see any difference in the read coverage between the  2cells  and 6h  conditions that might have given rise to this transcript being\ncalled as differentially expressed?  The coverage on the Ensembl browser is based on raw reads and no\nnormalisation has taken place contrary to the FPKM values.  The read coverage of this transcript ( ENSDARG00000045067 ) in the 6h\ndata set is much higher than in the 2cells data set.", 
            "title": "Differential Expression using cuffdiff"
        }, 
        {
            "location": "/modules/btp-module-rna-seq/rna-seq/#visualising-the-cuffdiff-expression-analysis", 
            "text": "We will use an R-Bioconductor package called  cummeRbund  to visualise,\nmanipulate and explore Cufflinks RNA-seq output. We will load an R\nenvironment and look at few quick tips to generate simple graphical\noutput of the cufflinks analysis we have just run.  CummeRbund  takes the cuffdiff output and populates a SQLite database\nwith various type of output generated by cuffdiff e.g, genes,\ntranscripts, transcription start site, isoforms and CDS regions. The\ndata from this database can be accessed and processed easily. This\npackage comes with a number of in-built plotting functions that are\ncommonly used for visualising the expression data. We strongly recommend\nreading through the bioconductor manual and user guide of CummeRbund to\nlearn about functionality of the tool. The reference is provided in the\nresource section.  Prepare the environment. Go to the  cuffdiff  output folder and copy the\ntranscripts file there.  1\n2\n3 cd /home/trainee/rnaseq/cuffdiff\ncp /home/trainee/rnaseq/annotation/Danio_rerio.Zv9.66.gtf /home/trainee/rnaseq/cuffdiff\nls -l   Load the R environment  1 R (press enter)   Load the require R package.  1 library(cummeRbund)   Read in the cuffdiff output  1 cuff - readCufflinks ( dir = /home/trainee/rnaseq/cuffdiff\u201d, gtfFile= Danio_rerio.Zv9.66.gtf ,genome= Zv9 , rebuild=T)    Assess the distribution of FPKM scores across samples  1\n2\n3\n4 pdf ( file  =   SCV.pdf ,  height  =   6 ,  width  =   6 ) \ndens - csDensity ( genes ( cuff )) \ndens\ndev.off ()    Box plots of the FPKM values for each samples  1\n2\n3\n4 pdf ( file  =   BoxP.pdf ,  height  =   6 ,  width  =   6 ) \nb - csBoxplot ( genes ( cuff )) \nb\ndev.off ()    Accessing the data  1\n2\n3\n4\n5\n6 sigGeneIds - getSig ( cuff , alpha = 0.05 , level = genes )  head ( sigGeneIds ) \nsigGenes - getGenes ( cuff , sigGeneIds ) \nsigGenes head ( fpkm ( sigGenes ))  head ( fpkm ( isoforms ( sigGenes )))    Plotting a heatmap of the differentially expressed genes  1\n2\n3\n4 pdf ( file  =   heatmap.pdf ,  height  =   6 ,  width  =   6 ) \nh - csHeatmap ( sigGenes , cluster = both ) \nh\ndev.off ()    What options would you use to draw a density or boxplot for different\nreplicates if available ? (Hint: look at the manual at Bioconductor\nwebsite)  1\n2 densRep - csDensity ( genes ( cuff ), replicates = T ) \nbrep - csBoxplot ( genes ( cuff ), replicates = T )    How many differentially expressed genes did you observe?  type \u2019summary(sigGenes)\u2019 on the R prompt to see.", 
            "title": "Visualising the CuffDiff expression analysis"
        }, 
        {
            "location": "/modules/btp-module-rna-seq/rna-seq/#references", 
            "text": "Trapnell, C., Pachter, L.   Salzberg, S. L. TopHat: discovering\n    splice junctions with RNA-Seq. Bioinformatics 25, 1105-1111 (2009).    Trapnell, C. et al. Transcript assembly and quantification by\n    RNA-Seq reveals unannotated transcripts and isoform switching during\n    cell differentiation. Nat. Biotechnol. 28, 511-515 (2010).    Langmead, B., Trapnell, C., Pop, M.   Salzberg, S. L. Ultrafast and\n    memory-efficient alignment of short DNA sequences to the human\n    genome. Genome Biol. 10, R25 (2009).    Roberts, A., Pimentel, H., Trapnell, C.   Pachter, L. Identification\n    of novel transcripts in annotated genomes using RNA-Seq.\n    Bioinformatics 27, 2325-2329 (2011).    Roberts, A., Trapnell, C., Donaghey, J., Rinn, J. L.   Pachter, L.\n    Improving RNA-Seq expression estimates by correcting for fragment\n    bias. Genome Biol. 12, R22 (2011).    Robinson MD, McCarthy DJ and Smyth GK. edgeR: a Bioconductor package\n    for differential expression analysis of digital gene expression\n    data. Bioinformatics, 26 (2010).    Robinson MD and Smyth GK Moderated statistical tests for assessing\n    differences in tag abundance. Bioinformatics, 23, pp. -6.    Robinson MD and Smyth GK (2008). Small-sample estimation of negative\n    binomial dispersion, with applications to SAGE data.\u201d Biostatistics,\n    9.    McCarthy, J. D, Chen, Yunshun, Smyth and K. G (2012). Differential\n    expression analysis of multifactor RNA-Seq experiments with respect\n    to biological variation. Nucleic Acids Research, 40(10), pp. -9.", 
            "title": "References"
        }, 
        {
            "location": "/modules/btp-module-ngs-bio/biol/", 
            "text": "Key Learning Outcomes\n\n\nAfter completing this module the trainee should be able to:\n\n\n\n\n\n\nFind gene ontology enrichment in a list of differentially expressed\n    genes using R-based packages.\n\n\n\n\n\n\nRunning GO enrichment analysis using the web tool DAVID and the web\n    tool Gorilla\n\n\n\n\n\n\nTo run webtools such as REVIGO and STRING\n\n\n\n\n\n\nResources You\u2019ll be Using\n\n\nTools Used\n\n\nGoana from Limma:\n\n\nhttps://bioconductor.org/packages/release/bioc/html/limma.html\n\n\nDAVID:\n\n\nhttp://david.abcc.ncifcrf.gov\n\n\nGOrilla:\n\n\nhttp://cbl-gorilla.cs.technion.ac.il\n\n\nREVIGO:\n\n\nhttp://revigo.irb.hr\n\n\nSTRING:\n\n\nhttp://string-db.org\n\n\nAuthor Information\n\n\nPrimary Author(s):\n\n    Susan M Corley \n\n\nContributor(s):\n\n    Sonika Tyagi \n \n\n    Nandan Deshpande \n\n\nIntroduction\n\n\nThe goal of this hands-on session is to allow you to develop some\nfamiliarity with commonly used, freely available R based packages and\nweb tools which can be used to gain biological insight from a\ndifferential expression experiment. We will use the differentially\nexpressed genes (DEGs) identified in the last session. First, we will\nlook at whether these genes are enriched for gene ontology terms which\ngives us some insight as to whether the DEGs are involved in particular\nfunctions. Then we will use a tool that constructs an interaction\nnetwork from these genes. This will allow us to identify clusters of\nDEGs that are known to interact.\n\n\nIn using any database tools it is always advisable to check whether they\nare regularly updated. We suggest that you experiment with more than one\ntool.\n\n\nGene ontology analysis with GOana\n\n\nFirst we will go back to the R environment and use the function GOana\nassociated with the limma package. To use this function we need to have\nour DEGs annotated with the entrez gene identifier for each gene.We did\nthis early on in our data processing. We use the fit object (fit_v) generated\nusing the voom function in limma for this analysis. To obtain more\ninformation regarding the goana function type ?goana within your R\nsession.\n\n\nOpen the Terminal and go to the \nrnaseq/edgeR\n working directory:\n\n\n1\ncd /home/trainee/rnaseq/edgeR\n\n\n\n\n\n\n\n\nSTOP\n\n\nAll commands entered into the terminal for this tutorial should be from\nwithin the \n/home/trainee/rnaseq/edgeR\n directory.\n\n\n\n\nR (press enter)\n\n\nCheck that the directory you are in contains the above-mentioned fit_v file by\ntyping:\n\n\n1\n2\n3\n4\n5\nls()\nlibrary(limma)\nlibrary(RColorBrewer)\nlibrary(gplots)\nlibrary(org.Hs.eg.db)\n\n\n\n\n\n\nWe will use the goana function to obtain the gene ontology terms associated with the DEGs.\n\n\n1\nDE_GOana\n-\ngoana\n(\nfit_v\n,\n coef\n=\n2\n,\n geneid\n=\nfit_v\n$\ngenes\n$\nEntrez\n,\n FDR\n=\n0.05\n,\n species \n=\n \nHs\n,\n trend\n=\nF\n,\n plot\n=\nF\n \n)\n\n\n\n\n\n\n\nNow we will look at the most significant biological process (BP)ontology\nterms\n\n\n1\n2\n3\n4\n5\n6\nDE_GOana_top_BP\n-\n topGO\n(\nDE_GOana\n,\n ontology\n=\nc\n(\nBP\n),\n number\n=\n150L\n,\n truncate.term\n=\n50\n)\n\n\nhead\n(\nDE_GOana_top_BP\n,\n \n20\n)\n      \nDE_GOana_top_BP_down\n-\n topGO\n(\nDE_GOana\n,\n ontology\n=\nc\n(\nBP\n),\n sort \n=\n \ndown\n,\n number\n=\n150L\n,\n truncate.term\n=\n50\n)\n\n\nhead\n(\nDE_GOana_top_BP_down\n,\n \n10\n)\n\nDE_GOana_top_BP_up\n-\n topGO\n(\nDE_GOana\n,\n ontology\n=\nc\n(\nBP\n),\n sort \n=\n \nup\n,\n number\n=\n150L\n,\n truncate.term\n=\n50\n)\n\n\nhead\n(\nDE_GOana_top_BP_up\n,\n \n10\n)\n\n\n\n\n\n\n\nRather than looking at biological process (BP) let\u2019s now look at\nmolecular function (MF) terms\n\n\n1\n2\n3\n4\nDE_GOana_top_MF_down\n-\n topGO\n(\nDE_GOana\n,\n ontology\n=\nc\n(\nMF\n),\n sort \n=\n \ndown\n,\n number\n=\n150L\n,\n truncate.term\n=\n50\n)\n\n\nhead\n(\nDE_GOana_top_MF_down\n,\n \n10\n)\n\nDE_GOana_top_MF_up\n-\n topGO\n(\nDE_GOana\n,\n ontology\n=\nc\n(\nMF\n),\n sort \n=\n \nup\n,\n number\n=\n150L\n,\n truncate.term\n=\n50\n)\n\n\nhead\n(\nDE_GOana_top_MF_up\n,\n \n10\n)\n\n\n\n\n\n\n\nQuestions and Answers\n\n\nBased on the above section:\n\n\n\n\nQuestion\n\n\nWhat is the general theme emerging when we look at biological process in the down-regulated genes and the up-regulated genes?\n\n\n\n\nAnswer\nWe see terms involving the cell cycle and cell division are enriched in the down-regulated genes and terms involving ER stress and protein folding are enriched in the up-regulated genes. \n\n\n\n\nQuestion\n\n\nLooking at the biological process (BP) term \u201ccell division: GO:0051301\u201d, how many down-regulated and up-regulated DEGs are annotated with this term, and what statistical significance is associated with this enrichment?\n\n\n\n\nAnswer\nLook for the row for GO:0051301 in the top 20 BP ontology terms. \n\n\nThere are a number of tools and packages available with the\nR-bioconductor repositories that you can use with your R code to run\nontologies and pathway analysis. \n\n\nGene ontology analysis with DAVID\n\n\nClick on your Firefox web browser. Go to the DAVID website:\n\nhttp://david.abcc.ncifcrf.gov\n. Go to your edgeR folder and open the\nfile \nvoom_res_sig_lfc.txt\n using LibreOffice Calc. For the separator\noptions in LibreOffice Calc choose \nSeparated by tab\n. Once you have\nopened this file copy the \nEnsembl Gene Ids\n (Column A). This list can\nthen be pasted into DAVID.  \n\n\nScreenshots of the DAVID website and the steps to move through the\nwebsite are provided in the presentation prepared for this session. Use\nthat material to work through this exercise.  \n\n\nWe will use the Functional Annotation Clustering tool in DAVID. First we\nwill uncheck all the defaults and look only at the GO terms involving\nbiological process. After unchecking all the defaults, expand Gene\nOntology and select GOTERM_BP_5. Then select the button Functional\nAnnotation Clustering.  \n\n\nThis will bring up a screen where GO terms are clustered. Statistical\ntesting is performed to assess whether the GO terms are more enriched in\nthe list of DEGs than would be expected by chance. You will see a column\nof P_Value and also adjusted P values. Have a look at the brief\ndescription of the statistical test used in DAVID\n(\nhttps://david.ncifcrf.gov/helps/functional_annotation.html\n).\n\n\nQuestions and Answers\n\n\nBased on the above section:\n\n\n\n\nQuestion\n\n\nWhat functional themes emerge in Cluster 1 and Cluster 2?\n\n\n\n\nAnswer\nCluster 1 is enriched for cell singling and Cluster 2 is enriched for cell cycle. Note that answers may vary based on the genes entered and the options selected. \n\n\n\n\nQuestion\n\n\nHow many of the DEGs are annotated with the term \u201cintracellular signal transduction\u201d and name five genes?\n\n\n\n\nAnswer\nLooking at the first row of the DAVID results we see that 181 DEGs are annotated with \u201cIntracellular signal transduction\u201d. The first 5 genes listed are DHCR24, HMGCR, ADAP12, ARL5A, ARFGEF3.\n\n\n\n\nQuestion\n\n\nDoes this seem to be sensible in an experiment that looks at the response of cancer cells to a stimulant?\n\n\n\n\nGene ontology analysis with GOrilla\n\n\nClick on your Firefox web browser. Go to the GOrilla website:\n\nhttp://cbl-gorilla.cs.technion.ac.il\n  \n\n\nFor this tool we will use a background list of genes. Open the file\n\nvoom_res.txt\n using LibreOffice Calc. Copy the \nEnsembl Gene Ids\n\n(Column A) and paste this into GOrilla as the background set.\n\n\nAs in the previous exercise we will use the DEGs found by voom\n(\nvoom_res_sig_lfc.txt\n) as the Target set.  \n\n\nWe will firstly look for enriched GO process terms. Screenshots of the\nwebsite showing the steps you need to follow are in the presentation for\nthis session.  \n\n\nGOrilla will display the GO term hierarchy. This shows you which terms\nare parent and child terms and how the terms are related.Under this you\nwill find a table of the most significantly enriched GO terms. Have a\nlook at the DEGs associated with the most enriched clusters.\n\n\nQuestions and Answers\n\n\nBased on the above section:\n\n\n\n\nQuestion\n\n\nFind the GO term \nregulation of cell proliferation\n how far can you trace this back to the parent terms. \n\n\n\n\nAnswer\nregulation of cell proliferation \u2013 regulation of cellular process \u2013 regulation of biological process \u2013 biological regulation \u2013 biological process\n\n\n\n\nQuestion\n\n\nWhat are the direct child terms of \nregulation of cell proliferation\n?. \n\n\n\n\nAnswer\nregulation of stem cell proliferation, regulation of sooth muscle cell proliferation, positive regulation of cell proliferation, regulation of mesenchymal cell proliferation\n\n\n\n\nQuestion\n\n\nWhat is the enrichment score for \nregulation of cell proliferation\n ? How is this calculated (Hint: scroll down the page for the heading Enrichment). \n\n\n\n\nAnswer\n1.58 ((104/870)/(919/12137))\n\n\nREVIGO to reduce redundancy and visualise\n\n\nWe can use the results generated by the GOrilla web tool as input to\nREVIGO which will summarise the GO data and allow us to visualize the\nsimplified data. Click on the link Visualize output in REViGO. Follow the\nscreen shots in the presentation.Go to the treemap view.\n\n\n\n\nQuestion\n\n\nWhat are the main functional categories emerging in this analysis? \n\n\n\n\nSTRING\n\n\nUsing STRING to look at networks that may be formed by the DEGsClick on\nyour Firefox web browser. Go to the STRING website:\n\nhttp://string-db.org\n. For this exercise we will only use the top 500\nDEGs. Go to the file (\nvoom_res_sig_lfc.txt\n) copy only the top 500.\nThese will be pasted as input to the STRING website. Follow the screen\nshots in the presentation.  \n\n\nYou will see a large interaction network being built from the 500\nDEGs.We will refine this by clicking on the Data settings tab and\nselecting \nhigh confidence\n (see the screen shot in the presentation).\nLook at the gene clusters that are generated.\n\n\nFind the gene CDK1. Look at the cluster generated around this gene. What\nother DEGs are interaction partners of CDK1.  \n\n\nClick on CDK1 and find what functions it is involved in. Click on the\ninteraction partners of CDK1 and find their functions. \n\n\n\n\nQuestion\n\n\nDoes this help in further explaining some of the gene ontology results? \n\n\n\n\nExplore other clusters that are formed in this analysis.", 
            "title": "NGS Biological Insight"
        }, 
        {
            "location": "/modules/btp-module-ngs-bio/biol/#key-learning-outcomes", 
            "text": "After completing this module the trainee should be able to:    Find gene ontology enrichment in a list of differentially expressed\n    genes using R-based packages.    Running GO enrichment analysis using the web tool DAVID and the web\n    tool Gorilla    To run webtools such as REVIGO and STRING", 
            "title": "Key Learning Outcomes"
        }, 
        {
            "location": "/modules/btp-module-ngs-bio/biol/#resources-youll-be-using", 
            "text": "", 
            "title": "Resources You\u2019ll be Using"
        }, 
        {
            "location": "/modules/btp-module-ngs-bio/biol/#tools-used", 
            "text": "Goana from Limma:  https://bioconductor.org/packages/release/bioc/html/limma.html  DAVID:  http://david.abcc.ncifcrf.gov  GOrilla:  http://cbl-gorilla.cs.technion.ac.il  REVIGO:  http://revigo.irb.hr  STRING:  http://string-db.org", 
            "title": "Tools Used"
        }, 
        {
            "location": "/modules/btp-module-ngs-bio/biol/#author-information", 
            "text": "Primary Author(s): \n    Susan M Corley   Contributor(s): \n    Sonika Tyagi    \n    Nandan Deshpande", 
            "title": "Author Information"
        }, 
        {
            "location": "/modules/btp-module-ngs-bio/biol/#introduction", 
            "text": "The goal of this hands-on session is to allow you to develop some\nfamiliarity with commonly used, freely available R based packages and\nweb tools which can be used to gain biological insight from a\ndifferential expression experiment. We will use the differentially\nexpressed genes (DEGs) identified in the last session. First, we will\nlook at whether these genes are enriched for gene ontology terms which\ngives us some insight as to whether the DEGs are involved in particular\nfunctions. Then we will use a tool that constructs an interaction\nnetwork from these genes. This will allow us to identify clusters of\nDEGs that are known to interact.  In using any database tools it is always advisable to check whether they\nare regularly updated. We suggest that you experiment with more than one\ntool.", 
            "title": "Introduction"
        }, 
        {
            "location": "/modules/btp-module-ngs-bio/biol/#gene-ontology-analysis-with-goana", 
            "text": "First we will go back to the R environment and use the function GOana\nassociated with the limma package. To use this function we need to have\nour DEGs annotated with the entrez gene identifier for each gene.We did\nthis early on in our data processing. We use the fit object (fit_v) generated\nusing the voom function in limma for this analysis. To obtain more\ninformation regarding the goana function type ?goana within your R\nsession.  Open the Terminal and go to the  rnaseq/edgeR  working directory:  1 cd /home/trainee/rnaseq/edgeR    STOP  All commands entered into the terminal for this tutorial should be from\nwithin the  /home/trainee/rnaseq/edgeR  directory.   R (press enter)  Check that the directory you are in contains the above-mentioned fit_v file by\ntyping:  1\n2\n3\n4\n5 ls()\nlibrary(limma)\nlibrary(RColorBrewer)\nlibrary(gplots)\nlibrary(org.Hs.eg.db)   We will use the goana function to obtain the gene ontology terms associated with the DEGs.  1 DE_GOana - goana ( fit_v ,  coef = 2 ,  geneid = fit_v $ genes $ Entrez ,  FDR = 0.05 ,  species  =   Hs ,  trend = F ,  plot = F   )    Now we will look at the most significant biological process (BP)ontology\nterms  1\n2\n3\n4\n5\n6 DE_GOana_top_BP -  topGO ( DE_GOana ,  ontology = c ( BP ),  number = 150L ,  truncate.term = 50 )  head ( DE_GOana_top_BP ,   20 )       \nDE_GOana_top_BP_down -  topGO ( DE_GOana ,  ontology = c ( BP ),  sort  =   down ,  number = 150L ,  truncate.term = 50 )  head ( DE_GOana_top_BP_down ,   10 ) \nDE_GOana_top_BP_up -  topGO ( DE_GOana ,  ontology = c ( BP ),  sort  =   up ,  number = 150L ,  truncate.term = 50 )  head ( DE_GOana_top_BP_up ,   10 )    Rather than looking at biological process (BP) let\u2019s now look at\nmolecular function (MF) terms  1\n2\n3\n4 DE_GOana_top_MF_down -  topGO ( DE_GOana ,  ontology = c ( MF ),  sort  =   down ,  number = 150L ,  truncate.term = 50 )  head ( DE_GOana_top_MF_down ,   10 ) \nDE_GOana_top_MF_up -  topGO ( DE_GOana ,  ontology = c ( MF ),  sort  =   up ,  number = 150L ,  truncate.term = 50 )  head ( DE_GOana_top_MF_up ,   10 )", 
            "title": "Gene ontology analysis with GOana"
        }, 
        {
            "location": "/modules/btp-module-ngs-bio/biol/#questions-and-answers", 
            "text": "Based on the above section:   Question  What is the general theme emerging when we look at biological process in the down-regulated genes and the up-regulated genes?   Answer We see terms involving the cell cycle and cell division are enriched in the down-regulated genes and terms involving ER stress and protein folding are enriched in the up-regulated genes.    Question  Looking at the biological process (BP) term \u201ccell division: GO:0051301\u201d, how many down-regulated and up-regulated DEGs are annotated with this term, and what statistical significance is associated with this enrichment?   Answer Look for the row for GO:0051301 in the top 20 BP ontology terms.   There are a number of tools and packages available with the\nR-bioconductor repositories that you can use with your R code to run\nontologies and pathway analysis.", 
            "title": "Questions and Answers"
        }, 
        {
            "location": "/modules/btp-module-ngs-bio/biol/#gene-ontology-analysis-with-david", 
            "text": "Click on your Firefox web browser. Go to the DAVID website: http://david.abcc.ncifcrf.gov . Go to your edgeR folder and open the\nfile  voom_res_sig_lfc.txt  using LibreOffice Calc. For the separator\noptions in LibreOffice Calc choose  Separated by tab . Once you have\nopened this file copy the  Ensembl Gene Ids  (Column A). This list can\nthen be pasted into DAVID.    Screenshots of the DAVID website and the steps to move through the\nwebsite are provided in the presentation prepared for this session. Use\nthat material to work through this exercise.    We will use the Functional Annotation Clustering tool in DAVID. First we\nwill uncheck all the defaults and look only at the GO terms involving\nbiological process. After unchecking all the defaults, expand Gene\nOntology and select GOTERM_BP_5. Then select the button Functional\nAnnotation Clustering.    This will bring up a screen where GO terms are clustered. Statistical\ntesting is performed to assess whether the GO terms are more enriched in\nthe list of DEGs than would be expected by chance. You will see a column\nof P_Value and also adjusted P values. Have a look at the brief\ndescription of the statistical test used in DAVID\n( https://david.ncifcrf.gov/helps/functional_annotation.html ).", 
            "title": "Gene ontology analysis with DAVID"
        }, 
        {
            "location": "/modules/btp-module-ngs-bio/biol/#questions-and-answers_1", 
            "text": "Based on the above section:   Question  What functional themes emerge in Cluster 1 and Cluster 2?   Answer Cluster 1 is enriched for cell singling and Cluster 2 is enriched for cell cycle. Note that answers may vary based on the genes entered and the options selected.    Question  How many of the DEGs are annotated with the term \u201cintracellular signal transduction\u201d and name five genes?   Answer Looking at the first row of the DAVID results we see that 181 DEGs are annotated with \u201cIntracellular signal transduction\u201d. The first 5 genes listed are DHCR24, HMGCR, ADAP12, ARL5A, ARFGEF3.   Question  Does this seem to be sensible in an experiment that looks at the response of cancer cells to a stimulant?", 
            "title": "Questions and Answers"
        }, 
        {
            "location": "/modules/btp-module-ngs-bio/biol/#gene-ontology-analysis-with-gorilla", 
            "text": "Click on your Firefox web browser. Go to the GOrilla website: http://cbl-gorilla.cs.technion.ac.il     For this tool we will use a background list of genes. Open the file voom_res.txt  using LibreOffice Calc. Copy the  Ensembl Gene Ids \n(Column A) and paste this into GOrilla as the background set.  As in the previous exercise we will use the DEGs found by voom\n( voom_res_sig_lfc.txt ) as the Target set.    We will firstly look for enriched GO process terms. Screenshots of the\nwebsite showing the steps you need to follow are in the presentation for\nthis session.    GOrilla will display the GO term hierarchy. This shows you which terms\nare parent and child terms and how the terms are related.Under this you\nwill find a table of the most significantly enriched GO terms. Have a\nlook at the DEGs associated with the most enriched clusters.", 
            "title": "Gene ontology analysis with GOrilla"
        }, 
        {
            "location": "/modules/btp-module-ngs-bio/biol/#questions-and-answers_2", 
            "text": "Based on the above section:   Question  Find the GO term  regulation of cell proliferation  how far can you trace this back to the parent terms.    Answer regulation of cell proliferation \u2013 regulation of cellular process \u2013 regulation of biological process \u2013 biological regulation \u2013 biological process   Question  What are the direct child terms of  regulation of cell proliferation ?.    Answer regulation of stem cell proliferation, regulation of sooth muscle cell proliferation, positive regulation of cell proliferation, regulation of mesenchymal cell proliferation   Question  What is the enrichment score for  regulation of cell proliferation  ? How is this calculated (Hint: scroll down the page for the heading Enrichment).    Answer 1.58 ((104/870)/(919/12137))", 
            "title": "Questions and Answers"
        }, 
        {
            "location": "/modules/btp-module-ngs-bio/biol/#revigo-to-reduce-redundancy-and-visualise", 
            "text": "We can use the results generated by the GOrilla web tool as input to\nREVIGO which will summarise the GO data and allow us to visualize the\nsimplified data. Click on the link Visualize output in REViGO. Follow the\nscreen shots in the presentation.Go to the treemap view.   Question  What are the main functional categories emerging in this analysis?", 
            "title": "REVIGO to reduce redundancy and visualise"
        }, 
        {
            "location": "/modules/btp-module-ngs-bio/biol/#string", 
            "text": "Using STRING to look at networks that may be formed by the DEGsClick on\nyour Firefox web browser. Go to the STRING website: http://string-db.org . For this exercise we will only use the top 500\nDEGs. Go to the file ( voom_res_sig_lfc.txt ) copy only the top 500.\nThese will be pasted as input to the STRING website. Follow the screen\nshots in the presentation.    You will see a large interaction network being built from the 500\nDEGs.We will refine this by clicking on the Data settings tab and\nselecting  high confidence  (see the screen shot in the presentation).\nLook at the gene clusters that are generated.  Find the gene CDK1. Look at the cluster generated around this gene. What\nother DEGs are interaction partners of CDK1.    Click on CDK1 and find what functions it is involved in. Click on the\ninteraction partners of CDK1 and find their functions.    Question  Does this help in further explaining some of the gene ontology results?    Explore other clusters that are formed in this analysis.", 
            "title": "STRING"
        }, 
        {
            "location": "/modules/btp-module-velvet/velvet/", 
            "text": "Key Learning Outcomes\n\n\nAfter completing this practical the trainee should be able to:\n\n\n\n\n\n\nCompile velvet with appropriate compile-time parameters set for a\n    specific analysis\n\n\n\n\n\n\nBe able to choose appropriate assembly parameters\n\n\n\n\n\n\nAssemble a set of paired-end reads from a single insert-size library\n\n\n\n\n\n\nBe able to visualise an assembly in AMOS Hawkeye\n\n\n\n\n\n\nResources You\u2019ll be Using\n\n\nAlthough we have provided you with an environment which contains all the\ntools and data you will be using in this module, you may like to know\nwhere we have sourced those tools and data from.\n\n\nTools Used\n\n\nVelvet:\n\n\nhttp://www.ebi.ac.uk/~zerbino/velvet/\n\n\nAMOS Hawkeye:\n\n\nhttp://apps.sourceforge.net/mediawiki/amos/index.php?title=Hawkeye\n\n\ngnx-tools:\n\n\nhttps://github.com/mh11/gnx-tools\n\n\nFastQC:\n\n\nhttp://www.bioinformatics.bbsrc.ac.uk/projects/fastqc/\n\n\nR:\n\n\nhttp://www.r-project.org/\n\n\nSources of Data\n\n\n\n\n\n\nftp://ftp.ensemblgenomes.org/pub/release-8/bacteria/fasta/Staphylococcus/s_aureus_mrsa252/dna/s_aureus_mrsa252.EB1_s_aureus_mrsa252.dna.chromosome.Chromosome.fa.gz\n\n\n\n\n\n\nhttp://www.ebi.ac.uk/ena/data/view/SRX008042\n\n\n\n\n\n\nftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR022/SRR022852/SRR022852_1.fastq.gz\n\n\n\n\n\n\nftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR022/SRR022852/SRR022852_2.fastq.gz\n\n\n\n\n\n\nftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR023/SRR023408/SRR023408_1.fastq.gz\n\n\n\n\n\n\nftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR023/SRR023408/SRR023408_2.fastq.gz\n\n\n\n\n\n\nIntroduction\n\n\nThe aim of this module is to become familiar with performing \nde novo\n\ngenome assembly using Velvet, a de Bruijn graph based assembler, on a\nvariety of sequence data.\n\n\nPrepare the Environment\n\n\nThe first exercise should get you a little more comfortable with the\ncomputer environment and the command line.\n\n\nFirst make sure that you are in the denovo working directory by typing:\n\n\n1\ncd /home/trainee/denovo\n\n\n\n\n\n\nand making absolutely sure you\u2019re there by typing:\n\n\n1\npwd\n\n\n\n\n\n\nNow create sub-directories for this and the two other velvet practicals.\nAll these directories will be made as sub-directories of a directory for\nthe whole course called NGS. For this you can use the following\ncommands:\n\n\n1\nmkdir -p NGS/velvet/{part1,part2,part3}\n\n\n\n\n\n\nThe \n-p\n tells \nmkdir\n (make directory) to make any parent directories\nif they don\u2019t already exist. You could have created the above\ndirectories one-at-a-time by doing this instead:\n\n\n1\n2\n3\n4\nmkdir NGS\nmkdir NGS/velvet\nmkdir NGS/velvet/part1\nmkdir NGS/velvet/part2\n\n\n\n\n\n\nAfter creating the directories, examine the structure and move into the\ndirectory ready for the first velvet exercise by typing:\n\n\n1\n2\n3\nls -R NGS\ncd NGS/velvet/part1\npwd\n\n\n\n\n\n\nDownloading and Compiling Velvet\n\n\nFor the duration of this workshop, all the software you require has been\nset up for you already. This might not be the case when you return to\n\u201creal life\u201d. Many of the programs you will need, including velvet, are\nquite easy to set up, it might be instructive to try a couple.\n\n\nAlthough you will be using the preinstalled version of velvet, it is\nuseful to know how to compile velvet as some of the parameters you might\nlike to control can only be set at compile time. You can find the latest\nversion of velvet at:\n\n\nhttp://www.ebi.ac.uk/~zerbino/velvet/\n\n\nYou could go to this URL and download the latest velvet version, or\nequivalently, you could type the following, which will download, unpack,\ninspect, compile and execute your locally compiled version of velvet:\n\n\n1\n2\n3\n4\n5\n6\n7\ncd /home/trainee/denovo/NGS/velvet/part1\npwd\ntar xzf /home/trainee/denovo/data/velvet_1.2.10.tgz\nls -R\ncd velvet_1.2.10\nmake\n./velveth\n\n\n\n\n\n\nThe standout displayed to screen when \u2019make\u2019 runs may contain an error\nmessage but it is ignored\n\n\nTake a look at the executables you have created. They will be displayed\nas green by the command:\n\n\n1\nls --color=always\n\n\n\n\n\n\nThe switch \n\u2013color\n, instructs that files be coloured according to their\ntype. This is often the default but we are just being explicit. By\nspecifying the value \nalways\n, we ensure that colouring is always\napplied, even from a script.\n\n\nHave a look of the output the command produces and you will see that\n\nMAXKMERLENGTH=31\n and \nCATEGORIES=2\n parameters were passed into the\ncompiler.\n\n\nThis indicates that the default compilation was set for de Bruijn graph\nk-mers of maximum size 31 and to allow a maximum of just 2 read\ncategories. You can override these, and other, default configuration\nchoices using command line parameters. Assume, you want to run velvet\nwith a k-mer length of 41 using 3 categories, velvet needs to be\nrecompiled to enable this functionality by typing:\n\n\n1\n2\n3\nmake clean\nmake MAXKMERLENGTH=41 CATEGORIES=3\n./velveth\n\n\n\n\n\n\nDiscuss with the persons next to you the following questions:\n\n\n\n\nQuestion\n\n\nWhat are the consequences of the parameters you have given make for velvet?\n\n\n\n\nAnswer\nMAXKMERLENGTH: increase the max k-mer length from 31 to 41   \n\nCATEGORIES: paired-end data require to be put into separate categories. By increasing this parameter from 2 to 3 allows you to process 3 paired / mate-pair libraries and unpaired data.\n\n\n\n\nQuestion\n\n\nWhy does Velvet use k-mer 31 and 2 categories as default?\n\n\n\n\nAnswer\nPossibly a number of reason:    \n\n1) odd number to avoid palindromes      \n\n2) The first reads were very short (20-40 bp) and there were hardly any paired-end data around so there was no need to allow for longer k-mer lengths / more categories.      \n\n3) For programmers: 31 bp get stored in 64 bits (using 2bit encoding)\n\n\n\n\nQuestion\n\n\nShould you get better results by using a longer k-mer length?\n\n\n\n\nAnswer\nIf you can achieve a good k-mer coverage - yes.\n\n\nWhat effect would the following compile-time parameters have on velvet:\n\n\n\n\nQuestion\n\n\n\n\nOPENMP=Y\n\n\nAnswer\nTurn on multithreading\n\n\n\n\nQuestion\n\n\nLONGSEQUENCES=Y\n\n\n\n\nAnswer\nAssembling reads / contigs longer than 32kb long\n\n\n\n\nQuestion\n\n\nBIGASSEMBLY=Y\n\n\n\n\nAnswer\nUsing more than 2.2 billion reads\n\n\n\n\nQuestion\n\n\nSINGLE_COV_CAT=Y\n\n\n\n\nAnswer\nMerge all coverage statistics into a single variable - save memory\n\n\nFor a further description of velvet compile and runtime parameters\nplease see the velvet Manual:\n\nhttps://github.com/dzerbino/velvet/wiki/Manual\n\n\nAssembling Paired-end Reads using Velvet\n\n\nThe use of paired-end data in \nde novo\n genome assembly results in\nbetter quality assemblies, particularly for larger, more complex\ngenomes. In addition, paired-end constraint violation (expected distance\nand orientation of paired reads) can be used to identify misassemblies.\n\n\nIf you are doing \nde novo\n assembly, pay the extra and get paired-ends:\nthey\u2019re worth it!\n\n\nThe data you will examine in this exercise is again from Staphylococcus\naureus which has a genome of around 3MBases. The reads are Illumina\npaired end with an insert size of ~350 bp.\n\n\nThe required data can be downloaded from the SRA. Specifically, the run\ndata (SRR022852) from the SRA Sample SRS004748.\n\n\nThe following exercise focuses on preparing the paired-end FASTQ files\nready for Velvet, using Velvet in paired-end mode and comparing results\nwith Velvet\u2019s \u2019auto\u2019 option.\n\n\nFirst move to the directory you made for this exercise and make a\nsuitable named directory for the exercise:\n\n\n1\n2\n3\ncd /home/trainee/denovo/NGS/velvet/part2 \nmkdir SRS004748 \ncd SRS004748\n\n\n\n\n\n\nThere is no need to download the read files, as they are already stored\nlocally. You will simply create a symlink to this pre-downloaded data\nusing the following commands:\n\n\n1\nln -s /home/trainee/denovo/data/SRR022852_?.fastq.gz ./\n\n\n\n\n\n\nIt is interesting to monitor the computer\u2019s resource utilisation,\nparticularly memory. A simple way to do this is to open a second\nterminal and in it type:\n\n\n1\ntop\n\n\n\n\n\n\ntop\n is a program that continually monitors all the processes running\non your computer, showing the resources used by each. Leave this running\nand refer to it periodically throughout your Velvet analyses.\nParticularly if they are taking a long time or whenever your curiosity\ngets the better of you. You should find that as this practical\nprogresses, memory usage will increase significantly.\n\n\nNow, back to the first terminal, you are ready to run \nvelveth\n and\n\nvelvetg\n. The reads are \n-shortPaired\n and for the first run you should\nnot use any parameters for \nvelvetg\n.\n\n\nFrom this point on, where it will be informative to time your runs. This\nis very easy to do, just prefix the command to run the program with the\ncommand \ntime\n. This will cause UNIX to report how long the program took\nto complete its task.\n\n\nSet the two stages of velvet running, whilst you watch the memory usage\nas reported by \ntop\n. Time the \nvelvetg\n stage:\n\n\n1\n2\nvelveth run_25 25 -fmtAuto -create_binary -shortPaired -separate SRR022852_1.fastq.gz SRR022852_2.fastq.gz\ntime velvetg run_25\n\n\n\n\n\n\n\n\nQuestion\n\n\nWhat does \n-fmtAuto\n and \n-create_binary\n do? (see help menu)\n\n\n\n\nAnswer\n-fmtAuto\n tries to detect the correct format of the input files e.g. FASTA, FASTQ and whether they are compressed or not.   \n\n\n-create_binary\n outputs sequences as a binary file. That means that \nvelvetg\n can read the sequences from the binary file more quickly that from the original sequence files.\n\n\n\n\nQuestion\n\n\nComment on the use of memory and CPU for \nvelveth\n and \nvelvetg\n?\n\n\n\n\nAnswer\nvelveth\n uses only one CPU while \nvelvetg\n uses all possible CPUs for some parts of the calculation.\n\n\n\n\nQuestion\n\n\nHow long did \nvelvetg\n take?\n\n\n\n\nAnswer\nMy own measurements are:   \n\n\nreal 1m8.877s; user 4m15.324s; sys 0m4.716s\n\n\nNext, after saving your \ncontigs.fa\n file from being overwritten, set\nthe cut-off parameters that you investigated in the previous exercise\nand rerun \nvelvetg\n. time and monitor the use of resources as\npreviously. Start with \n-cov_cutoff 16\n thus:\n\n\n1\n2\nmv run_25/contigs.fa run_25/contigs.fa.0\ntime velvetg run_25 -cov_cutoff 16\n\n\n\n\n\n\nUp until now, \nvelvetg\n has ignored the paired-end information. Now try\nrunning \nvelvetg\n with both \n-cov_cutoff 16\n and \n-exp_cov 26\n, but\nfirst save your \ncontigs.fa\n file. By using \n-cov_cutoff\n and\n\n-exp_cov\n, \nvelvetg\n tries to estimate the insert length, which you\nwill see in the \nvelvetg\n output. The command is, of course:\n\n\n1\n2\nmv run_25/contigs.fa run_25/contigs.fa.1\ntime velvetg run_25 -cov_cutoff 16 -exp_cov 26\n\n\n\n\n\n\n\n\nQuestion\n\n\nComment on the time required, use of memory and CPU for \nvelvetg\n?\n\n\n\n\nAnswer\nRuntime is lower when velvet can reuse previously calculated data. By using \n-exp_cov\n, the memory usage increases.\n\n\n\n\nQuestion\n\n\nWhich insert length does Velvet estimate?\n\n\n\n\nAnswer\nPaired-end library 1 has length: 228, sample standard deviation: 26\n\n\nNext try running \nvelvetg\n in \u2018paired-end mode\u2018. This entails running\n\nvelvetg\n specifying the insert length with the parameter \n-ins_length\n\nset to 350. Even though velvet estimates the insert length it is always\nadvisable to check / provide the insert length manually as velvet can\nget the statistics wrong due to noise. Just in case, save your last\nversion of \ncontigs.fa\n. The commands are:\n\n\n1\n2\n3\nmv run_25/contigs.fa run_25/contigs.fa.2\ntime velvetg run_25 -cov_cutoff 16 -exp_cov 26 -ins_length 350\nmv run_25/contigs.fa run_25/contigs.fa.3\n\n\n\n\n\n\n\n\nQuestion\n\n\nHow fast was this run?\n\n\n\n\nAnswer\nMy own measurements are:   \n\n\nreal 0m29.792s; user 1m4.372s; sys 0m3.880s\n\n\nTake a look into the Log file.\n\n\n\n\nQuestion\n\n\nWhat is the N50 value for the \nvelvetg\n runs using the switches\n\n\n\n\nAnswer\nBase run: 19,510 bp  \n\n\n-cov_cutoff 16\n: 24,739 bp   \n\n\n-cov_cutoff 16 -exp_cov 26\n: 61,793 bp   \n\n\n-cov_cutoff 16 -exp_cov 26 -ins_length 350\n: n50 of 62,740 bp; max 194,649 bp; total 2,871,093 bp     \n\n\nTry giving the \n-cov_cutoff\n and/or \n-exp_cov\n parameters the value\n\nauto\n. See the \nvelvetg\n help to show you how. The information Velvet\nprints during running includes information about the values used\n(coverage cut-off or insert length) when using the \nauto\n option.\n\n\n\n\nQuestion\n\n\nWhat coverage values does Velvet choose (hint: look at the output that Velvet produces while running)?\n\n\n\n\nAnswer\nMedian coverage depth = 26.021837  \n\nRemoving contigs with coverage \n 13.010918 \u2026\n\n\n\n\nQuestion\n\n\nHow does the N50 value change?\n\n\n\n\nAnswer\nn50 of 68,843 bp; max 194,645 bp; total 2,872,678 bp\n\n\nRun \ngnx\n on all the \ncontig.fa\n files you have generated in the course\nof this exercise. The command will be:\n\n\n1\ngnx -min 100 -nx 25,50,75 run_25/contigs.fa*\n\n\n\n\n\n\n\n\nQuestion\n\n\nFor which runs are there Ns in the \ncontigs.fa\n file and why?\n\n\n\n\nAnswer\ncontigs.fa.2, contigs.fa.3, contigs.fa\n\n\nVelvet tries to use the provided (or infers) the insert length and fills\nambiguous regions with Ns.\n\n\nComment on the number of contigs and total length generated for each\nrun.\n\n\n\n\n\n\n\n\nFilename\n\n\nNo. contigs\n\n\nTotal length\n\n\nNo. Ns\n\n\n\n\n\n\n\n\n\n\nContigs.fa.0\n\n\n631\n\n\n2,830,659\n\n\n0\n\n\n\n\n\n\nContigs.fa.1\n\n\n580\n\n\n2,832,670\n\n\n0\n\n\n\n\n\n\nContigs.fa.2\n\n\n166\n\n\n2,849,919\n\n\n4,847\n\n\n\n\n\n\nContigs.fa.3\n\n\n166\n\n\n2,856,795\n\n\n11,713\n\n\n\n\n\n\nContigs.fa\n\n\n163\n\n\n2,857,439\n\n\n11,526\n\n\n\n\n\n\n\n\nAMOS Hawkeye\n\n\nWe will now output the assembly in the AMOS massage format and visualise\nthe assembly using AMOS Hawkeye.\n\n\nRun \nvelvetg\n with appropriate arguments and output the AMOS message\nfile, then convert it to an AMOS bank and open it in Hawkeye:\n\n\n1\n2\n3\ntime velvetg run_25 -cov_cutoff 16 -exp_cov 26 -ins_length 350 -amos_file yes -read_trkg yes \ntime bank-transact -c -b run_25/velvet_asm.bnk -m run_25/velvet_asm.afg         \nhawkeye run_25/velvet_asm.bnk\n\n\n\n\n\n\nLooking at the scaffold view of a contig, comment on the proportion of\n\u201chappy mates\u201d to \u201ccompressed mates\u201d and \u201cstretched mates\u201d.\n\n\nNearly all mates are compressed with no stretched mates and very few\nhappy mates.\n\n\n\n\nQuestion\n\n\nWhat is the mean and standard deviation of the insert size reported under the Libraries tab?\n\n\n\n\nAnswer\nMean: 350 bp SD: 35 bp\n\n\n\n\nQuestion\n\n\nLook at the actual distribution of insert sizes for this library. Can you explain where there is a difference between the mean and SD reported in those two places?\n\n\n\n\nAnswer\nWe specified \n-ins_length 350\n to the \nvelvetg\n command. Velvet uses this value, in the AMOS message file that it outputs, rather than its own estimate.\n\n\nYou can get AMOS to re-estimate the mean and SD of insert sizes using\nintra-contig pairs. First, close Hawkeye and then run the following\ncommands before reopening the AMOS bank to see what has changed.\n\n\n1\n2\nasmQC -b run_25/velvet_asm.bnk -scaff -recompute -update -numsd 2\nhawkeye run_25/velvet_asm.bnk\n\n\n\n\n\n\nLooking at the scaffold view of a contig, comment on the proportion of\n\u201chappy mates\u201d to \u201ccompressed mates\u201d and \u201cstretched mates\u201d.\n\n\nThere are only a few compressed and stretched mates compared to happy\nmates. There are similar numbers of stretched and compressed mates.\n\n\n\n\nQuestion\n\n\nWhat is the mean and standard deviation of the insert size reported under the Libraries tab?\n\n\n\n\nAnswer\nMean: 226 bp SD: 25 bp\n\n\n\n\nQuestion\n\n\nLook at the actual distribution of insert sizes for this library. Does the mean and SD reported in both places now match?\n\n\n\n\nAnswer\nYes\n\n\n\n\nQuestion\n\n\nCan you find a region with an unusually high proportion of stretched, compressed, incorrectly orientated or linking mates? What might this situation indicate?\n\n\n\n\nAnswer\nThis would indicate a possible misassembly and worthy of further investigation.   \n\nLook at the largest scaffold, there are stacks of stretched pairs which span contig boundaries. This indicates that the gap size has been underestimated during the scaffolding phase.\n\n\nVelvet and Data Quality\n\n\nSo far we have used the raw read data without performing any quality\ncontrol or read trimming prior to doing our velvet assemblies.\n\n\nVelvet does not use quality information present in FASTQ files.\n\n\nFor this reason, it is vitally important to perform read QC and quality\ntrimming. In doing so, we remove errors/noise from the dataset which in\nturn means velvet will run faster, will use less memory and will produce\na better assembly. Assuming we haven\u2019t compromised too much on coverage.\n\n\nTo investigate the effect of data quality, we will use the run data\n(SRR023408) from the SRA experiment SRX008042. The reads are Illumina\npaired end with an insert size of 92 bp.\n\n\nGo back to the main directory for this exercise and create and enter a\nnew directory dedicated to this phase of the exercise. The commands are:\n\n\n1\n2\n3\ncd /home/trainee/denovo/NGS/velvet/part2 \nmkdir SRX008042 \ncd SRX008042\n\n\n\n\n\n\nCreate symlinks to the read data files that we downloaded for you from\nthe SRA:\n\n\n1\nln -s /home/trainee/denovo/data/SRR023408_?.fastq.gz ./\n\n\n\n\n\n\nWe will use FastQC, a tool you should be familiar with, to visualise the\nquality of our data. We will use FastQC in the Graphical User Interface\n(GUI) mode.\n\n\nStart FastQC and set the process running in the background, by using a\ntrailing \n, so we get control of our terminal back for entering more\ncommands:\n\n\n1\nfastqc \n\n\n\n\n\n\n\nOpen the two compressed FASTQ files (File -\n Open) by selecting them\nboth and clicking OK). Look at tabs for both files:\n\n\n\n\n\n\nQuestion\n\n\nAre the quality scores the same for both files?\n\n\n\n\nAnswer\nOverall yes\n\n\n\n\nQuestion\n\n\nWhich value varies?\n\n\n\n\nAnswer\nPer sequence quality scores\n\n\n\n\nQuestion\n\n\nTake a look at the Per base sequence quality for both files. Did you note that it is not good for either file?\n\n\n\n\nAnswer\nThe quality score of both files drop very fast. Qualities of the REV strand drop faster than the FWD strand. This is because the template has been sat around while the FWD strand was sequenced.\n\n\n\n\nQuestion\n\n\nAt which positions would you cut the reads if we did \u201cfixed length trimming\u201d?\n\n\n\n\nAnswer\nLooking at the \u201cPer base quality\n and \u201cPer base sequence content\u201d, I would choose around 27\n\n\n\n\nQuestion\n\n\nWhy does the quality deteriorate towards the end of the read?\n\n\n\n\nAnswer\nErrors more likely for later cycles\n\n\n\n\nQuestion\n\n\nDoes it make sense to trim the 5\u2019 start of reads?\n\n\n\n\nAnswer\nLooking at the \u201cPer base sequence content\n, yes - there is a clear signal at the beginning.\n\n\nHave a look at the other options that FastQC offers.\n\n\n\n\nQuestion\n\n\nWhich other statistics could you use to support your trimming strategy?\n\n\n\n\nAnswer\n\u201cPer base sequence content\n, \u201cPer base GC content\n, \u201cKmer content\n, \u201cPer base sequence quality\n\n\n\n\nOnce you have decided what your trim points will be, close FastQC. We\nwill use \nfastx_trimmer\n from the FASTX-Toolkit to perform fixed-length\ntrimming. For usage information see the help:\n\n\n1\nfastx_trimmer -h\n\n\n\n\n\n\nfastx_trimmer\n is not able to read compressed FASTQ files, so we first\nneed to decompress the files ready for input.\n\n\nThe suggestion (hopefully not far from your own thoughts?) is that you\ntrim your reads as follows:\n\n\n1\n2\n3\n4\ngunzip \n SRR023408_1.fastq.gz \n SRR023408_1.fastq\ngunzip \n SRR023408_2.fastq.gz \n SRR023408_2.fastq\nfastx_trimmer -Q 33 -f 1 -l 32 -i SRR023408_1.fastq -o SRR023408_trim1.fastq \nfastx_trimmer -Q 33 -f 1 -l 27 -i SRR023408_2.fastq -o SRR023408_trim2.fastq\n\n\n\n\n\n\nMany NGS read files are large. This means that simply reading and\nwriting files can become the bottleneck, also known as I/O bound.\nTherefore, it is often good practice to avoid unnecessary disk\nread/write.\n\n\nWe could do what is called pipelining to send a stream of data from one\ncommand to another, using the pipe (\n|\n) character, without the need for\nintermediary files. The following command would achieve this:\n\n\n1\n2\ngunzip --to-stdout \n SRR023408_1.fastq.gz | fastx_trimmer -Q 33 -f 4 -l 32 -o SRR023408_trim1.fastq \ngunzip --to-stdout \n SRR023408_2.fastq.gz | fastx_trimmer -Q 33 -f 3 -l 29 -o SRR023408_trim2.fastq\n\n\n\n\n\n\nNow run \nvelveth\n with a k-mer value of 21 for both the untrimmed and\ntrimmed read files in \n-shortPaired\n mode. Separate the output of the\ntwo executions of \nvelveth\n into suitably named directories, followed by\n\nvelvetg\n:\n\n\n1\n2\n3\n4\n5\n6\n7\n# untrimmed reads\nvelveth run_21 21 -fmtAuto -create_binary -shortPaired -separate SRR023408_1.fastq SRR023408_2.fastq\ntime velvetg run_21\n\n# trimmed reads\nvelveth run_21trim 21 -fmtAuto -create_binary -shortPaired -separate SRR023408_trim1.fastq SRR023408_trim2.fastq\ntime velvetg run_21trim\n\n\n\n\n\n\n\n\nQuestion\n\n\nHow long did the two \nvelvetg\n runs take?\n\n\n\n\nAnswer\nrun_25: \nreal 3m16.132s; user 8m18.261s; sys 0m7.317s\n   \n\nrun_25trim: \nreal 1m18.611s; user 3m53.140s; sys 0m4.962s\n\n\n\n\nQuestion\n\n\nWhat N50 scores did you achieve?\n\n\n\n\nAnswer\nUntrimmed: 11   \n\nTrimmed: 15\n\n\n\n\nQuestion\n\n\nWhat were the overall effects of trimming?\n\n\n\n\nAnswer\nTime saving, increased N50, reduced coverage\n\n\nThe evidence is that trimming improved the assembly. The thing to do\nsurely, is to run \nvelvetg\n with the \n-cov_cutoff\n and \n-exp_cov\n. In\norder to use \n-cov_cutoff\n and \n-exp_cov\n sensibly, you need to\ninvestigate with R, as you did in the previous exercise, what parameter\nvalues to use. Start up R and produce the weighted histograms:\n\n\n1\n2\n3\n4\n5\n6\n7\nR \n--\nno\n-\nsave\n\n\nlibrary\n(\nplotrix\n)\n \ndata \n-\n read.table\n(\nrun_21/stats.txt\n,\n header\n=\nTRUE\n)\n \ndata2 \n-\n read.table\n(\nrun_21trim/stats.txt\n,\n header\n=\nTRUE\n)\n \npar\n(\nmfrow\n=\nc\n(\n1\n,\n2\n))\n\nweighted.hist\n(\ndata\n$\nshort1_cov\n,\n data\n$\nlgth\n,\n breaks\n=\n0\n:\n50\n)\n\nweighted.hist\n(\ndata2\n$\nshort1_cov\n,\n data2\n$\nlgth\n,\n breaks\n=\n0\n:\n50\n)\n\n\n\n\n\n\n\nWeighted k-mer coverage histograms of the paired-end reads pre-trimmed (left) and post-trimmed\n(right).\n\n\n\n\nFor the untrimmed read histogram (left) there is an expected coverage of\naround 13 with a coverage cut-off of around 7. For the trimmed read\nhistogram (right) there is an expected coverage of around 9 with a\ncoverage cut-off of around 5.\n\n\nIf you disagree, feel free to try different settings, but first quit R\nbefore running \nvelvetg\n:\n\n\n1\n2\n3\n4\nq()\n\ntime velvetg run_21 -cov_cutoff 7 -exp_cov 13 -ins_length 92\ntime velvetg run_21trim -cov_cutoff 5 -exp_cov 9 -ins_length 92\n\n\n\n\n\n\n\n\nQuestion\n\n\nHow good does it look now?\n\n\n\n\nAnswer\nStill not great   \n\nRuntime: Reduced runtime   \n\nMemory: Lower memory usage\n\n\n\n\nQuestion\n\n\nK-mer choice (Can you use k-mer 31 for a read of length 30 bp?)\n\n\n\n\nAnswer\nK-mer has to be lower than the read length and the K-mer coverage should be sufficient to produce results.\n\n\n\n\nQuestion\n\n\nDoes less data mean \u201cworse\n results?\n\n\n\n\nAnswer\nNot necessarily. If you have lots of data you can safely remove poor data without too much impact on overall coverage.\n\n\nCompare the results, produced during the last exercises, with each other, \n\n\n\n\n\n\n\n\nMetric\n\n\nSRR023408\n\n\nSRR023408.trimmed\n\n\n\n\n\n\n\n\n\n\nOverall Quality (1-5)\n\n\n5\n\n\n4\n\n\n\n\n\n\nbp Coverage\n\n\n95x (37bp; 7761796)\n\n\n82x (32bp; 7761796)\n\n\n\n\n\n\nk-mer Coverage\n\n\n43x (21); 33x (25)\n\n\n30x (21); 20.5x (25)\n\n\n\n\n\n\nN50 (k-mer used)\n\n\n2,803 (21)\n\n\n2,914 (21)\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\nWhat would you consider as the \u201cbest\n assembly?\n\n\n\n\nAnswer\nSRR023408.trimmed\n\n\n\n\nQuestion\n\n\nIf you found a candidate, why do you consider it as \u201cbest\n assembly?\n\n\n\n\nAnswer\nOverall data quality and coverage\n\n\n\n\n\n\nQuestion\n\n\nHow else might you assess the the quality of an assembly? \n\n\n\n\nHint\nHawkeye\n\n\nAnswer\nBy trying to identify paired-end constraint violations using AMOS Hawkeye.", 
            "title": "de novo Genome Assembly"
        }, 
        {
            "location": "/modules/btp-module-velvet/velvet/#key-learning-outcomes", 
            "text": "After completing this practical the trainee should be able to:    Compile velvet with appropriate compile-time parameters set for a\n    specific analysis    Be able to choose appropriate assembly parameters    Assemble a set of paired-end reads from a single insert-size library    Be able to visualise an assembly in AMOS Hawkeye", 
            "title": "Key Learning Outcomes"
        }, 
        {
            "location": "/modules/btp-module-velvet/velvet/#resources-youll-be-using", 
            "text": "Although we have provided you with an environment which contains all the\ntools and data you will be using in this module, you may like to know\nwhere we have sourced those tools and data from.", 
            "title": "Resources You\u2019ll be Using"
        }, 
        {
            "location": "/modules/btp-module-velvet/velvet/#tools-used", 
            "text": "Velvet:  http://www.ebi.ac.uk/~zerbino/velvet/  AMOS Hawkeye:  http://apps.sourceforge.net/mediawiki/amos/index.php?title=Hawkeye  gnx-tools:  https://github.com/mh11/gnx-tools  FastQC:  http://www.bioinformatics.bbsrc.ac.uk/projects/fastqc/  R:  http://www.r-project.org/", 
            "title": "Tools Used"
        }, 
        {
            "location": "/modules/btp-module-velvet/velvet/#sources-of-data", 
            "text": "ftp://ftp.ensemblgenomes.org/pub/release-8/bacteria/fasta/Staphylococcus/s_aureus_mrsa252/dna/s_aureus_mrsa252.EB1_s_aureus_mrsa252.dna.chromosome.Chromosome.fa.gz    http://www.ebi.ac.uk/ena/data/view/SRX008042    ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR022/SRR022852/SRR022852_1.fastq.gz    ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR022/SRR022852/SRR022852_2.fastq.gz    ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR023/SRR023408/SRR023408_1.fastq.gz    ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR023/SRR023408/SRR023408_2.fastq.gz", 
            "title": "Sources of Data"
        }, 
        {
            "location": "/modules/btp-module-velvet/velvet/#introduction", 
            "text": "The aim of this module is to become familiar with performing  de novo \ngenome assembly using Velvet, a de Bruijn graph based assembler, on a\nvariety of sequence data.", 
            "title": "Introduction"
        }, 
        {
            "location": "/modules/btp-module-velvet/velvet/#prepare-the-environment", 
            "text": "The first exercise should get you a little more comfortable with the\ncomputer environment and the command line.  First make sure that you are in the denovo working directory by typing:  1 cd /home/trainee/denovo   and making absolutely sure you\u2019re there by typing:  1 pwd   Now create sub-directories for this and the two other velvet practicals.\nAll these directories will be made as sub-directories of a directory for\nthe whole course called NGS. For this you can use the following\ncommands:  1 mkdir -p NGS/velvet/{part1,part2,part3}   The  -p  tells  mkdir  (make directory) to make any parent directories\nif they don\u2019t already exist. You could have created the above\ndirectories one-at-a-time by doing this instead:  1\n2\n3\n4 mkdir NGS\nmkdir NGS/velvet\nmkdir NGS/velvet/part1\nmkdir NGS/velvet/part2   After creating the directories, examine the structure and move into the\ndirectory ready for the first velvet exercise by typing:  1\n2\n3 ls -R NGS\ncd NGS/velvet/part1\npwd", 
            "title": "Prepare the Environment"
        }, 
        {
            "location": "/modules/btp-module-velvet/velvet/#downloading-and-compiling-velvet", 
            "text": "For the duration of this workshop, all the software you require has been\nset up for you already. This might not be the case when you return to\n\u201creal life\u201d. Many of the programs you will need, including velvet, are\nquite easy to set up, it might be instructive to try a couple.  Although you will be using the preinstalled version of velvet, it is\nuseful to know how to compile velvet as some of the parameters you might\nlike to control can only be set at compile time. You can find the latest\nversion of velvet at:  http://www.ebi.ac.uk/~zerbino/velvet/  You could go to this URL and download the latest velvet version, or\nequivalently, you could type the following, which will download, unpack,\ninspect, compile and execute your locally compiled version of velvet:  1\n2\n3\n4\n5\n6\n7 cd /home/trainee/denovo/NGS/velvet/part1\npwd\ntar xzf /home/trainee/denovo/data/velvet_1.2.10.tgz\nls -R\ncd velvet_1.2.10\nmake\n./velveth   The standout displayed to screen when \u2019make\u2019 runs may contain an error\nmessage but it is ignored  Take a look at the executables you have created. They will be displayed\nas green by the command:  1 ls --color=always   The switch  \u2013color , instructs that files be coloured according to their\ntype. This is often the default but we are just being explicit. By\nspecifying the value  always , we ensure that colouring is always\napplied, even from a script.  Have a look of the output the command produces and you will see that MAXKMERLENGTH=31  and  CATEGORIES=2  parameters were passed into the\ncompiler.  This indicates that the default compilation was set for de Bruijn graph\nk-mers of maximum size 31 and to allow a maximum of just 2 read\ncategories. You can override these, and other, default configuration\nchoices using command line parameters. Assume, you want to run velvet\nwith a k-mer length of 41 using 3 categories, velvet needs to be\nrecompiled to enable this functionality by typing:  1\n2\n3 make clean\nmake MAXKMERLENGTH=41 CATEGORIES=3\n./velveth   Discuss with the persons next to you the following questions:   Question  What are the consequences of the parameters you have given make for velvet?   Answer MAXKMERLENGTH: increase the max k-mer length from 31 to 41    \nCATEGORIES: paired-end data require to be put into separate categories. By increasing this parameter from 2 to 3 allows you to process 3 paired / mate-pair libraries and unpaired data.   Question  Why does Velvet use k-mer 31 and 2 categories as default?   Answer Possibly a number of reason:     \n1) odd number to avoid palindromes       \n2) The first reads were very short (20-40 bp) and there were hardly any paired-end data around so there was no need to allow for longer k-mer lengths / more categories.       \n3) For programmers: 31 bp get stored in 64 bits (using 2bit encoding)   Question  Should you get better results by using a longer k-mer length?   Answer If you can achieve a good k-mer coverage - yes.  What effect would the following compile-time parameters have on velvet:   Question   OPENMP=Y  Answer Turn on multithreading   Question  LONGSEQUENCES=Y   Answer Assembling reads / contigs longer than 32kb long   Question  BIGASSEMBLY=Y   Answer Using more than 2.2 billion reads   Question  SINGLE_COV_CAT=Y   Answer Merge all coverage statistics into a single variable - save memory  For a further description of velvet compile and runtime parameters\nplease see the velvet Manual: https://github.com/dzerbino/velvet/wiki/Manual", 
            "title": "Downloading and Compiling Velvet"
        }, 
        {
            "location": "/modules/btp-module-velvet/velvet/#assembling-paired-end-reads-using-velvet", 
            "text": "The use of paired-end data in  de novo  genome assembly results in\nbetter quality assemblies, particularly for larger, more complex\ngenomes. In addition, paired-end constraint violation (expected distance\nand orientation of paired reads) can be used to identify misassemblies.  If you are doing  de novo  assembly, pay the extra and get paired-ends:\nthey\u2019re worth it!  The data you will examine in this exercise is again from Staphylococcus\naureus which has a genome of around 3MBases. The reads are Illumina\npaired end with an insert size of ~350 bp.  The required data can be downloaded from the SRA. Specifically, the run\ndata (SRR022852) from the SRA Sample SRS004748.  The following exercise focuses on preparing the paired-end FASTQ files\nready for Velvet, using Velvet in paired-end mode and comparing results\nwith Velvet\u2019s \u2019auto\u2019 option.  First move to the directory you made for this exercise and make a\nsuitable named directory for the exercise:  1\n2\n3 cd /home/trainee/denovo/NGS/velvet/part2 \nmkdir SRS004748 \ncd SRS004748   There is no need to download the read files, as they are already stored\nlocally. You will simply create a symlink to this pre-downloaded data\nusing the following commands:  1 ln -s /home/trainee/denovo/data/SRR022852_?.fastq.gz ./   It is interesting to monitor the computer\u2019s resource utilisation,\nparticularly memory. A simple way to do this is to open a second\nterminal and in it type:  1 top   top  is a program that continually monitors all the processes running\non your computer, showing the resources used by each. Leave this running\nand refer to it periodically throughout your Velvet analyses.\nParticularly if they are taking a long time or whenever your curiosity\ngets the better of you. You should find that as this practical\nprogresses, memory usage will increase significantly.  Now, back to the first terminal, you are ready to run  velveth  and velvetg . The reads are  -shortPaired  and for the first run you should\nnot use any parameters for  velvetg .  From this point on, where it will be informative to time your runs. This\nis very easy to do, just prefix the command to run the program with the\ncommand  time . This will cause UNIX to report how long the program took\nto complete its task.  Set the two stages of velvet running, whilst you watch the memory usage\nas reported by  top . Time the  velvetg  stage:  1\n2 velveth run_25 25 -fmtAuto -create_binary -shortPaired -separate SRR022852_1.fastq.gz SRR022852_2.fastq.gz\ntime velvetg run_25    Question  What does  -fmtAuto  and  -create_binary  do? (see help menu)   Answer -fmtAuto  tries to detect the correct format of the input files e.g. FASTA, FASTQ and whether they are compressed or not.     -create_binary  outputs sequences as a binary file. That means that  velvetg  can read the sequences from the binary file more quickly that from the original sequence files.   Question  Comment on the use of memory and CPU for  velveth  and  velvetg ?   Answer velveth  uses only one CPU while  velvetg  uses all possible CPUs for some parts of the calculation.   Question  How long did  velvetg  take?   Answer My own measurements are:     real 1m8.877s; user 4m15.324s; sys 0m4.716s  Next, after saving your  contigs.fa  file from being overwritten, set\nthe cut-off parameters that you investigated in the previous exercise\nand rerun  velvetg . time and monitor the use of resources as\npreviously. Start with  -cov_cutoff 16  thus:  1\n2 mv run_25/contigs.fa run_25/contigs.fa.0\ntime velvetg run_25 -cov_cutoff 16   Up until now,  velvetg  has ignored the paired-end information. Now try\nrunning  velvetg  with both  -cov_cutoff 16  and  -exp_cov 26 , but\nfirst save your  contigs.fa  file. By using  -cov_cutoff  and -exp_cov ,  velvetg  tries to estimate the insert length, which you\nwill see in the  velvetg  output. The command is, of course:  1\n2 mv run_25/contigs.fa run_25/contigs.fa.1\ntime velvetg run_25 -cov_cutoff 16 -exp_cov 26    Question  Comment on the time required, use of memory and CPU for  velvetg ?   Answer Runtime is lower when velvet can reuse previously calculated data. By using  -exp_cov , the memory usage increases.   Question  Which insert length does Velvet estimate?   Answer Paired-end library 1 has length: 228, sample standard deviation: 26  Next try running  velvetg  in \u2018paired-end mode\u2018. This entails running velvetg  specifying the insert length with the parameter  -ins_length \nset to 350. Even though velvet estimates the insert length it is always\nadvisable to check / provide the insert length manually as velvet can\nget the statistics wrong due to noise. Just in case, save your last\nversion of  contigs.fa . The commands are:  1\n2\n3 mv run_25/contigs.fa run_25/contigs.fa.2\ntime velvetg run_25 -cov_cutoff 16 -exp_cov 26 -ins_length 350\nmv run_25/contigs.fa run_25/contigs.fa.3    Question  How fast was this run?   Answer My own measurements are:     real 0m29.792s; user 1m4.372s; sys 0m3.880s  Take a look into the Log file.   Question  What is the N50 value for the  velvetg  runs using the switches   Answer Base run: 19,510 bp    -cov_cutoff 16 : 24,739 bp     -cov_cutoff 16 -exp_cov 26 : 61,793 bp     -cov_cutoff 16 -exp_cov 26 -ins_length 350 : n50 of 62,740 bp; max 194,649 bp; total 2,871,093 bp       Try giving the  -cov_cutoff  and/or  -exp_cov  parameters the value auto . See the  velvetg  help to show you how. The information Velvet\nprints during running includes information about the values used\n(coverage cut-off or insert length) when using the  auto  option.   Question  What coverage values does Velvet choose (hint: look at the output that Velvet produces while running)?   Answer Median coverage depth = 26.021837   \nRemoving contigs with coverage   13.010918 \u2026   Question  How does the N50 value change?   Answer n50 of 68,843 bp; max 194,645 bp; total 2,872,678 bp  Run  gnx  on all the  contig.fa  files you have generated in the course\nof this exercise. The command will be:  1 gnx -min 100 -nx 25,50,75 run_25/contigs.fa*    Question  For which runs are there Ns in the  contigs.fa  file and why?   Answer contigs.fa.2, contigs.fa.3, contigs.fa  Velvet tries to use the provided (or infers) the insert length and fills\nambiguous regions with Ns.  Comment on the number of contigs and total length generated for each\nrun.     Filename  No. contigs  Total length  No. Ns      Contigs.fa.0  631  2,830,659  0    Contigs.fa.1  580  2,832,670  0    Contigs.fa.2  166  2,849,919  4,847    Contigs.fa.3  166  2,856,795  11,713    Contigs.fa  163  2,857,439  11,526", 
            "title": "Assembling Paired-end Reads using Velvet"
        }, 
        {
            "location": "/modules/btp-module-velvet/velvet/#amos-hawkeye", 
            "text": "We will now output the assembly in the AMOS massage format and visualise\nthe assembly using AMOS Hawkeye.  Run  velvetg  with appropriate arguments and output the AMOS message\nfile, then convert it to an AMOS bank and open it in Hawkeye:  1\n2\n3 time velvetg run_25 -cov_cutoff 16 -exp_cov 26 -ins_length 350 -amos_file yes -read_trkg yes \ntime bank-transact -c -b run_25/velvet_asm.bnk -m run_25/velvet_asm.afg         \nhawkeye run_25/velvet_asm.bnk   Looking at the scaffold view of a contig, comment on the proportion of\n\u201chappy mates\u201d to \u201ccompressed mates\u201d and \u201cstretched mates\u201d.  Nearly all mates are compressed with no stretched mates and very few\nhappy mates.   Question  What is the mean and standard deviation of the insert size reported under the Libraries tab?   Answer Mean: 350 bp SD: 35 bp   Question  Look at the actual distribution of insert sizes for this library. Can you explain where there is a difference between the mean and SD reported in those two places?   Answer We specified  -ins_length 350  to the  velvetg  command. Velvet uses this value, in the AMOS message file that it outputs, rather than its own estimate.  You can get AMOS to re-estimate the mean and SD of insert sizes using\nintra-contig pairs. First, close Hawkeye and then run the following\ncommands before reopening the AMOS bank to see what has changed.  1\n2 asmQC -b run_25/velvet_asm.bnk -scaff -recompute -update -numsd 2\nhawkeye run_25/velvet_asm.bnk   Looking at the scaffold view of a contig, comment on the proportion of\n\u201chappy mates\u201d to \u201ccompressed mates\u201d and \u201cstretched mates\u201d.  There are only a few compressed and stretched mates compared to happy\nmates. There are similar numbers of stretched and compressed mates.   Question  What is the mean and standard deviation of the insert size reported under the Libraries tab?   Answer Mean: 226 bp SD: 25 bp   Question  Look at the actual distribution of insert sizes for this library. Does the mean and SD reported in both places now match?   Answer Yes   Question  Can you find a region with an unusually high proportion of stretched, compressed, incorrectly orientated or linking mates? What might this situation indicate?   Answer This would indicate a possible misassembly and worthy of further investigation.    \nLook at the largest scaffold, there are stacks of stretched pairs which span contig boundaries. This indicates that the gap size has been underestimated during the scaffolding phase.", 
            "title": "AMOS Hawkeye"
        }, 
        {
            "location": "/modules/btp-module-velvet/velvet/#velvet-and-data-quality", 
            "text": "So far we have used the raw read data without performing any quality\ncontrol or read trimming prior to doing our velvet assemblies.  Velvet does not use quality information present in FASTQ files.  For this reason, it is vitally important to perform read QC and quality\ntrimming. In doing so, we remove errors/noise from the dataset which in\nturn means velvet will run faster, will use less memory and will produce\na better assembly. Assuming we haven\u2019t compromised too much on coverage.  To investigate the effect of data quality, we will use the run data\n(SRR023408) from the SRA experiment SRX008042. The reads are Illumina\npaired end with an insert size of 92 bp.  Go back to the main directory for this exercise and create and enter a\nnew directory dedicated to this phase of the exercise. The commands are:  1\n2\n3 cd /home/trainee/denovo/NGS/velvet/part2 \nmkdir SRX008042 \ncd SRX008042   Create symlinks to the read data files that we downloaded for you from\nthe SRA:  1 ln -s /home/trainee/denovo/data/SRR023408_?.fastq.gz ./   We will use FastQC, a tool you should be familiar with, to visualise the\nquality of our data. We will use FastQC in the Graphical User Interface\n(GUI) mode.  Start FastQC and set the process running in the background, by using a\ntrailing  , so we get control of our terminal back for entering more\ncommands:  1 fastqc     Open the two compressed FASTQ files (File -  Open) by selecting them\nboth and clicking OK). Look at tabs for both files:    Question  Are the quality scores the same for both files?   Answer Overall yes   Question  Which value varies?   Answer Per sequence quality scores   Question  Take a look at the Per base sequence quality for both files. Did you note that it is not good for either file?   Answer The quality score of both files drop very fast. Qualities of the REV strand drop faster than the FWD strand. This is because the template has been sat around while the FWD strand was sequenced.   Question  At which positions would you cut the reads if we did \u201cfixed length trimming\u201d?   Answer Looking at the \u201cPer base quality  and \u201cPer base sequence content\u201d, I would choose around 27   Question  Why does the quality deteriorate towards the end of the read?   Answer Errors more likely for later cycles   Question  Does it make sense to trim the 5\u2019 start of reads?   Answer Looking at the \u201cPer base sequence content , yes - there is a clear signal at the beginning.  Have a look at the other options that FastQC offers.   Question  Which other statistics could you use to support your trimming strategy?   Answer \u201cPer base sequence content , \u201cPer base GC content , \u201cKmer content , \u201cPer base sequence quality   Once you have decided what your trim points will be, close FastQC. We\nwill use  fastx_trimmer  from the FASTX-Toolkit to perform fixed-length\ntrimming. For usage information see the help:  1 fastx_trimmer -h   fastx_trimmer  is not able to read compressed FASTQ files, so we first\nneed to decompress the files ready for input.  The suggestion (hopefully not far from your own thoughts?) is that you\ntrim your reads as follows:  1\n2\n3\n4 gunzip   SRR023408_1.fastq.gz   SRR023408_1.fastq\ngunzip   SRR023408_2.fastq.gz   SRR023408_2.fastq\nfastx_trimmer -Q 33 -f 1 -l 32 -i SRR023408_1.fastq -o SRR023408_trim1.fastq \nfastx_trimmer -Q 33 -f 1 -l 27 -i SRR023408_2.fastq -o SRR023408_trim2.fastq   Many NGS read files are large. This means that simply reading and\nwriting files can become the bottleneck, also known as I/O bound.\nTherefore, it is often good practice to avoid unnecessary disk\nread/write.  We could do what is called pipelining to send a stream of data from one\ncommand to another, using the pipe ( | ) character, without the need for\nintermediary files. The following command would achieve this:  1\n2 gunzip --to-stdout   SRR023408_1.fastq.gz | fastx_trimmer -Q 33 -f 4 -l 32 -o SRR023408_trim1.fastq \ngunzip --to-stdout   SRR023408_2.fastq.gz | fastx_trimmer -Q 33 -f 3 -l 29 -o SRR023408_trim2.fastq   Now run  velveth  with a k-mer value of 21 for both the untrimmed and\ntrimmed read files in  -shortPaired  mode. Separate the output of the\ntwo executions of  velveth  into suitably named directories, followed by velvetg :  1\n2\n3\n4\n5\n6\n7 # untrimmed reads\nvelveth run_21 21 -fmtAuto -create_binary -shortPaired -separate SRR023408_1.fastq SRR023408_2.fastq\ntime velvetg run_21\n\n# trimmed reads\nvelveth run_21trim 21 -fmtAuto -create_binary -shortPaired -separate SRR023408_trim1.fastq SRR023408_trim2.fastq\ntime velvetg run_21trim    Question  How long did the two  velvetg  runs take?   Answer run_25:  real 3m16.132s; user 8m18.261s; sys 0m7.317s     \nrun_25trim:  real 1m18.611s; user 3m53.140s; sys 0m4.962s   Question  What N50 scores did you achieve?   Answer Untrimmed: 11    \nTrimmed: 15   Question  What were the overall effects of trimming?   Answer Time saving, increased N50, reduced coverage  The evidence is that trimming improved the assembly. The thing to do\nsurely, is to run  velvetg  with the  -cov_cutoff  and  -exp_cov . In\norder to use  -cov_cutoff  and  -exp_cov  sensibly, you need to\ninvestigate with R, as you did in the previous exercise, what parameter\nvalues to use. Start up R and produce the weighted histograms:  1\n2\n3\n4\n5\n6\n7 R  -- no - save  library ( plotrix )  \ndata  -  read.table ( run_21/stats.txt ,  header = TRUE )  \ndata2  -  read.table ( run_21trim/stats.txt ,  header = TRUE )  \npar ( mfrow = c ( 1 , 2 )) \nweighted.hist ( data $ short1_cov ,  data $ lgth ,  breaks = 0 : 50 ) \nweighted.hist ( data2 $ short1_cov ,  data2 $ lgth ,  breaks = 0 : 50 )    Weighted k-mer coverage histograms of the paired-end reads pre-trimmed (left) and post-trimmed\n(right).   For the untrimmed read histogram (left) there is an expected coverage of\naround 13 with a coverage cut-off of around 7. For the trimmed read\nhistogram (right) there is an expected coverage of around 9 with a\ncoverage cut-off of around 5.  If you disagree, feel free to try different settings, but first quit R\nbefore running  velvetg :  1\n2\n3\n4 q()\n\ntime velvetg run_21 -cov_cutoff 7 -exp_cov 13 -ins_length 92\ntime velvetg run_21trim -cov_cutoff 5 -exp_cov 9 -ins_length 92    Question  How good does it look now?   Answer Still not great    \nRuntime: Reduced runtime    \nMemory: Lower memory usage   Question  K-mer choice (Can you use k-mer 31 for a read of length 30 bp?)   Answer K-mer has to be lower than the read length and the K-mer coverage should be sufficient to produce results.   Question  Does less data mean \u201cworse  results?   Answer Not necessarily. If you have lots of data you can safely remove poor data without too much impact on overall coverage.  Compare the results, produced during the last exercises, with each other,      Metric  SRR023408  SRR023408.trimmed      Overall Quality (1-5)  5  4    bp Coverage  95x (37bp; 7761796)  82x (32bp; 7761796)    k-mer Coverage  43x (21); 33x (25)  30x (21); 20.5x (25)    N50 (k-mer used)  2,803 (21)  2,914 (21)      Question  What would you consider as the \u201cbest  assembly?   Answer SRR023408.trimmed   Question  If you found a candidate, why do you consider it as \u201cbest  assembly?   Answer Overall data quality and coverage    Question  How else might you assess the the quality of an assembly?    Hint Hawkeye  Answer By trying to identify paired-end constraint violations using AMOS Hawkeye.", 
            "title": "Velvet and Data Quality"
        }, 
        {
            "location": "/modules/btp-module-denovo-canu/denovo_canu/", 
            "text": "Pacbio reads: assembly with command line tools\n\n\n\n\n\nThis tutorial demonstrates how to use long PacBio sequence reads to assemble a bacterial genome, including correcting the assembly with short Illumina reads.\n\n\nResources\n\n\nTools (and versions) used in this tutorial include:\n\n\n\n\ncanu 1.5 (requires java 1.8)\n\n\ninfoseq and sizeseq (part of EMBOSS) 6.6.0.0\n\n\ncirclator 1.5.1\n\n\nbwa 0.7.15\n\n\nsamtools 1.3.1\n\n\nmakeblastdb and blastn (part of blast) 2.4.0+\n\n\npilon 1.20\n\n\nspades 3.10.1\n\n\n\n\n\nLearning objectives\n\n\nAt the end of this tutorial, be able to:\n\n\n\n\nAssemble and circularise a bacterial genome from PacBio sequence data.\n\n\nRecover small plasmids missed by long read sequencing, using Illumina data\n\n\nExplore the effect of polishing assembled sequences with a different data set.\n\n\n\n\nOverview\n\n\nSimplified version of workflow:\n\n\n\n\nGet data\n\n\nThe files we need are:\n\n\n\n\npacbio.fastq.gz\n - the PacBio reads\n\n\nillumina_R1.fastq.gz\n - the Illumina forward reads\n\n\nillumina_R2.fastq.gz\n - the Illumina reverse reads\n\n\n\n\n\n\n\nNGS Workshop\n\n\nSome of these tools will take too long to run in this workshop. For these tools, we have pre-computed the output files. In this workshop, we will still enter in the commands and set the tool running, but will sometimes then stop the run and move on to pre-computed output files.\n\n\nIn your directory, along with the PacBio and Illumina files, you may also see folders of pre-computed data.\n\n\n\n\n\nSample information\n\n\nThe sample used in this tutorial is a gram-positive bacteria called \nStaphylococcus aureus\n (sample number 25747). This particular sample is from a strain that is resistant to the antibiotic methicillin (a type of penicillin). It is also called MRSA: methicillin-resistant \nStaphylococcus aureus\n. It was isolated from (human) blood and caused bacteraemia, an infection of the bloodstream.\n\n\nAssemble\n\n\n\n\nWe will use the assembly software called Canu, \nhttps://github.com/marbl/canu\n.\n\n\nRun Canu with these commands:\n\n\n\n\n1\ncanu -p canu -d canu_outdir_NGS genomeSize=2.8m -pacbio-raw pacbio.fastq.gz\n\n\n\n\n\n\n\n\nthe first \ncanu\n tells the program to run\n\n\n-p canu\n names prefix for output files (\ncanu\n)\n\n\n-d canu_outdir_NGS\n names output directory\n\n\n\n\ngenomeSize\n only has to be approximate.\n\n\n\n\ne.g. \nStaphylococcus aureus\n, 2.8m\n\n\ne.g. \nStreptococcus pyogenes\n, 1.8m\n\n\n\n\n\n\n\n\nCanu will correct, trim and assemble the reads.\n\n\n\n\nVarious output will be displayed on the screen.\n\n\n\n\nNGS workshop: As we don\nt have time for Canu to complete, stop the run by typing \nCtrl-C\n. We will look at pre-computed data in the folder \ncanu_outdir\n.\n\n\nCanu output\n\n\nMove into \ncanu_outdir\n and \nls\n to see the output files.\n\n\n\n\nThe \ncanu.contigs.fasta\n are the assembled sequences.\n\n\nThe \ncanu.unassembled.fasta\n are the reads that could not be assembled.\n\n\nThe \ncanu.correctedReads.fasta.gz\n are the corrected Pacbio reads that were used in the assembly.\n\n\nThe \ncanu.contigs.gfa\n is the graph of the assembly.\n\n\nDisplay summary information about the contigs: (\ninfoseq\n is a tool from \nEMBOSS\n)\n\n\n\n\n1\ninfoseq canu.contigs.fasta\n\n\n\n\n\n\n\n\nThis will show the contigs found by Canu. e.g.,\n\n\n\n\n1\n    - tig00000001   2851805\n\n\n\n\n\n\nThis looks like a chromosome of approximately 2.8 million bases.\n\n\nThis matches what we would expect for this sample. For other data, Canu may not be able to join all the reads into one contig, so there may be several contigs in the output. Also, the sample may contain some plasmids and these may be found full or partially by Canu as additional contigs.  \n\n\nChange Canu parameters if required\n\n\nIf the assembly is poor with many contigs, re-run Canu with extra sensitivity parameters; e.g.\n\n\n1\ncanu -p prefix -d outdir corMhapSensitivity=high corMinCoverage=0 genomeSize=2.8m -pacbio-raw pacbio.fastq.gz\n\n\n\n\n\n\nQuestions\n\n\n\n\nQuestion\n\n\nHow do long- and short-read assembly methods differ?\n\n\n\n\nAnswer\nShort reads are usually assembled using De Bruijn graphs. With long reads, there is a move back towards simpler overlap-layout-consensus methods.\n\n\n\n\nQuestion\n\n\nWhere can we find out the what the approximate genome size should be for the species being assembled?\n\n\n\n\nAnswer\nGo to \nhttps://www.ncbi.nlm.nih.gov/genome/\n - enter species name - click on Genome Assembly and Annotation report - sort table by clicking on the column header Size (Mb) - look at range of sizes in this column.\n\n\n\n\nQuestion\n\n\nWhere could you view the output \nfilename.gfa\n and what would it show?\n\n\n\n\nAnswer\nThis is the assembly graph. You can view it using the tool \nBandage\n, \nhttps://rrwick.github.io/Bandage/\n, to see how the contigs are connected (including ambiguities).\n\n\nTrim and circularise\n\n\nRun Circlator\n\n\nCirclator (\nhttps://github.com/sanger-pathogens/circlator\n)\nidentifies and trims overhangs (on chromosomes and plasmids) and orients the start position at an appropriate gene (e.g. dnaA). It takes in the assembled contigs from Canu, as well as the corrected reads prepared by Canu.\n\n\nOverhangs are shown in blue:\n\n\n\n\nAdapted from Figure 1. Hunt et al. Genome Biology 2015\n\n\nMove back into your main analysis folder.\n\n\nRun Circlator:\n\n\n1\ncirclator all --threads 4 --verbose canu_outdir/canu.contigs.fasta canu_outdir/canu.correctedReads.fasta.gz circlator_outdir_NGS\n\n\n\n\n\n\n\n\n--threads\n is the number of cores \n\n\n--verbose\n prints progress information to the screen\n\n\ncanu_outdir/canu.contigs.fasta\n is the file path to the input Canu assembly\n\n\ncanu_outdir/canu.correctedReads.fasta.gz\n is the file path to the corrected Pacbio reads - note, fastA not fastQ\n\n\ncirclator_outdir_NGS\n is the name of the output directory.\n\n\n\n\nNGS workshop: Stop the run by typing \nCtrl-C\n. We will look at pre-computed data in the folder \ncirclator_outdir\n.\n\n\nCirclator output\n\n\nMove into the \ncirclator_outdir\n directory and \nls\n to list files.\n\n\nWere the contigs circularised?\n\n\n1\nless 04.merge.circularise.log\n\n\n\n\n\n\n\n\nYes, the contig was circularised (last column).\n\n\nType \nq\n to exit.\n\n\n\n\nWhere were the contigs oriented (which gene)?\n\n\n1\nless 06.fixstart.log\n\n\n\n\n\n\n\n\nLook in the \ngene_name\n column.\n\n\nThe contig has been oriented at tr|A0A090N2A8|A0A090N2A8_STAAU, which is another name for dnaA. \n This is typically used as the start of bacterial chromosome sequences.\n\n\n\n\nWhat are the trimmed contig sizes?\n\n\n1\ninfoseq 06.fixstart.fasta\n\n\n\n\n\n\n\n\ntig00000001 2823331 (28564 bases trimmed)\n\n\n\n\nThis trimmed part is the overlap.\n\n\nRe-name the contigs file\n:\n\n\n\n\nThe trimmed contigs are in the file called \n06.fixstart.fasta\n.\n\n\nRe-name it \ncontig1.fasta\n:\n\n\n\n\n1\ncp 06.fixstart.fasta contig1.fasta\n\n\n\n\n\n\nOpen this file in a text editor (e.g. nano: \nnano contig1.fasta\n) and change the header to \nchromosome\n.\n\n\nMove the file back into the main folder (\nmv contig1.fasta ../\n).\n\n\nOptions\n\n\nIf all the contigs have not circularised with Circlator, an option is to change the \n--b2r_length_cutoff\n setting to approximately 2X the average read depth.\n\n\nQuestions\n\n\n\n\nQuestion\n\n\nWere all the contigs circularised? Why/why not?\n\n\n\n\nAnswer\nIn this example, the contig could be circularized because it contained the entire sequence, with overhangs that were trimmed.\n\n\n\n\nQuestion\n\n\nCirclator can set the start of the sequence at a particular gene. Which gene does it use? Is this appropriate for all contigs?\n\n\n\n\nAnswer\nCirclator uses dnaA for the chromosomal contig. For other contigs, it uses a centrally-located gene. However, ideally, plasmids would be oriented on a gene such as a rep gene. It is possible to provide a file to Circlator to do this.\n\n\nFind smaller plasmids\n\n\nPacbio reads are long, and may have been longer than small plasmids. We will look for any small plasmids using the Illumina reads.\n\n\nThis section involves several steps:\n\n\n\n\nUse the Canu+Circlator output of a trimmed assembly contig.\n\n\nMap all the Illumina reads against this PacBio-assembled contig.\n\n\nExtract any reads that \ndidn\nt\n map and assemble them together: this could be a plasmid, or part of a plasmid.\n\n\nLook for overhang: if found, trim.\n\n\n\n\nAlign Illumina reads to the PacBio contig\n\n\n\n\nIndex the contigs file:\n\n\n\n\n1\nbwa index contig1.fasta\n\n\n\n\n\n\n\n\nAlign Illumina reads using using bwa mem:\n\n\n\n\n1\nbwa mem -t 4 contig1.fasta illumina_R1.fastq.gz illumina_R2.fastq.gz | samtools sort \n aln_NGS.bam\n\n\n\n\n\n\n\n\nbwa mem\n is the alignment tool\n\n\n-t 4\n is the number of cores \n\n\ncontig1.fasta\n is the input assembly file\n\n\nillumina_R1.fastq.gz illumina_R2.fastq.gz\n are the Illumina reads\n\n\n| samtools sort\n pipes the output to samtools to sort\n\n\n aln_NGS.bam\n sends the alignment to the file \naln_NGS.bam\n\n\n\n\nNGS workshop: Stop the run by typing \nCtrl-C\n. We will use the pre-computed file called \naln.bam\n.\n\n\nExtract unmapped Illumina reads\n\n\n\n\nIndex the alignment file:\n\n\n\n\n1\nsamtools index aln.bam\n\n\n\n\n\n\n\n\nExtract the fastq files from the bam alignment - those reads that were unmapped to the Pacbio alignment - and save them in various \nunmapped\n files:\n\n\n\n\n1\nsamtools fastq -f 4 -1 unmapped.R1.fastq -2 unmapped.R2.fastq -s unmapped.RS.fastq aln.bam\n\n\n\n\n\n\n\n\nfastq\n is a command that coverts a \n.bam\n file into fastq format\n\n\n-f 4\n : only output unmapped reads\n\n\n-1\n : put R1 reads into a file called \nunmapped.R1.fastq\n\n\n-2\n : put R2 reads into a file called \nunmapped.R2.fastq\n\n\n-s\n : put singleton reads into a file called \nunmapped.RS.fastq\n\n\naln.bam\n : input alignment file\n\n\n\n\nWe now have three files of the unampped reads: \n unmapped.R1.fastq\n, \n unmapped.R2.fastq\n, \n unmapped.RS.fastq\n.\n\n\nAssemble the unmapped reads\n\n\n\n\nAssemble with Spades (\nhttp://cab.spbu.ru/software/spades/\n):\n\n\n\n\n1\nspades.py -1 unmapped.R1.fastq -2 unmapped.R2.fastq -s unmapped.RS.fastq --careful --cov-cutoff auto -o spades_assembly_NGS\n\n\n\n\n\n\n\n\n-1\n is input file forward\n\n\n-2\n is input file reverse\n\n\n-s\n is unpaired\n\n\n--careful\n minimizes mismatches and short indels\n\n\n--cov-cutoff auto\n computes the coverage threshold (rather than the default setting, \noff\n)\n\n\n-o\n is the output directory\n\n\n\n\nNGS workshop: Stop the run by typing \nCtrl-C\n. We will use the pre-computed file in the folder \nspades_assembly\n.\n\n\nMove into the output directory (\nspades_assembly\n) and look at the contigs:\n\n\n1\ninfoseq contigs.fasta\n\n\n\n\n\n\n\n\n78 contigs were assembled, with the max length of 2250 (the first contig).  \n\n\nAll other nodes are \n 650kb so we will disregard as they are unlikely to be plasmids.\n\n\nWe will extract the first sequence (NODE_1):\n\n\n\n\n1\nsamtools faidx contigs.fasta\n\n\n\n\n\n\n1\nsamtools faidx contigs.fasta NODE_1_length_2550_cov_496.613 \n contig2.fasta\n\n\n\n\n\n\n\n\nThis is now saved as \ncontig2.fasta\n\n\nOpen in nano and change header to \nplasmid\n.\n\n\n\n\nTrim the plasmid\n\n\nTo trim any overhang on this plasmid, we will blast the start of contig2 against itself.\n\n\n\n\nTake the start of the contig:\n\n\n\n\n1\nhead -n 10 contig2.fasta \n contig2.fa.head\n\n\n\n\n\n\n\n\nWe want to see if it matches the end (overhang).\n\n\nFormat the assembly file for blast:\n\n\n\n\n1\nmakeblastdb -in contig2.fasta -dbtype nucl\n\n\n\n\n\n\n\n\nBlast the start of the assembly (.head file) against all of the assembly:\n\n\n\n\n1\nblastn -query contig2.fa.head -db contig2.fasta -evalue 1e-3 -dust no -out contig2.bls\n\n\n\n\n\n\n\n\nLook at \ncontig2.bls\n to see hits:\n\n\n\n\n1\nless contig2.bls\n\n\n\n\n\n\n\n\nThe first hit is at start, as expected.\n\n\nThe second hit is at 2474 all the way to the end - 2550.\n\n\nThis is the overhang.\n\n\nTrim to position 2473.\n\n\nType \nq\n to exit.\n\n\nIndex the plasmid.fa file:\n\n\n\n\n1\nsamtools faidx contig2.fasta\n\n\n\n\n\n\n\n\nTrim\n\n\n\n\n1\nsamtools faidx contig2.fasta plasmid:1-2473 \n plasmid.fa.trimmed\n\n\n\n\n\n\n\n\n\n\nplasmid\n is the name of the contig, and we want the sequence from 1-2473.\n\n\n\n\n\n\nOpen this file in nano (\nnano plasmid.fa.trimmed\n) and change the header to \nplasmid\n, save.\n\n\n\n\n(Use the side scroll bar to see the top of the file.)\n\n\nWe now have a trimmed plasmid.\n\n\nMove file back into main folder:\n\n\n\n\n1\ncp plasmid.fa.trimmed ../\n\n\n\n\n\n\n\n\nMove into the main folder.\n\n\n\n\nPlasmid contig orientation\n\n\nThe bacterial chromosome was oriented at the gene dnaA. Plasmids are often oriented at the replication gene, but this is highly variable and there is no established convention. Here we will orient the plasmid at a gene found by Prodigal, in Circlator:\n\n\n1\ncirclator fixstart plasmid.fa.trimmed plasmid_fixstart\n\n\n\n\n\n\n\n\nfixstart\n is an option in Circlator just to orient a sequence.\n\n\nplasmid.fa.trimmed\n is our small plasmid.\n\n\nplasmid_fixstart\n is the prefix for the output files.\n\n\n\n\nView the output:\n\n\n1\nless plasmid_fixstart.log\n\n\n\n\n\n\n\n\nThe plasmid has been oriented at a gene predicted by Prodigal, and the break-point is at position 1200.\n\n\nChange the file name:\n\n\n\n\n1\ncp plasmid_fixstart.fasta contig2.fasta\n\n\n\n\n\n\n\n\n\nCollect contigs\n\n\n1\ncat contig1.fasta contig2.fasta \n genome.fasta\n\n\n\n\n\n\n\n\nSee the contigs and sizes:\n\n\n\n\n1\ninfoseq genome.fasta\n\n\n\n\n\n\n\n\nchromosome: 2823331\n\n\nplasmid: 2473\n\n\n\n\nQuestions\n\n\n\n\nQuestion\n\n\nWhy is this section so complicated?\n\n\n\n\nAnswer\nFinding small plasmids is difficult for many reasons! This paper has a nice summary: On the (im)possibility to reconstruct plasmids from whole genome short-read sequencing data. doi: \nhttps://doi.org/10.1101/086744\n\n\n\n\nQuestion\n\n\nWhy can PacBio sequencing miss small plasmids?\n\n\n\n\nAnswer\nLibrary prep size selection\n\n\n\n\nQuestion\n\n\nWe extract unmapped Illumina reads and assemble these to find small plasmids. What could they be missing?\n\n\n\n\nAnswer\nRepeats that have mapped to the PacBio assembly.\n\n\n\n\nQuestion\n\n\nHow do you find a plasmid in a Bandage graph?\n\n\n\n\nAnswer\nIt is probably circular, matches the size of a known plasmid, and has a rep gene.\n\n\n\n\nQuestion\n\n\nAre there easier ways to find plasmids?\n\n\n\n\nAnswer\nPossibly. One option is the program called Unicycler which may automate many of these steps. \nhttps://github.com/rrwick/Unicycler\n\n\nCorrect\n\n\nWe will correct the Pacbio assembly with Illumina reads, using the tool Pilon (\nhttps://github.com/broadinstitute/pilon/wiki\n).\n\n\nMake an alignment file\n\n\n\n\nAlign the Illumina reads (R1 and R2) to the draft PacBio assembly, e.g. \ngenome.fasta\n:\n\n\n\n\n1\n2\nbwa index genome.fasta\nbwa mem -t 4 genome.fasta illumina_R1.fastq.gz illumina_R2.fastq.gz | samtools sort \n aln_illumina_pacbio_NGS.bam\n\n\n\n\n\n\n\n\n-t\n is the number of cores \n\n\n\n\nNGS workshop: Stop the run by typing \nCtrl-C\n. We will use the pre-computed file called \naln_illumina_pacbio.bam\n.\n\n\n\n\nIndex the files:\n\n\n\n\n1\n2\nsamtools index aln_illumina_pacbio.bam\nsamtools faidx genome.fasta\n\n\n\n\n\n\n\n\nNow we have an alignment file to use in Pilon: \naln_illumina_pacbio.bam\n\n\n\n\nRun Pilon\n\n\n\n\nRun:\n\n\n\n\n1\npilon --genome genome.fasta --frags aln_illumina_pacbio.bam --output pilon1_NGS --fix all --mindepth 0.5 --changes --verbose --threads 4\n\n\n\n\n\n\n\n\n--genome\n is the name of the input assembly to be corrected\n\n\n--frags\n is the alignment of the reads against the assembly\n\n\n--output\n is the name of the output prefix\n\n\n--fix\n is an option for types of corrections\n\n\n--mindepth\n gives a minimum read depth to use\n\n\n--changes\n produces an output file of the changes made\n\n\n--verbose\n prints information to the screen during the run\n\n\n--threads\n: the number of cores\n\n\n\n\nNGS workshop: Stop the run by typing \nCtrl-C\n. We will use the pre-computed files called with the prefixes \npilon1\n.\n\n\nLook at the changes file:\n\n\n1\nless pilon1.changes\n\n\n\n\n\n\nExample:\n\n\n\n\nLook at the details of the fasta file:\n\n\n1\ninfoseq pilon1.fasta\n\n\n\n\n\n\n\n\nchromosome - 2823340 (net +9 bases)\n\n\nplasmid - 2473 (no change)\n\n\n\n\nOption:\n\n\nIf there are many changes, run Pilon again, using the \npilon1.fasta\n file as the input assembly, and the Illumina reads to correct.\n\n\nGenome output\n\n\n\n\nChange the file name:\n\n\n\n\n1\ncp pilon1.fasta assembly.fasta\n\n\n\n\n\n\n\n\nWe now have the corrected genome assembly of \nStaphylococcus aureus\n in .fasta format, containing a chromosome and a small plasmid.  \n\n\n\n\nQuestions\n\n\n\n\nQuestion\n\n\nWhy don\nt we correct earlier in the assembly process?\n\n\n\n\nAnswer\nWe need to circularise the contigs and trim overhangs first.\n\n\n\n\nQuestion\n\n\nWhy can we use some reads (Illumina) to correct other reads (PacBio) ?\n\n\n\n\nAnswer\nIllumina reads have higher accuracy\n\n\n\n\nQuestion\n\n\nCould we just use PacBio reads to assemble the genome?\n\n\n\n\nAnswer\nYes, if accuracy adequate.\n\n\n\n\n\nNext\n\n\nFurther analyses\n\n\n\n\nAnnotate genomes, e.g. with Prokka, \nhttps://github.com/tseemann/prokka\n\n\nComparative genomics, e.g. with Roary, \nhttps://sanger-pathogens.github.io/Roary/\n\n\n\n\nLinks\n\n\n\n\n\n\n\nCanu \nmanual\n and \ngitub repository\n\n\nCirclator \narticle\n and \ngithub repository\n\n\nPilon \narticle\n and \ngithub repository\n\n\nNotes on \nfinishing\n and \nevaluating\n assemblies.", 
            "title": "DeNovo Canu"
        }, 
        {
            "location": "/modules/btp-module-denovo-canu/denovo_canu/#pacbio-reads-assembly-with-command-line-tools", 
            "text": "This tutorial demonstrates how to use long PacBio sequence reads to assemble a bacterial genome, including correcting the assembly with short Illumina reads.", 
            "title": "Pacbio reads: assembly with command line tools"
        }, 
        {
            "location": "/modules/btp-module-denovo-canu/denovo_canu/#resources", 
            "text": "Tools (and versions) used in this tutorial include:   canu 1.5 (requires java 1.8)  infoseq and sizeseq (part of EMBOSS) 6.6.0.0  circlator 1.5.1  bwa 0.7.15  samtools 1.3.1  makeblastdb and blastn (part of blast) 2.4.0+  pilon 1.20  spades 3.10.1", 
            "title": "Resources"
        }, 
        {
            "location": "/modules/btp-module-denovo-canu/denovo_canu/#learning-objectives", 
            "text": "At the end of this tutorial, be able to:   Assemble and circularise a bacterial genome from PacBio sequence data.  Recover small plasmids missed by long read sequencing, using Illumina data  Explore the effect of polishing assembled sequences with a different data set.", 
            "title": "Learning objectives"
        }, 
        {
            "location": "/modules/btp-module-denovo-canu/denovo_canu/#overview", 
            "text": "Simplified version of workflow:", 
            "title": "Overview"
        }, 
        {
            "location": "/modules/btp-module-denovo-canu/denovo_canu/#get-data", 
            "text": "The files we need are:   pacbio.fastq.gz  - the PacBio reads  illumina_R1.fastq.gz  - the Illumina forward reads  illumina_R2.fastq.gz  - the Illumina reverse reads", 
            "title": "Get data"
        }, 
        {
            "location": "/modules/btp-module-denovo-canu/denovo_canu/#ngs-workshop", 
            "text": "Some of these tools will take too long to run in this workshop. For these tools, we have pre-computed the output files. In this workshop, we will still enter in the commands and set the tool running, but will sometimes then stop the run and move on to pre-computed output files.  In your directory, along with the PacBio and Illumina files, you may also see folders of pre-computed data.", 
            "title": "NGS Workshop"
        }, 
        {
            "location": "/modules/btp-module-denovo-canu/denovo_canu/#sample-information", 
            "text": "The sample used in this tutorial is a gram-positive bacteria called  Staphylococcus aureus  (sample number 25747). This particular sample is from a strain that is resistant to the antibiotic methicillin (a type of penicillin). It is also called MRSA: methicillin-resistant  Staphylococcus aureus . It was isolated from (human) blood and caused bacteraemia, an infection of the bloodstream.", 
            "title": "Sample information"
        }, 
        {
            "location": "/modules/btp-module-denovo-canu/denovo_canu/#assemble", 
            "text": "We will use the assembly software called Canu,  https://github.com/marbl/canu .  Run Canu with these commands:   1 canu -p canu -d canu_outdir_NGS genomeSize=2.8m -pacbio-raw pacbio.fastq.gz    the first  canu  tells the program to run  -p canu  names prefix for output files ( canu )  -d canu_outdir_NGS  names output directory   genomeSize  only has to be approximate.   e.g.  Staphylococcus aureus , 2.8m  e.g.  Streptococcus pyogenes , 1.8m     Canu will correct, trim and assemble the reads.   Various output will be displayed on the screen.   NGS workshop: As we don t have time for Canu to complete, stop the run by typing  Ctrl-C . We will look at pre-computed data in the folder  canu_outdir .", 
            "title": "Assemble"
        }, 
        {
            "location": "/modules/btp-module-denovo-canu/denovo_canu/#canu-output", 
            "text": "Move into  canu_outdir  and  ls  to see the output files.   The  canu.contigs.fasta  are the assembled sequences.  The  canu.unassembled.fasta  are the reads that could not be assembled.  The  canu.correctedReads.fasta.gz  are the corrected Pacbio reads that were used in the assembly.  The  canu.contigs.gfa  is the graph of the assembly.  Display summary information about the contigs: ( infoseq  is a tool from  EMBOSS )   1 infoseq canu.contigs.fasta    This will show the contigs found by Canu. e.g.,   1     - tig00000001   2851805   This looks like a chromosome of approximately 2.8 million bases.  This matches what we would expect for this sample. For other data, Canu may not be able to join all the reads into one contig, so there may be several contigs in the output. Also, the sample may contain some plasmids and these may be found full or partially by Canu as additional contigs.", 
            "title": "Canu output"
        }, 
        {
            "location": "/modules/btp-module-denovo-canu/denovo_canu/#change-canu-parameters-if-required", 
            "text": "If the assembly is poor with many contigs, re-run Canu with extra sensitivity parameters; e.g.  1 canu -p prefix -d outdir corMhapSensitivity=high corMinCoverage=0 genomeSize=2.8m -pacbio-raw pacbio.fastq.gz", 
            "title": "Change Canu parameters if required"
        }, 
        {
            "location": "/modules/btp-module-denovo-canu/denovo_canu/#questions", 
            "text": "Question  How do long- and short-read assembly methods differ?   Answer Short reads are usually assembled using De Bruijn graphs. With long reads, there is a move back towards simpler overlap-layout-consensus methods.   Question  Where can we find out the what the approximate genome size should be for the species being assembled?   Answer Go to  https://www.ncbi.nlm.nih.gov/genome/  - enter species name - click on Genome Assembly and Annotation report - sort table by clicking on the column header Size (Mb) - look at range of sizes in this column.   Question  Where could you view the output  filename.gfa  and what would it show?   Answer This is the assembly graph. You can view it using the tool  Bandage ,  https://rrwick.github.io/Bandage/ , to see how the contigs are connected (including ambiguities).", 
            "title": "Questions"
        }, 
        {
            "location": "/modules/btp-module-denovo-canu/denovo_canu/#trim-and-circularise", 
            "text": "", 
            "title": "Trim and circularise"
        }, 
        {
            "location": "/modules/btp-module-denovo-canu/denovo_canu/#run-circlator", 
            "text": "Circlator ( https://github.com/sanger-pathogens/circlator )\nidentifies and trims overhangs (on chromosomes and plasmids) and orients the start position at an appropriate gene (e.g. dnaA). It takes in the assembled contigs from Canu, as well as the corrected reads prepared by Canu.  Overhangs are shown in blue:   Adapted from Figure 1. Hunt et al. Genome Biology 2015  Move back into your main analysis folder.  Run Circlator:  1 circlator all --threads 4 --verbose canu_outdir/canu.contigs.fasta canu_outdir/canu.correctedReads.fasta.gz circlator_outdir_NGS    --threads  is the number of cores   --verbose  prints progress information to the screen  canu_outdir/canu.contigs.fasta  is the file path to the input Canu assembly  canu_outdir/canu.correctedReads.fasta.gz  is the file path to the corrected Pacbio reads - note, fastA not fastQ  circlator_outdir_NGS  is the name of the output directory.   NGS workshop: Stop the run by typing  Ctrl-C . We will look at pre-computed data in the folder  circlator_outdir .", 
            "title": "Run Circlator"
        }, 
        {
            "location": "/modules/btp-module-denovo-canu/denovo_canu/#circlator-output", 
            "text": "Move into the  circlator_outdir  directory and  ls  to list files.  Were the contigs circularised?  1 less 04.merge.circularise.log    Yes, the contig was circularised (last column).  Type  q  to exit.   Where were the contigs oriented (which gene)?  1 less 06.fixstart.log    Look in the  gene_name  column.  The contig has been oriented at tr|A0A090N2A8|A0A090N2A8_STAAU, which is another name for dnaA.   This is typically used as the start of bacterial chromosome sequences.   What are the trimmed contig sizes?  1 infoseq 06.fixstart.fasta    tig00000001 2823331 (28564 bases trimmed)   This trimmed part is the overlap.  Re-name the contigs file :   The trimmed contigs are in the file called  06.fixstart.fasta .  Re-name it  contig1.fasta :   1 cp 06.fixstart.fasta contig1.fasta   Open this file in a text editor (e.g. nano:  nano contig1.fasta ) and change the header to  chromosome .  Move the file back into the main folder ( mv contig1.fasta ../ ).", 
            "title": "Circlator output"
        }, 
        {
            "location": "/modules/btp-module-denovo-canu/denovo_canu/#options", 
            "text": "If all the contigs have not circularised with Circlator, an option is to change the  --b2r_length_cutoff  setting to approximately 2X the average read depth.", 
            "title": "Options"
        }, 
        {
            "location": "/modules/btp-module-denovo-canu/denovo_canu/#questions_1", 
            "text": "Question  Were all the contigs circularised? Why/why not?   Answer In this example, the contig could be circularized because it contained the entire sequence, with overhangs that were trimmed.   Question  Circlator can set the start of the sequence at a particular gene. Which gene does it use? Is this appropriate for all contigs?   Answer Circlator uses dnaA for the chromosomal contig. For other contigs, it uses a centrally-located gene. However, ideally, plasmids would be oriented on a gene such as a rep gene. It is possible to provide a file to Circlator to do this.", 
            "title": "Questions"
        }, 
        {
            "location": "/modules/btp-module-denovo-canu/denovo_canu/#find-smaller-plasmids", 
            "text": "Pacbio reads are long, and may have been longer than small plasmids. We will look for any small plasmids using the Illumina reads.  This section involves several steps:   Use the Canu+Circlator output of a trimmed assembly contig.  Map all the Illumina reads against this PacBio-assembled contig.  Extract any reads that  didn t  map and assemble them together: this could be a plasmid, or part of a plasmid.  Look for overhang: if found, trim.", 
            "title": "Find smaller plasmids"
        }, 
        {
            "location": "/modules/btp-module-denovo-canu/denovo_canu/#align-illumina-reads-to-the-pacbio-contig", 
            "text": "Index the contigs file:   1 bwa index contig1.fasta    Align Illumina reads using using bwa mem:   1 bwa mem -t 4 contig1.fasta illumina_R1.fastq.gz illumina_R2.fastq.gz | samtools sort   aln_NGS.bam    bwa mem  is the alignment tool  -t 4  is the number of cores   contig1.fasta  is the input assembly file  illumina_R1.fastq.gz illumina_R2.fastq.gz  are the Illumina reads  | samtools sort  pipes the output to samtools to sort   aln_NGS.bam  sends the alignment to the file  aln_NGS.bam   NGS workshop: Stop the run by typing  Ctrl-C . We will use the pre-computed file called  aln.bam .", 
            "title": "Align Illumina reads to the PacBio contig"
        }, 
        {
            "location": "/modules/btp-module-denovo-canu/denovo_canu/#extract-unmapped-illumina-reads", 
            "text": "Index the alignment file:   1 samtools index aln.bam    Extract the fastq files from the bam alignment - those reads that were unmapped to the Pacbio alignment - and save them in various  unmapped  files:   1 samtools fastq -f 4 -1 unmapped.R1.fastq -2 unmapped.R2.fastq -s unmapped.RS.fastq aln.bam    fastq  is a command that coverts a  .bam  file into fastq format  -f 4  : only output unmapped reads  -1  : put R1 reads into a file called  unmapped.R1.fastq  -2  : put R2 reads into a file called  unmapped.R2.fastq  -s  : put singleton reads into a file called  unmapped.RS.fastq  aln.bam  : input alignment file   We now have three files of the unampped reads:   unmapped.R1.fastq ,   unmapped.R2.fastq ,   unmapped.RS.fastq .", 
            "title": "Extract unmapped Illumina reads"
        }, 
        {
            "location": "/modules/btp-module-denovo-canu/denovo_canu/#assemble-the-unmapped-reads", 
            "text": "Assemble with Spades ( http://cab.spbu.ru/software/spades/ ):   1 spades.py -1 unmapped.R1.fastq -2 unmapped.R2.fastq -s unmapped.RS.fastq --careful --cov-cutoff auto -o spades_assembly_NGS    -1  is input file forward  -2  is input file reverse  -s  is unpaired  --careful  minimizes mismatches and short indels  --cov-cutoff auto  computes the coverage threshold (rather than the default setting,  off )  -o  is the output directory   NGS workshop: Stop the run by typing  Ctrl-C . We will use the pre-computed file in the folder  spades_assembly .  Move into the output directory ( spades_assembly ) and look at the contigs:  1 infoseq contigs.fasta    78 contigs were assembled, with the max length of 2250 (the first contig).    All other nodes are   650kb so we will disregard as they are unlikely to be plasmids.  We will extract the first sequence (NODE_1):   1 samtools faidx contigs.fasta   1 samtools faidx contigs.fasta NODE_1_length_2550_cov_496.613   contig2.fasta    This is now saved as  contig2.fasta  Open in nano and change header to  plasmid .", 
            "title": "Assemble the unmapped reads"
        }, 
        {
            "location": "/modules/btp-module-denovo-canu/denovo_canu/#trim-the-plasmid", 
            "text": "To trim any overhang on this plasmid, we will blast the start of contig2 against itself.   Take the start of the contig:   1 head -n 10 contig2.fasta   contig2.fa.head    We want to see if it matches the end (overhang).  Format the assembly file for blast:   1 makeblastdb -in contig2.fasta -dbtype nucl    Blast the start of the assembly (.head file) against all of the assembly:   1 blastn -query contig2.fa.head -db contig2.fasta -evalue 1e-3 -dust no -out contig2.bls    Look at  contig2.bls  to see hits:   1 less contig2.bls    The first hit is at start, as expected.  The second hit is at 2474 all the way to the end - 2550.  This is the overhang.  Trim to position 2473.  Type  q  to exit.  Index the plasmid.fa file:   1 samtools faidx contig2.fasta    Trim   1 samtools faidx contig2.fasta plasmid:1-2473   plasmid.fa.trimmed     plasmid  is the name of the contig, and we want the sequence from 1-2473.    Open this file in nano ( nano plasmid.fa.trimmed ) and change the header to  plasmid , save.   (Use the side scroll bar to see the top of the file.)  We now have a trimmed plasmid.  Move file back into main folder:   1 cp plasmid.fa.trimmed ../    Move into the main folder.", 
            "title": "Trim the plasmid"
        }, 
        {
            "location": "/modules/btp-module-denovo-canu/denovo_canu/#plasmid-contig-orientation", 
            "text": "The bacterial chromosome was oriented at the gene dnaA. Plasmids are often oriented at the replication gene, but this is highly variable and there is no established convention. Here we will orient the plasmid at a gene found by Prodigal, in Circlator:  1 circlator fixstart plasmid.fa.trimmed plasmid_fixstart    fixstart  is an option in Circlator just to orient a sequence.  plasmid.fa.trimmed  is our small plasmid.  plasmid_fixstart  is the prefix for the output files.   View the output:  1 less plasmid_fixstart.log    The plasmid has been oriented at a gene predicted by Prodigal, and the break-point is at position 1200.  Change the file name:   1 cp plasmid_fixstart.fasta contig2.fasta", 
            "title": "Plasmid contig orientation"
        }, 
        {
            "location": "/modules/btp-module-denovo-canu/denovo_canu/#collect-contigs", 
            "text": "1 cat contig1.fasta contig2.fasta   genome.fasta    See the contigs and sizes:   1 infoseq genome.fasta    chromosome: 2823331  plasmid: 2473", 
            "title": "Collect contigs"
        }, 
        {
            "location": "/modules/btp-module-denovo-canu/denovo_canu/#questions_2", 
            "text": "Question  Why is this section so complicated?   Answer Finding small plasmids is difficult for many reasons! This paper has a nice summary: On the (im)possibility to reconstruct plasmids from whole genome short-read sequencing data. doi:  https://doi.org/10.1101/086744   Question  Why can PacBio sequencing miss small plasmids?   Answer Library prep size selection   Question  We extract unmapped Illumina reads and assemble these to find small plasmids. What could they be missing?   Answer Repeats that have mapped to the PacBio assembly.   Question  How do you find a plasmid in a Bandage graph?   Answer It is probably circular, matches the size of a known plasmid, and has a rep gene.   Question  Are there easier ways to find plasmids?   Answer Possibly. One option is the program called Unicycler which may automate many of these steps.  https://github.com/rrwick/Unicycler", 
            "title": "Questions"
        }, 
        {
            "location": "/modules/btp-module-denovo-canu/denovo_canu/#correct", 
            "text": "We will correct the Pacbio assembly with Illumina reads, using the tool Pilon ( https://github.com/broadinstitute/pilon/wiki ).", 
            "title": "Correct"
        }, 
        {
            "location": "/modules/btp-module-denovo-canu/denovo_canu/#make-an-alignment-file", 
            "text": "Align the Illumina reads (R1 and R2) to the draft PacBio assembly, e.g.  genome.fasta :   1\n2 bwa index genome.fasta\nbwa mem -t 4 genome.fasta illumina_R1.fastq.gz illumina_R2.fastq.gz | samtools sort   aln_illumina_pacbio_NGS.bam    -t  is the number of cores    NGS workshop: Stop the run by typing  Ctrl-C . We will use the pre-computed file called  aln_illumina_pacbio.bam .   Index the files:   1\n2 samtools index aln_illumina_pacbio.bam\nsamtools faidx genome.fasta    Now we have an alignment file to use in Pilon:  aln_illumina_pacbio.bam", 
            "title": "Make an alignment file"
        }, 
        {
            "location": "/modules/btp-module-denovo-canu/denovo_canu/#run-pilon", 
            "text": "Run:   1 pilon --genome genome.fasta --frags aln_illumina_pacbio.bam --output pilon1_NGS --fix all --mindepth 0.5 --changes --verbose --threads 4    --genome  is the name of the input assembly to be corrected  --frags  is the alignment of the reads against the assembly  --output  is the name of the output prefix  --fix  is an option for types of corrections  --mindepth  gives a minimum read depth to use  --changes  produces an output file of the changes made  --verbose  prints information to the screen during the run  --threads : the number of cores   NGS workshop: Stop the run by typing  Ctrl-C . We will use the pre-computed files called with the prefixes  pilon1 .  Look at the changes file:  1 less pilon1.changes   Example:   Look at the details of the fasta file:  1 infoseq pilon1.fasta    chromosome - 2823340 (net +9 bases)  plasmid - 2473 (no change)   Option:  If there are many changes, run Pilon again, using the  pilon1.fasta  file as the input assembly, and the Illumina reads to correct.", 
            "title": "Run Pilon"
        }, 
        {
            "location": "/modules/btp-module-denovo-canu/denovo_canu/#genome-output", 
            "text": "Change the file name:   1 cp pilon1.fasta assembly.fasta    We now have the corrected genome assembly of  Staphylococcus aureus  in .fasta format, containing a chromosome and a small plasmid.", 
            "title": "Genome output"
        }, 
        {
            "location": "/modules/btp-module-denovo-canu/denovo_canu/#questions_3", 
            "text": "Question  Why don t we correct earlier in the assembly process?   Answer We need to circularise the contigs and trim overhangs first.   Question  Why can we use some reads (Illumina) to correct other reads (PacBio) ?   Answer Illumina reads have higher accuracy   Question  Could we just use PacBio reads to assemble the genome?   Answer Yes, if accuracy adequate.", 
            "title": "Questions"
        }, 
        {
            "location": "/modules/btp-module-denovo-canu/denovo_canu/#next", 
            "text": "", 
            "title": "Next"
        }, 
        {
            "location": "/modules/btp-module-denovo-canu/denovo_canu/#further-analyses", 
            "text": "Annotate genomes, e.g. with Prokka,  https://github.com/tseemann/prokka  Comparative genomics, e.g. with Roary,  https://sanger-pathogens.github.io/Roary/", 
            "title": "Further analyses"
        }, 
        {
            "location": "/modules/btp-module-denovo-canu/denovo_canu/#links", 
            "text": "Canu  manual  and  gitub repository  Circlator  article  and  github repository  Pilon  article  and  github repository  Notes on  finishing  and  evaluating  assemblies.", 
            "title": "Links"
        }
    ]
}